{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWeYRKjGiY552jXuiD6tot"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JoNPFT9bsSD7","executionInfo":{"status":"ok","timestamp":1695014006858,"user_tz":-540,"elapsed":83765,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"2141219d-6df6-492a-9f43-24a91d92411e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3665\n","Epoch 1: val_loss improved from inf to 0.34944, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 734ms/step - loss: 0.3665 - val_loss: 0.3494\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3630\n","Epoch 2: val_loss improved from 0.34944 to 0.34616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3630 - val_loss: 0.3462\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3595\n","Epoch 3: val_loss improved from 0.34616 to 0.34292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3595 - val_loss: 0.3429\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3560\n","Epoch 4: val_loss improved from 0.34292 to 0.33971, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3560 - val_loss: 0.3397\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3526\n","Epoch 5: val_loss improved from 0.33971 to 0.33653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3526 - val_loss: 0.3365\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3492\n","Epoch 6: val_loss improved from 0.33653 to 0.33339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3492 - val_loss: 0.3334\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3459\n","Epoch 7: val_loss improved from 0.33339 to 0.33029, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3459 - val_loss: 0.3303\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3426\n","Epoch 8: val_loss improved from 0.33029 to 0.32721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3426 - val_loss: 0.3272\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3393\n","Epoch 9: val_loss improved from 0.32721 to 0.32417, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3393 - val_loss: 0.3242\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3360\n","Epoch 10: val_loss improved from 0.32417 to 0.32117, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3360 - val_loss: 0.3212\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3328\n","Epoch 11: val_loss improved from 0.32117 to 0.31819, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3328 - val_loss: 0.3182\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3296\n","Epoch 12: val_loss improved from 0.31819 to 0.31525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3296 - val_loss: 0.3153\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3265\n","Epoch 13: val_loss improved from 0.31525 to 0.31234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3265 - val_loss: 0.3123\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3233\n","Epoch 14: val_loss improved from 0.31234 to 0.30946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3233 - val_loss: 0.3095\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3203\n","Epoch 15: val_loss improved from 0.30946 to 0.30661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3203 - val_loss: 0.3066\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3172\n","Epoch 16: val_loss improved from 0.30661 to 0.30380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3172 - val_loss: 0.3038\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3142\n","Epoch 17: val_loss improved from 0.30380 to 0.30101, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3142 - val_loss: 0.3010\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3112\n","Epoch 18: val_loss improved from 0.30101 to 0.29825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3112 - val_loss: 0.2983\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3082\n","Epoch 19: val_loss improved from 0.29825 to 0.29552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3082 - val_loss: 0.2955\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3053\n","Epoch 20: val_loss improved from 0.29552 to 0.29283, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3053 - val_loss: 0.2928\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3024\n","Epoch 21: val_loss improved from 0.29283 to 0.29016, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3024 - val_loss: 0.2902\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2995\n","Epoch 22: val_loss improved from 0.29016 to 0.28752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2995 - val_loss: 0.2875\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2967\n","Epoch 23: val_loss improved from 0.28752 to 0.28491, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2967 - val_loss: 0.2849\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2939\n","Epoch 24: val_loss improved from 0.28491 to 0.28232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2939 - val_loss: 0.2823\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2911\n","Epoch 25: val_loss improved from 0.28232 to 0.27977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2911 - val_loss: 0.2798\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2884\n","Epoch 26: val_loss improved from 0.27977 to 0.27724, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2884 - val_loss: 0.2772\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2856\n","Epoch 27: val_loss improved from 0.27724 to 0.27474, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2856 - val_loss: 0.2747\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2830\n","Epoch 28: val_loss improved from 0.27474 to 0.27226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2830 - val_loss: 0.2723\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2803\n","Epoch 29: val_loss improved from 0.27226 to 0.26981, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2803 - val_loss: 0.2698\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2777\n","Epoch 30: val_loss improved from 0.26981 to 0.26739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2777 - val_loss: 0.2674\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2750\n","Epoch 31: val_loss improved from 0.26739 to 0.26500, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2750 - val_loss: 0.2650\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2725\n","Epoch 32: val_loss improved from 0.26500 to 0.26263, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.2725 - val_loss: 0.2626\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2699\n","Epoch 33: val_loss improved from 0.26263 to 0.26028, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.2699 - val_loss: 0.2603\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2674\n","Epoch 34: val_loss improved from 0.26028 to 0.25796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.2674 - val_loss: 0.2580\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2649\n","Epoch 35: val_loss improved from 0.25796 to 0.25567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.2649 - val_loss: 0.2557\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2624\n","Epoch 36: val_loss improved from 0.25567 to 0.25340, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.2624 - val_loss: 0.2534\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2599\n","Epoch 37: val_loss improved from 0.25340 to 0.25116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2599 - val_loss: 0.2512\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2575\n","Epoch 38: val_loss improved from 0.25116 to 0.24894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.2575 - val_loss: 0.2489\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2551\n","Epoch 39: val_loss improved from 0.24894 to 0.24674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2551 - val_loss: 0.2467\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2527\n","Epoch 40: val_loss improved from 0.24674 to 0.24457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2527 - val_loss: 0.2446\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2504\n","Epoch 41: val_loss improved from 0.24457 to 0.24242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2504 - val_loss: 0.2424\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2480\n","Epoch 42: val_loss improved from 0.24242 to 0.24029, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.2480 - val_loss: 0.2403\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2457\n","Epoch 43: val_loss improved from 0.24029 to 0.23818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2457 - val_loss: 0.2382\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2435\n","Epoch 44: val_loss improved from 0.23818 to 0.23610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2435 - val_loss: 0.2361\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2412\n","Epoch 45: val_loss improved from 0.23610 to 0.23404, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2412 - val_loss: 0.2340\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2390\n","Epoch 46: val_loss improved from 0.23404 to 0.23201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2390 - val_loss: 0.2320\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2367\n","Epoch 47: val_loss improved from 0.23201 to 0.22999, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2367 - val_loss: 0.2300\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2346\n","Epoch 48: val_loss improved from 0.22999 to 0.22800, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.2346 - val_loss: 0.2280\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2324\n","Epoch 49: val_loss improved from 0.22800 to 0.22603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.2324 - val_loss: 0.2260\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2303\n","Epoch 50: val_loss improved from 0.22603 to 0.22408, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.2303 - val_loss: 0.2241\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2281\n","Epoch 51: val_loss improved from 0.22408 to 0.22215, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2281 - val_loss: 0.2221\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2260\n","Epoch 52: val_loss improved from 0.22215 to 0.22024, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2260 - val_loss: 0.2202\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2240\n","Epoch 53: val_loss improved from 0.22024 to 0.21835, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2240 - val_loss: 0.2184\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2219\n","Epoch 54: val_loss improved from 0.21835 to 0.21648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2219 - val_loss: 0.2165\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2199\n","Epoch 55: val_loss improved from 0.21648 to 0.21463, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2199 - val_loss: 0.2146\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2178\n","Epoch 56: val_loss improved from 0.21463 to 0.21281, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2178 - val_loss: 0.2128\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2159\n","Epoch 57: val_loss improved from 0.21281 to 0.21100, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2159 - val_loss: 0.2110\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2139\n","Epoch 58: val_loss improved from 0.21100 to 0.20921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2139 - val_loss: 0.2092\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2119\n","Epoch 59: val_loss improved from 0.20921 to 0.20744, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2119 - val_loss: 0.2074\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 60: val_loss improved from 0.20744 to 0.20569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2100 - val_loss: 0.2057\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2081\n","Epoch 61: val_loss improved from 0.20569 to 0.20396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.2081 - val_loss: 0.2040\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2062\n","Epoch 62: val_loss improved from 0.20396 to 0.20225, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2062 - val_loss: 0.2022\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2043\n","Epoch 63: val_loss improved from 0.20225 to 0.20055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2043 - val_loss: 0.2006\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2025\n","Epoch 64: val_loss improved from 0.20055 to 0.19888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2025 - val_loss: 0.1989\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2006\n","Epoch 65: val_loss improved from 0.19888 to 0.19722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.2006 - val_loss: 0.1972\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1988\n","Epoch 66: val_loss improved from 0.19722 to 0.19558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1988 - val_loss: 0.1956\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1970\n","Epoch 67: val_loss improved from 0.19558 to 0.19396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1970 - val_loss: 0.1940\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1952\n","Epoch 68: val_loss improved from 0.19396 to 0.19235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1952 - val_loss: 0.1924\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1935\n","Epoch 69: val_loss improved from 0.19235 to 0.19076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1935 - val_loss: 0.1908\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1917\n","Epoch 70: val_loss improved from 0.19076 to 0.18919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1917 - val_loss: 0.1892\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 71: val_loss improved from 0.18919 to 0.18764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1900 - val_loss: 0.1876\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1883\n","Epoch 72: val_loss improved from 0.18764 to 0.18610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1883 - val_loss: 0.1861\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1866\n","Epoch 73: val_loss improved from 0.18610 to 0.18458, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1866 - val_loss: 0.1846\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1849\n","Epoch 74: val_loss improved from 0.18458 to 0.18308, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1849 - val_loss: 0.1831\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1833\n","Epoch 75: val_loss improved from 0.18308 to 0.18159, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1833 - val_loss: 0.1816\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1816\n","Epoch 76: val_loss improved from 0.18159 to 0.18012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1816 - val_loss: 0.1801\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1800\n","Epoch 77: val_loss improved from 0.18012 to 0.17867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1800 - val_loss: 0.1787\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1784\n","Epoch 78: val_loss improved from 0.17867 to 0.17723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1784 - val_loss: 0.1772\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1768\n","Epoch 79: val_loss improved from 0.17723 to 0.17581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1768 - val_loss: 0.1758\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1753\n","Epoch 80: val_loss improved from 0.17581 to 0.17440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1753 - val_loss: 0.1744\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1737\n","Epoch 81: val_loss improved from 0.17440 to 0.17301, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1737 - val_loss: 0.1730\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1722\n","Epoch 82: val_loss improved from 0.17301 to 0.17163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1722 - val_loss: 0.1716\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1706\n","Epoch 83: val_loss improved from 0.17163 to 0.17027, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1706 - val_loss: 0.1703\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1691\n","Epoch 84: val_loss improved from 0.17027 to 0.16892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1691 - val_loss: 0.1689\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1676\n","Epoch 85: val_loss improved from 0.16892 to 0.16758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 170ms/step - loss: 0.1676 - val_loss: 0.1676\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1662\n","Epoch 86: val_loss improved from 0.16758 to 0.16627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1662 - val_loss: 0.1663\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1647\n","Epoch 87: val_loss improved from 0.16627 to 0.16496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1647 - val_loss: 0.1650\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1633\n","Epoch 88: val_loss improved from 0.16496 to 0.16367, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1633 - val_loss: 0.1637\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1618\n","Epoch 89: val_loss improved from 0.16367 to 0.16240, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1618 - val_loss: 0.1624\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1604\n","Epoch 90: val_loss improved from 0.16240 to 0.16113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1604 - val_loss: 0.1611\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1590\n","Epoch 91: val_loss improved from 0.16113 to 0.15988, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1590 - val_loss: 0.1599\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1576\n","Epoch 92: val_loss improved from 0.15988 to 0.15865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1576 - val_loss: 0.1587\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1562\n","Epoch 93: val_loss improved from 0.15865 to 0.15743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1562 - val_loss: 0.1574\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1549\n","Epoch 94: val_loss improved from 0.15743 to 0.15622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1549 - val_loss: 0.1562\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1535\n","Epoch 95: val_loss improved from 0.15622 to 0.15503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 170ms/step - loss: 0.1535 - val_loss: 0.1550\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1522\n","Epoch 96: val_loss improved from 0.15503 to 0.15384, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.1522 - val_loss: 0.1538\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1509\n","Epoch 97: val_loss improved from 0.15384 to 0.15268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1509 - val_loss: 0.1527\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1496\n","Epoch 98: val_loss improved from 0.15268 to 0.15152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1496 - val_loss: 0.1515\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1483\n","Epoch 99: val_loss improved from 0.15152 to 0.15038, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1483 - val_loss: 0.1504\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1470\n","Epoch 100: val_loss improved from 0.15038 to 0.14925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1470 - val_loss: 0.1492\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1458\n","Epoch 101: val_loss improved from 0.14925 to 0.14813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1458 - val_loss: 0.1481\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1445\n","Epoch 102: val_loss improved from 0.14813 to 0.14702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1445 - val_loss: 0.1470\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1433\n","Epoch 103: val_loss improved from 0.14702 to 0.14593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1433 - val_loss: 0.1459\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1420\n","Epoch 104: val_loss improved from 0.14593 to 0.14485, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1420 - val_loss: 0.1448\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1408\n","Epoch 105: val_loss improved from 0.14485 to 0.14378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1408 - val_loss: 0.1438\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1396\n","Epoch 106: val_loss improved from 0.14378 to 0.14272, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 165ms/step - loss: 0.1396 - val_loss: 0.1427\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1384\n","Epoch 107: val_loss improved from 0.14272 to 0.14167, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1384 - val_loss: 0.1417\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1373\n","Epoch 108: val_loss improved from 0.14167 to 0.14064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1373 - val_loss: 0.1406\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1361\n","Epoch 109: val_loss improved from 0.14064 to 0.13961, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1361 - val_loss: 0.1396\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1350\n","Epoch 110: val_loss improved from 0.13961 to 0.13860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1350 - val_loss: 0.1386\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1338\n","Epoch 111: val_loss improved from 0.13860 to 0.13760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1338 - val_loss: 0.1376\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1327\n","Epoch 112: val_loss improved from 0.13760 to 0.13661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1327 - val_loss: 0.1366\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1316\n","Epoch 113: val_loss improved from 0.13661 to 0.13563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1316 - val_loss: 0.1356\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1305\n","Epoch 114: val_loss improved from 0.13563 to 0.13466, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1305 - val_loss: 0.1347\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1294\n","Epoch 115: val_loss improved from 0.13466 to 0.13370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1294 - val_loss: 0.1337\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1283\n","Epoch 116: val_loss improved from 0.13370 to 0.13276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1283 - val_loss: 0.1328\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1272\n","Epoch 117: val_loss improved from 0.13276 to 0.13182, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1272 - val_loss: 0.1318\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1262\n","Epoch 118: val_loss improved from 0.13182 to 0.13089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1262 - val_loss: 0.1309\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1251\n","Epoch 119: val_loss improved from 0.13089 to 0.12998, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1251 - val_loss: 0.1300\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1241\n","Epoch 120: val_loss improved from 0.12998 to 0.12907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1241 - val_loss: 0.1291\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1231\n","Epoch 121: val_loss improved from 0.12907 to 0.12818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1231 - val_loss: 0.1282\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1220\n","Epoch 122: val_loss improved from 0.12818 to 0.12729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1220 - val_loss: 0.1273\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1210\n","Epoch 123: val_loss improved from 0.12729 to 0.12641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1210 - val_loss: 0.1264\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1200\n","Epoch 124: val_loss improved from 0.12641 to 0.12555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1200 - val_loss: 0.1255\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1191\n","Epoch 125: val_loss improved from 0.12555 to 0.12469, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1191 - val_loss: 0.1247\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1181\n","Epoch 126: val_loss improved from 0.12469 to 0.12385, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1181 - val_loss: 0.1238\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1171\n","Epoch 127: val_loss improved from 0.12385 to 0.12301, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1171 - val_loss: 0.1230\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1162\n","Epoch 128: val_loss improved from 0.12301 to 0.12218, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1162 - val_loss: 0.1222\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1152\n","Epoch 129: val_loss improved from 0.12218 to 0.12136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1152 - val_loss: 0.1214\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1143\n","Epoch 130: val_loss improved from 0.12136 to 0.12055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1143 - val_loss: 0.1206\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1134\n","Epoch 131: val_loss improved from 0.12055 to 0.11975, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1134 - val_loss: 0.1197\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1125\n","Epoch 132: val_loss improved from 0.11975 to 0.11896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1125 - val_loss: 0.1190\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1115\n","Epoch 133: val_loss improved from 0.11896 to 0.11817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1115 - val_loss: 0.1182\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1107\n","Epoch 134: val_loss improved from 0.11817 to 0.11740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1107 - val_loss: 0.1174\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1098\n","Epoch 135: val_loss improved from 0.11740 to 0.11663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1098 - val_loss: 0.1166\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1089\n","Epoch 136: val_loss improved from 0.11663 to 0.11588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1089 - val_loss: 0.1159\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1080\n","Epoch 137: val_loss improved from 0.11588 to 0.11513, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1080 - val_loss: 0.1151\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1072\n","Epoch 138: val_loss improved from 0.11513 to 0.11439, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1072 - val_loss: 0.1144\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 139: val_loss improved from 0.11439 to 0.11366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1063 - val_loss: 0.1137\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1055\n","Epoch 140: val_loss improved from 0.11366 to 0.11293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1055 - val_loss: 0.1129\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1046\n","Epoch 141: val_loss improved from 0.11293 to 0.11222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1046 - val_loss: 0.1122\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1038\n","Epoch 142: val_loss improved from 0.11222 to 0.11151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1038 - val_loss: 0.1115\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1030\n","Epoch 143: val_loss improved from 0.11151 to 0.11081, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1030 - val_loss: 0.1108\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1022\n","Epoch 144: val_loss improved from 0.11081 to 0.11012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1022 - val_loss: 0.1101\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1014\n","Epoch 145: val_loss improved from 0.11012 to 0.10944, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1014 - val_loss: 0.1094\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1006\n","Epoch 146: val_loss improved from 0.10944 to 0.10876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1006 - val_loss: 0.1088\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0998\n","Epoch 147: val_loss improved from 0.10876 to 0.10809, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0998 - val_loss: 0.1081\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0991\n","Epoch 148: val_loss improved from 0.10809 to 0.10743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0991 - val_loss: 0.1074\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0983\n","Epoch 149: val_loss improved from 0.10743 to 0.10678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0983 - val_loss: 0.1068\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 150: val_loss improved from 0.10678 to 0.10613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0975 - val_loss: 0.1061\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0968\n","Epoch 151: val_loss improved from 0.10613 to 0.10549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0968 - val_loss: 0.1055\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0960\n","Epoch 152: val_loss improved from 0.10549 to 0.10486, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0960 - val_loss: 0.1049\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0953\n","Epoch 153: val_loss improved from 0.10486 to 0.10424, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0953 - val_loss: 0.1042\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0946\n","Epoch 154: val_loss improved from 0.10424 to 0.10362, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0946 - val_loss: 0.1036\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0939\n","Epoch 155: val_loss improved from 0.10362 to 0.10301, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0939 - val_loss: 0.1030\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0932\n","Epoch 156: val_loss improved from 0.10301 to 0.10241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0932 - val_loss: 0.1024\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0924\n","Epoch 157: val_loss improved from 0.10241 to 0.10181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0924 - val_loss: 0.1018\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0918\n","Epoch 158: val_loss improved from 0.10181 to 0.10122, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0918 - val_loss: 0.1012\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0911\n","Epoch 159: val_loss improved from 0.10122 to 0.10064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0911 - val_loss: 0.1006\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0904\n","Epoch 160: val_loss improved from 0.10064 to 0.10006, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0904 - val_loss: 0.1001\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0897\n","Epoch 161: val_loss improved from 0.10006 to 0.09949, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0897 - val_loss: 0.0995\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0890\n","Epoch 162: val_loss improved from 0.09949 to 0.09893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0890 - val_loss: 0.0989\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0884\n","Epoch 163: val_loss improved from 0.09893 to 0.09838, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0884 - val_loss: 0.0984\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0877\n","Epoch 164: val_loss improved from 0.09838 to 0.09783, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0877 - val_loss: 0.0978\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0871\n","Epoch 165: val_loss improved from 0.09783 to 0.09728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0871 - val_loss: 0.0973\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0864\n","Epoch 166: val_loss improved from 0.09728 to 0.09674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0864 - val_loss: 0.0967\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0858\n","Epoch 167: val_loss improved from 0.09674 to 0.09621, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0858 - val_loss: 0.0962\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0852\n","Epoch 168: val_loss improved from 0.09621 to 0.09569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0852 - val_loss: 0.0957\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0846\n","Epoch 169: val_loss improved from 0.09569 to 0.09517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0846 - val_loss: 0.0952\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0840\n","Epoch 170: val_loss improved from 0.09517 to 0.09466, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0840 - val_loss: 0.0947\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0833\n","Epoch 171: val_loss improved from 0.09466 to 0.09415, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0833 - val_loss: 0.0941\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0827\n","Epoch 172: val_loss improved from 0.09415 to 0.09365, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0827 - val_loss: 0.0936\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0822\n","Epoch 173: val_loss improved from 0.09365 to 0.09315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0822 - val_loss: 0.0932\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0816\n","Epoch 174: val_loss improved from 0.09315 to 0.09266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0816 - val_loss: 0.0927\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 175: val_loss improved from 0.09266 to 0.09218, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0810 - val_loss: 0.0922\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0804\n","Epoch 176: val_loss improved from 0.09218 to 0.09170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0804 - val_loss: 0.0917\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0798\n","Epoch 177: val_loss improved from 0.09170 to 0.09123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0798 - val_loss: 0.0912\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0793\n","Epoch 178: val_loss improved from 0.09123 to 0.09076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0793 - val_loss: 0.0908\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0787\n","Epoch 179: val_loss improved from 0.09076 to 0.09030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0787 - val_loss: 0.0903\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0782\n","Epoch 180: val_loss improved from 0.09030 to 0.08984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0782 - val_loss: 0.0898\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0776\n","Epoch 181: val_loss improved from 0.08984 to 0.08939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0776 - val_loss: 0.0894\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0771\n","Epoch 182: val_loss improved from 0.08939 to 0.08894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0771 - val_loss: 0.0889\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 183: val_loss improved from 0.08894 to 0.08850, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0766 - val_loss: 0.0885\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0760\n","Epoch 184: val_loss improved from 0.08850 to 0.08807, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0760 - val_loss: 0.0881\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 185: val_loss improved from 0.08807 to 0.08763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0755 - val_loss: 0.0876\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0750\n","Epoch 186: val_loss improved from 0.08763 to 0.08721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0750 - val_loss: 0.0872\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0745\n","Epoch 187: val_loss improved from 0.08721 to 0.08679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0745 - val_loss: 0.0868\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0740\n","Epoch 188: val_loss improved from 0.08679 to 0.08637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0740 - val_loss: 0.0864\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0735\n","Epoch 189: val_loss improved from 0.08637 to 0.08596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0735 - val_loss: 0.0860\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0730\n","Epoch 190: val_loss improved from 0.08596 to 0.08556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0730 - val_loss: 0.0856\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0725\n","Epoch 191: val_loss improved from 0.08556 to 0.08515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0725 - val_loss: 0.0852\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0720\n","Epoch 192: val_loss improved from 0.08515 to 0.08476, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0720 - val_loss: 0.0848\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0715\n","Epoch 193: val_loss improved from 0.08476 to 0.08437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0715 - val_loss: 0.0844\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0710\n","Epoch 194: val_loss improved from 0.08437 to 0.08398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0710 - val_loss: 0.0840\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0706\n","Epoch 195: val_loss improved from 0.08398 to 0.08360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0706 - val_loss: 0.0836\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0701\n","Epoch 196: val_loss improved from 0.08360 to 0.08322, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0701 - val_loss: 0.0832\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 197: val_loss improved from 0.08322 to 0.08284, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0696 - val_loss: 0.0828\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0692\n","Epoch 198: val_loss improved from 0.08284 to 0.08248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0692 - val_loss: 0.0825\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0687\n","Epoch 199: val_loss improved from 0.08248 to 0.08211, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0687 - val_loss: 0.0821\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0683\n","Epoch 200: val_loss improved from 0.08211 to 0.08175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0683 - val_loss: 0.0818\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0679\n","Epoch 201: val_loss improved from 0.08175 to 0.08139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0679 - val_loss: 0.0814\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0674\n","Epoch 202: val_loss improved from 0.08139 to 0.08104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0674 - val_loss: 0.0810\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0670\n","Epoch 203: val_loss improved from 0.08104 to 0.08069, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0670 - val_loss: 0.0807\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0666\n","Epoch 204: val_loss improved from 0.08069 to 0.08035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0666 - val_loss: 0.0804\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 205: val_loss improved from 0.08035 to 0.08001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0661 - val_loss: 0.0800\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 206: val_loss improved from 0.08001 to 0.07968, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0657 - val_loss: 0.0797\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0653\n","Epoch 207: val_loss improved from 0.07968 to 0.07934, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0653 - val_loss: 0.0793\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0649\n","Epoch 208: val_loss improved from 0.07934 to 0.07902, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0649 - val_loss: 0.0790\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0645\n","Epoch 209: val_loss improved from 0.07902 to 0.07869, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0645 - val_loss: 0.0787\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 210: val_loss improved from 0.07869 to 0.07837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0641 - val_loss: 0.0784\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0637\n","Epoch 211: val_loss improved from 0.07837 to 0.07806, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0637 - val_loss: 0.0781\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0633\n","Epoch 212: val_loss improved from 0.07806 to 0.07775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0633 - val_loss: 0.0777\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 213: val_loss improved from 0.07775 to 0.07744, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0629 - val_loss: 0.0774\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0625\n","Epoch 214: val_loss improved from 0.07744 to 0.07713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0625 - val_loss: 0.0771\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 215: val_loss improved from 0.07713 to 0.07683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0622 - val_loss: 0.0768\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0618\n","Epoch 216: val_loss improved from 0.07683 to 0.07653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0618 - val_loss: 0.0765\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0614\n","Epoch 217: val_loss improved from 0.07653 to 0.07624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0614 - val_loss: 0.0762\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0611\n","Epoch 218: val_loss improved from 0.07624 to 0.07595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0611 - val_loss: 0.0760\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0607\n","Epoch 219: val_loss improved from 0.07595 to 0.07566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0607 - val_loss: 0.0757\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0603\n","Epoch 220: val_loss improved from 0.07566 to 0.07538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0603 - val_loss: 0.0754\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 221: val_loss improved from 0.07538 to 0.07510, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0600 - val_loss: 0.0751\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0596\n","Epoch 222: val_loss improved from 0.07510 to 0.07483, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0596 - val_loss: 0.0748\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0593\n","Epoch 223: val_loss improved from 0.07483 to 0.07455, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0593 - val_loss: 0.0746\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 224: val_loss improved from 0.07455 to 0.07428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0589 - val_loss: 0.0743\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 225: val_loss improved from 0.07428 to 0.07402, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0586 - val_loss: 0.0740\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0583\n","Epoch 226: val_loss improved from 0.07402 to 0.07375, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0583 - val_loss: 0.0738\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0579\n","Epoch 227: val_loss improved from 0.07375 to 0.07349, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0579 - val_loss: 0.0735\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 228: val_loss improved from 0.07349 to 0.07324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0576 - val_loss: 0.0732\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0573\n","Epoch 229: val_loss improved from 0.07324 to 0.07298, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0573 - val_loss: 0.0730\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 230: val_loss improved from 0.07298 to 0.07273, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0570 - val_loss: 0.0727\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0566\n","Epoch 231: val_loss improved from 0.07273 to 0.07249, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0566 - val_loss: 0.0725\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0563\n","Epoch 232: val_loss improved from 0.07249 to 0.07224, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0563 - val_loss: 0.0722\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0560\n","Epoch 233: val_loss improved from 0.07224 to 0.07200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0560 - val_loss: 0.0720\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 234: val_loss improved from 0.07200 to 0.07176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0557 - val_loss: 0.0718\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 235: val_loss improved from 0.07176 to 0.07153, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0554 - val_loss: 0.0715\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 236: val_loss improved from 0.07153 to 0.07129, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0551 - val_loss: 0.0713\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0548\n","Epoch 237: val_loss improved from 0.07129 to 0.07107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0548 - val_loss: 0.0711\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0545\n","Epoch 238: val_loss improved from 0.07107 to 0.07084, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0545 - val_loss: 0.0708\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 239: val_loss improved from 0.07084 to 0.07061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0542 - val_loss: 0.0706\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0539\n","Epoch 240: val_loss improved from 0.07061 to 0.07039, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0539 - val_loss: 0.0704\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 241: val_loss improved from 0.07039 to 0.07018, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0536 - val_loss: 0.0702\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0533\n","Epoch 242: val_loss improved from 0.07018 to 0.06996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0533 - val_loss: 0.0700\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0531\n","Epoch 243: val_loss improved from 0.06996 to 0.06975, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0531 - val_loss: 0.0697\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0528\n","Epoch 244: val_loss improved from 0.06975 to 0.06954, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0528 - val_loss: 0.0695\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0525\n","Epoch 245: val_loss improved from 0.06954 to 0.06933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0525 - val_loss: 0.0693\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0522\n","Epoch 246: val_loss improved from 0.06933 to 0.06912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0522 - val_loss: 0.0691\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0520\n","Epoch 247: val_loss improved from 0.06912 to 0.06892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0520 - val_loss: 0.0689\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0517\n","Epoch 248: val_loss improved from 0.06892 to 0.06872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0517 - val_loss: 0.0687\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 249: val_loss improved from 0.06872 to 0.06852, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0515 - val_loss: 0.0685\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 250: val_loss improved from 0.06852 to 0.06833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0512 - val_loss: 0.0683\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 251: val_loss improved from 0.06833 to 0.06814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0509 - val_loss: 0.0681\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0507\n","Epoch 252: val_loss improved from 0.06814 to 0.06795, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0507 - val_loss: 0.0679\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 253: val_loss improved from 0.06795 to 0.06776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0504 - val_loss: 0.0678\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0502\n","Epoch 254: val_loss improved from 0.06776 to 0.06757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0502 - val_loss: 0.0676\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0499\n","Epoch 255: val_loss improved from 0.06757 to 0.06739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0499 - val_loss: 0.0674\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 256: val_loss improved from 0.06739 to 0.06721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0497 - val_loss: 0.0672\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 257: val_loss improved from 0.06721 to 0.06703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0495 - val_loss: 0.0670\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 258: val_loss improved from 0.06703 to 0.06686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0492 - val_loss: 0.0669\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 259: val_loss improved from 0.06686 to 0.06668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0490 - val_loss: 0.0667\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 260: val_loss improved from 0.06668 to 0.06651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0488 - val_loss: 0.0665\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0485\n","Epoch 261: val_loss improved from 0.06651 to 0.06634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0485 - val_loss: 0.0663\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 262: val_loss improved from 0.06634 to 0.06618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0483 - val_loss: 0.0662\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 263: val_loss improved from 0.06618 to 0.06601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0481 - val_loss: 0.0660\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 264: val_loss improved from 0.06601 to 0.06585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0479 - val_loss: 0.0658\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 265: val_loss improved from 0.06585 to 0.06569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0476 - val_loss: 0.0657\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 266: val_loss improved from 0.06569 to 0.06553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0474 - val_loss: 0.0655\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 267: val_loss improved from 0.06553 to 0.06537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0472 - val_loss: 0.0654\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 268: val_loss improved from 0.06537 to 0.06522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0470 - val_loss: 0.0652\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 269: val_loss improved from 0.06522 to 0.06506, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0468 - val_loss: 0.0651\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 270: val_loss improved from 0.06506 to 0.06491, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0466 - val_loss: 0.0649\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0464\n","Epoch 271: val_loss improved from 0.06491 to 0.06477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0464 - val_loss: 0.0648\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 272: val_loss improved from 0.06477 to 0.06462, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0462 - val_loss: 0.0646\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 273: val_loss improved from 0.06462 to 0.06447, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0460 - val_loss: 0.0645\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 274: val_loss improved from 0.06447 to 0.06433, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0458 - val_loss: 0.0643\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 275: val_loss improved from 0.06433 to 0.06419, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0456 - val_loss: 0.0642\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 276: val_loss improved from 0.06419 to 0.06405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0454 - val_loss: 0.0641\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 277: val_loss improved from 0.06405 to 0.06391, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0452 - val_loss: 0.0639\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 278: val_loss improved from 0.06391 to 0.06378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0450 - val_loss: 0.0638\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 279: val_loss improved from 0.06378 to 0.06364, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0448 - val_loss: 0.0636\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 280: val_loss improved from 0.06364 to 0.06351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0446 - val_loss: 0.0635\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 281: val_loss improved from 0.06351 to 0.06338, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0444 - val_loss: 0.0634\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 282: val_loss improved from 0.06338 to 0.06325, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0442 - val_loss: 0.0633\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0441\n","Epoch 283: val_loss improved from 0.06325 to 0.06313, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0441 - val_loss: 0.0631\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 284: val_loss improved from 0.06313 to 0.06300, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0439 - val_loss: 0.0630\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 285: val_loss improved from 0.06300 to 0.06288, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0437 - val_loss: 0.0629\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 286: val_loss improved from 0.06288 to 0.06276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0435 - val_loss: 0.0628\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 287: val_loss improved from 0.06276 to 0.06264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0434 - val_loss: 0.0626\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 288: val_loss improved from 0.06264 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0432 - val_loss: 0.0625\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 289: val_loss improved from 0.06252 to 0.06240, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0430 - val_loss: 0.0624\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 290: val_loss improved from 0.06240 to 0.06228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0429 - val_loss: 0.0623\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 291: val_loss improved from 0.06228 to 0.06217, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0427 - val_loss: 0.0622\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 292: val_loss improved from 0.06217 to 0.06206, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0425 - val_loss: 0.0621\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 293: val_loss improved from 0.06206 to 0.06195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0424 - val_loss: 0.0619\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 294: val_loss improved from 0.06195 to 0.06184, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0422 - val_loss: 0.0618\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 295: val_loss improved from 0.06184 to 0.06173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0420 - val_loss: 0.0617\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 296: val_loss improved from 0.06173 to 0.06162, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 297: val_loss improved from 0.06162 to 0.06152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0417 - val_loss: 0.0615\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 298: val_loss improved from 0.06152 to 0.06142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0416 - val_loss: 0.0614\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 299: val_loss improved from 0.06142 to 0.06131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0414 - val_loss: 0.0613\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 300: val_loss improved from 0.06131 to 0.06121, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0413 - val_loss: 0.0612\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 301: val_loss improved from 0.06121 to 0.06111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0411 - val_loss: 0.0611\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 302: val_loss improved from 0.06111 to 0.06102, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0410 - val_loss: 0.0610\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 303: val_loss improved from 0.06102 to 0.06092, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0408 - val_loss: 0.0609\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 304: val_loss improved from 0.06092 to 0.06082, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0407 - val_loss: 0.0608\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 305: val_loss improved from 0.06082 to 0.06073, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0406 - val_loss: 0.0607\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 306: val_loss improved from 0.06073 to 0.06064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0404 - val_loss: 0.0606\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 307: val_loss improved from 0.06064 to 0.06055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0403 - val_loss: 0.0605\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 308: val_loss improved from 0.06055 to 0.06046, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 309: val_loss improved from 0.06046 to 0.06037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0400 - val_loss: 0.0604\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 310: val_loss improved from 0.06037 to 0.06028, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0399 - val_loss: 0.0603\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 311: val_loss improved from 0.06028 to 0.06019, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0398 - val_loss: 0.0602\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 312: val_loss improved from 0.06019 to 0.06011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 313: val_loss improved from 0.06011 to 0.06002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0395 - val_loss: 0.0600\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 314: val_loss improved from 0.06002 to 0.05994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0394 - val_loss: 0.0599\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 315: val_loss improved from 0.05994 to 0.05986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0392 - val_loss: 0.0599\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 316: val_loss improved from 0.05986 to 0.05978, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0391 - val_loss: 0.0598\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 317: val_loss improved from 0.05978 to 0.05970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0390 - val_loss: 0.0597\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 318: val_loss improved from 0.05970 to 0.05962, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0389 - val_loss: 0.0596\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 319: val_loss improved from 0.05962 to 0.05954, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0387 - val_loss: 0.0595\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 320: val_loss improved from 0.05954 to 0.05947, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0386 - val_loss: 0.0595\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 321: val_loss improved from 0.05947 to 0.05939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0385 - val_loss: 0.0594\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 322: val_loss improved from 0.05939 to 0.05932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0384 - val_loss: 0.0593\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 323: val_loss improved from 0.05932 to 0.05925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0383 - val_loss: 0.0592\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 324: val_loss improved from 0.05925 to 0.05917, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0382 - val_loss: 0.0592\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 325: val_loss improved from 0.05917 to 0.05910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0380 - val_loss: 0.0591\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 326: val_loss improved from 0.05910 to 0.05903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0379 - val_loss: 0.0590\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 327: val_loss improved from 0.05903 to 0.05896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0378 - val_loss: 0.0590\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 328: val_loss improved from 0.05896 to 0.05890, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0377 - val_loss: 0.0589\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 329: val_loss improved from 0.05890 to 0.05883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 330: val_loss improved from 0.05883 to 0.05876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0375 - val_loss: 0.0588\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 331: val_loss improved from 0.05876 to 0.05870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0374 - val_loss: 0.0587\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 332: val_loss improved from 0.05870 to 0.05864, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0373 - val_loss: 0.0586\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 333: val_loss improved from 0.05864 to 0.05857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0372 - val_loss: 0.0586\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 334: val_loss improved from 0.05857 to 0.05851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0371 - val_loss: 0.0585\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 335: val_loss improved from 0.05851 to 0.05845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0370 - val_loss: 0.0584\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 336: val_loss improved from 0.05845 to 0.05839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 337: val_loss improved from 0.05839 to 0.05833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0368 - val_loss: 0.0583\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 338: val_loss improved from 0.05833 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0367 - val_loss: 0.0583\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 339: val_loss improved from 0.05827 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0366 - val_loss: 0.0582\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 340: val_loss improved from 0.05821 to 0.05816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 341: val_loss improved from 0.05816 to 0.05810, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 342: val_loss improved from 0.05810 to 0.05805, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0363 - val_loss: 0.0580\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 343: val_loss improved from 0.05805 to 0.05799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0362 - val_loss: 0.0580\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 344: val_loss improved from 0.05799 to 0.05794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0361 - val_loss: 0.0579\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 345: val_loss improved from 0.05794 to 0.05789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0360 - val_loss: 0.0579\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 346: val_loss improved from 0.05789 to 0.05784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 347: val_loss improved from 0.05784 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 348: val_loss improved from 0.05778 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0358 - val_loss: 0.0577\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 349: val_loss improved from 0.05773 to 0.05768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 350: val_loss improved from 0.05768 to 0.05764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 351: val_loss improved from 0.05764 to 0.05759, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 352: val_loss improved from 0.05759 to 0.05754, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 353: val_loss improved from 0.05754 to 0.05749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 354: val_loss improved from 0.05749 to 0.05745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0353 - val_loss: 0.0574\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 355: val_loss improved from 0.05745 to 0.05740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 356: val_loss improved from 0.05740 to 0.05736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0351 - val_loss: 0.0574\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 357: val_loss improved from 0.05736 to 0.05732, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 358: val_loss improved from 0.05732 to 0.05727, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 359: val_loss improved from 0.05727 to 0.05723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0349 - val_loss: 0.0572\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 360: val_loss improved from 0.05723 to 0.05719, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 361: val_loss improved from 0.05719 to 0.05715, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 362: val_loss improved from 0.05715 to 0.05711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 363: val_loss improved from 0.05711 to 0.05707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 364: val_loss improved from 0.05707 to 0.05703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0345 - val_loss: 0.0570\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 365: val_loss improved from 0.05703 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 366: val_loss improved from 0.05699 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 367: val_loss improved from 0.05695 to 0.05692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 368: val_loss improved from 0.05692 to 0.05688, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0342 - val_loss: 0.0569\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 369: val_loss improved from 0.05688 to 0.05684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0342 - val_loss: 0.0568\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 370: val_loss improved from 0.05684 to 0.05681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 371: val_loss improved from 0.05681 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 372: val_loss improved from 0.05677 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0340 - val_loss: 0.0567\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 373: val_loss improved from 0.05674 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 374: val_loss improved from 0.05671 to 0.05667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 375: val_loss improved from 0.05667 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0338 - val_loss: 0.0566\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 376: val_loss improved from 0.05664 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 377: val_loss improved from 0.05661 to 0.05658, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0336 - val_loss: 0.0566\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 378: val_loss improved from 0.05658 to 0.05655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 379: val_loss improved from 0.05655 to 0.05652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 380: val_loss improved from 0.05652 to 0.05649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 381: val_loss improved from 0.05649 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 382: val_loss improved from 0.05646 to 0.05643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 383: val_loss improved from 0.05643 to 0.05640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 384: val_loss improved from 0.05640 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 385: val_loss improved from 0.05637 to 0.05635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0332 - val_loss: 0.0563\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 386: val_loss improved from 0.05635 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 387: val_loss improved from 0.05632 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 388: val_loss improved from 0.05629 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 389: val_loss improved from 0.05627 to 0.05624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 390: val_loss improved from 0.05624 to 0.05622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 391: val_loss improved from 0.05622 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 392: val_loss improved from 0.05619 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 393/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 393: val_loss improved from 0.05617 to 0.05614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 394/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 394: val_loss improved from 0.05614 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 395/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 395: val_loss improved from 0.05612 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 396/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 396: val_loss improved from 0.05610 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 397/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 397: val_loss improved from 0.05607 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0325 - val_loss: 0.0561\n","Epoch 398/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 398: val_loss improved from 0.05605 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 399/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 399: val_loss improved from 0.05603 to 0.05601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 400/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 400: val_loss improved from 0.05601 to 0.05599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 401/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 401: val_loss improved from 0.05599 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 402/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 402: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0323 - val_loss: 0.0559\n","Epoch 403/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 403: val_loss improved from 0.05595 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 404/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 404: val_loss improved from 0.05593 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 405/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 405: val_loss improved from 0.05591 to 0.05589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 406/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 406: val_loss improved from 0.05589 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 407/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 407: val_loss improved from 0.05587 to 0.05585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0320 - val_loss: 0.0559\n","Epoch 408/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 408: val_loss improved from 0.05585 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 409/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 409: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 410/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 410: val_loss improved from 0.05582 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 411/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 411: val_loss improved from 0.05580 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 412/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 412: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 413/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 413: val_loss improved from 0.05577 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 414/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 414: val_loss improved from 0.05575 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 415/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 415: val_loss improved from 0.05574 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 416/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 416: val_loss improved from 0.05572 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 417/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 417: val_loss improved from 0.05571 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 418/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 418: val_loss improved from 0.05569 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 419/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 419: val_loss improved from 0.05568 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 420/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 420: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 421/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 421: val_loss improved from 0.05565 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 422/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 422: val_loss improved from 0.05564 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 423/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 423: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 424/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 424: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 425/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 425: val_loss improved from 0.05560 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 426/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 426: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 427/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 427: val_loss improved from 0.05557 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 428/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 428: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 429/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 429: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 430/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 430: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 431/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 431: val_loss improved from 0.05553 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 432/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 432: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 433/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 433: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 434/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 434: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 435/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 435: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 436/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 436: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 437/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 437: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 438/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 438: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 439/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 439: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 440/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 440: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 441/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 441: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 442/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 442: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 443/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 443: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 444/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 444: val_loss improved from 0.05540 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 445/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 445: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 446/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 446: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 447/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 447: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 448/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 448: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 449/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 449: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 450/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 450: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 451/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 451: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 452/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 452: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 453/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 453: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 454/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 454: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 455/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 455: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 456/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 456: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 457/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 457: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 458/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 458: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 459/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 459: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 460/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 460: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 461/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 461: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 462/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 462: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 463/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 463: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 464/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 464: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 465/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 465: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 466/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 466: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 467/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 467: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 468/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 468: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 469/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 469: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 470/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 470: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 471/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 471: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 472/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 472: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 473/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 473: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 474/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 474: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 475/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 475: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 476/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 476: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 477/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 477: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 478/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 478: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 479/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 479: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 480/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 480: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 481/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 481: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 482/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 482: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 483/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 483: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 484/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 484: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 485/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 485: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 486/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 486: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 487/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 487: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 488/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 488: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 489/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 489: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 490/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 490: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 491/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 491: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 492/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 492: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 493/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 493: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 494/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 494: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 495/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 495: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 496/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 496: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 497/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 497: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 498/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 498: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 499/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 499: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 500/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 500: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 501/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 501: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 502/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 502: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 503/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 503: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 504/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 504: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 505/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 505: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 506/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 506: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 507/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 507: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 508/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 508: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 509/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 509: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 510/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 510: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 511/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 511: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 512/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 512: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 513/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 513: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 514/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 514: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 515/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 515: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 516/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 516: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 517/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 517: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 518/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 518: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 519/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 519: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 520/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 520: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 521/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 521: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 522/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 522: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 523/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 523: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 524/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 524: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 525/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 525: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 526/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 526: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 527/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 527: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 528/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 528: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 529/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 529: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 530/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 530: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 531/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 531: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 532/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 532: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 533/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 533: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 534/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 534: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 535/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 535: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 536/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 536: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 537/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 537: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 538/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 538: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 539/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 539: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 540/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 540: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 541/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 541: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 542/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 542: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 543/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 543: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 544/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 544: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 545/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 545: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 546/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 546: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 547/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 547: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 548/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 548: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 549/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 549: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 550/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 550: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 551/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 551: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 552/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 552: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 553/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 553: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 554/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 554: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 555/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 555: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 556/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 556: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 557/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 557: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 558/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 558: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 559/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 559: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 560/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 560: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 561/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 561: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 562/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 562: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 563/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 563: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 564/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 564: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 565/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 565: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 566/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 566: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 567/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 567: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 568/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 568: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 569/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 569: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 570/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 570: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 571/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 571: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 572/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 572: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 573/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 573: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 574/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 574: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 575/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 575: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 576/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 576: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 577/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 577: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 578/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 578: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 579/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 579: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 580/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 580: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 581/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 581: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 582/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 582: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 583/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 583: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 584/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 584: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 585/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 585: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 586/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 586: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 587/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 587: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 588/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 588: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 589/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 589: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 590/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 590: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 591/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 591: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 592/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 592: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 593/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 593: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 594/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 594: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 595/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 595: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 596/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 596: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 597/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 597: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 598/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 598: val_loss did not improve from 0.05522\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0286 - val_loss: 0.0555\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0733\n","loss_and_metrics : 0.07333377748727798\n","1/1 [==============================] - 0s 78ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=20, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}