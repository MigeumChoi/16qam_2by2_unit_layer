{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXE3Xpm20khpvEN4t9wgJC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jkhMH_ZluHOB","executionInfo":{"status":"ok","timestamp":1695015373104,"user_tz":-540,"elapsed":83614,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"a7643147-2680-4775-843c-00b51430eeab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3663\n","Epoch 1: val_loss improved from inf to 0.34936, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 741ms/step - loss: 0.3663 - val_loss: 0.3494\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3629\n","Epoch 2: val_loss improved from 0.34936 to 0.34619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3629 - val_loss: 0.3462\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3595\n","Epoch 3: val_loss improved from 0.34619 to 0.34306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3595 - val_loss: 0.3431\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3562\n","Epoch 4: val_loss improved from 0.34306 to 0.33996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3562 - val_loss: 0.3400\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3529\n","Epoch 5: val_loss improved from 0.33996 to 0.33690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3529 - val_loss: 0.3369\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3496\n","Epoch 6: val_loss improved from 0.33690 to 0.33386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3496 - val_loss: 0.3339\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3464\n","Epoch 7: val_loss improved from 0.33386 to 0.33086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.3464 - val_loss: 0.3309\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3432\n","Epoch 8: val_loss improved from 0.33086 to 0.32788, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3432 - val_loss: 0.3279\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3400\n","Epoch 9: val_loss improved from 0.32788 to 0.32494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3400 - val_loss: 0.3249\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3368\n","Epoch 10: val_loss improved from 0.32494 to 0.32203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3368 - val_loss: 0.3220\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3337\n","Epoch 11: val_loss improved from 0.32203 to 0.31915, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3337 - val_loss: 0.3192\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3306\n","Epoch 12: val_loss improved from 0.31915 to 0.31630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3306 - val_loss: 0.3163\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3276\n","Epoch 13: val_loss improved from 0.31630 to 0.31348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3276 - val_loss: 0.3135\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3246\n","Epoch 14: val_loss improved from 0.31348 to 0.31069, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3246 - val_loss: 0.3107\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3216\n","Epoch 15: val_loss improved from 0.31069 to 0.30793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3216 - val_loss: 0.3079\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3186\n","Epoch 16: val_loss improved from 0.30793 to 0.30520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3186 - val_loss: 0.3052\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3157\n","Epoch 17: val_loss improved from 0.30520 to 0.30249, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3157 - val_loss: 0.3025\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3128\n","Epoch 18: val_loss improved from 0.30249 to 0.29981, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3128 - val_loss: 0.2998\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3099\n","Epoch 19: val_loss improved from 0.29981 to 0.29717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3099 - val_loss: 0.2972\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3071\n","Epoch 20: val_loss improved from 0.29717 to 0.29455, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3071 - val_loss: 0.2945\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3043\n","Epoch 21: val_loss improved from 0.29455 to 0.29195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3043 - val_loss: 0.2920\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3015\n","Epoch 22: val_loss improved from 0.29195 to 0.28939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3015 - val_loss: 0.2894\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2987\n","Epoch 23: val_loss improved from 0.28939 to 0.28685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2987 - val_loss: 0.2868\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2960\n","Epoch 24: val_loss improved from 0.28685 to 0.28433, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2960 - val_loss: 0.2843\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2933\n","Epoch 25: val_loss improved from 0.28433 to 0.28185, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2933 - val_loss: 0.2818\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2906\n","Epoch 26: val_loss improved from 0.28185 to 0.27939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2906 - val_loss: 0.2794\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2880\n","Epoch 27: val_loss improved from 0.27939 to 0.27695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2880 - val_loss: 0.2770\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2853\n","Epoch 28: val_loss improved from 0.27695 to 0.27454, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2853 - val_loss: 0.2745\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2827\n","Epoch 29: val_loss improved from 0.27454 to 0.27216, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2827 - val_loss: 0.2722\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2802\n","Epoch 30: val_loss improved from 0.27216 to 0.26980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2802 - val_loss: 0.2698\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2776\n","Epoch 31: val_loss improved from 0.26980 to 0.26746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2776 - val_loss: 0.2675\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2751\n","Epoch 32: val_loss improved from 0.26746 to 0.26515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2751 - val_loss: 0.2652\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2726\n","Epoch 33: val_loss improved from 0.26515 to 0.26286, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2726 - val_loss: 0.2629\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2701\n","Epoch 34: val_loss improved from 0.26286 to 0.26060, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2701 - val_loss: 0.2606\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2677\n","Epoch 35: val_loss improved from 0.26060 to 0.25836, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2677 - val_loss: 0.2584\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2653\n","Epoch 36: val_loss improved from 0.25836 to 0.25615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2653 - val_loss: 0.2561\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2629\n","Epoch 37: val_loss improved from 0.25615 to 0.25396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2629 - val_loss: 0.2540\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2605\n","Epoch 38: val_loss improved from 0.25396 to 0.25179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2605 - val_loss: 0.2518\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2582\n","Epoch 39: val_loss improved from 0.25179 to 0.24964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2582 - val_loss: 0.2496\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2559\n","Epoch 40: val_loss improved from 0.24964 to 0.24752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2559 - val_loss: 0.2475\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2536\n","Epoch 41: val_loss improved from 0.24752 to 0.24541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2536 - val_loss: 0.2454\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2513\n","Epoch 42: val_loss improved from 0.24541 to 0.24333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2513 - val_loss: 0.2433\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2490\n","Epoch 43: val_loss improved from 0.24333 to 0.24128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2490 - val_loss: 0.2413\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2468\n","Epoch 44: val_loss improved from 0.24128 to 0.23924, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2468 - val_loss: 0.2392\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2446\n","Epoch 45: val_loss improved from 0.23924 to 0.23722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2446 - val_loss: 0.2372\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2424\n","Epoch 46: val_loss improved from 0.23722 to 0.23523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2424 - val_loss: 0.2352\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2402\n","Epoch 47: val_loss improved from 0.23523 to 0.23326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2402 - val_loss: 0.2333\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2381\n","Epoch 48: val_loss improved from 0.23326 to 0.23130, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2381 - val_loss: 0.2313\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2360\n","Epoch 49: val_loss improved from 0.23130 to 0.22937, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2360 - val_loss: 0.2294\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2339\n","Epoch 50: val_loss improved from 0.22937 to 0.22746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2339 - val_loss: 0.2275\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2318\n","Epoch 51: val_loss improved from 0.22746 to 0.22557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2318 - val_loss: 0.2256\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2297\n","Epoch 52: val_loss improved from 0.22557 to 0.22370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2297 - val_loss: 0.2237\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2277\n","Epoch 53: val_loss improved from 0.22370 to 0.22184, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2277 - val_loss: 0.2218\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2257\n","Epoch 54: val_loss improved from 0.22184 to 0.22001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2257 - val_loss: 0.2200\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2237\n","Epoch 55: val_loss improved from 0.22001 to 0.21820, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2237 - val_loss: 0.2182\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2217\n","Epoch 56: val_loss improved from 0.21820 to 0.21640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2217 - val_loss: 0.2164\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2198\n","Epoch 57: val_loss improved from 0.21640 to 0.21463, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2198 - val_loss: 0.2146\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2178\n","Epoch 58: val_loss improved from 0.21463 to 0.21287, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2178 - val_loss: 0.2129\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2159\n","Epoch 59: val_loss improved from 0.21287 to 0.21113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2159 - val_loss: 0.2111\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2140\n","Epoch 60: val_loss improved from 0.21113 to 0.20941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2140 - val_loss: 0.2094\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2121\n","Epoch 61: val_loss improved from 0.20941 to 0.20771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2121 - val_loss: 0.2077\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2103\n","Epoch 62: val_loss improved from 0.20771 to 0.20603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2103 - val_loss: 0.2060\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2084\n","Epoch 63: val_loss improved from 0.20603 to 0.20436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2084 - val_loss: 0.2044\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2066\n","Epoch 64: val_loss improved from 0.20436 to 0.20271, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2066 - val_loss: 0.2027\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2048\n","Epoch 65: val_loss improved from 0.20271 to 0.20108, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2048 - val_loss: 0.2011\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2030\n","Epoch 66: val_loss improved from 0.20108 to 0.19946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2030 - val_loss: 0.1995\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2013\n","Epoch 67: val_loss improved from 0.19946 to 0.19787, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2013 - val_loss: 0.1979\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1995\n","Epoch 68: val_loss improved from 0.19787 to 0.19628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1995 - val_loss: 0.1963\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1978\n","Epoch 69: val_loss improved from 0.19628 to 0.19472, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1978 - val_loss: 0.1947\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1961\n","Epoch 70: val_loss improved from 0.19472 to 0.19317, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1961 - val_loss: 0.1932\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1944\n","Epoch 71: val_loss improved from 0.19317 to 0.19164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1944 - val_loss: 0.1916\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1927\n","Epoch 72: val_loss improved from 0.19164 to 0.19013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1927 - val_loss: 0.1901\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1910\n","Epoch 73: val_loss improved from 0.19013 to 0.18863, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1910 - val_loss: 0.1886\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 74: val_loss improved from 0.18863 to 0.18714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1894 - val_loss: 0.1871\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1877\n","Epoch 75: val_loss improved from 0.18714 to 0.18568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1877 - val_loss: 0.1857\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1861\n","Epoch 76: val_loss improved from 0.18568 to 0.18422, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1861 - val_loss: 0.1842\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1845\n","Epoch 77: val_loss improved from 0.18422 to 0.18279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1845 - val_loss: 0.1828\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1829\n","Epoch 78: val_loss improved from 0.18279 to 0.18137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1829 - val_loss: 0.1814\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1814\n","Epoch 79: val_loss improved from 0.18137 to 0.17996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1814 - val_loss: 0.1800\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1798\n","Epoch 80: val_loss improved from 0.17996 to 0.17857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1798 - val_loss: 0.1786\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1783\n","Epoch 81: val_loss improved from 0.17857 to 0.17719, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1783 - val_loss: 0.1772\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1768\n","Epoch 82: val_loss improved from 0.17719 to 0.17583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1768 - val_loss: 0.1758\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1753\n","Epoch 83: val_loss improved from 0.17583 to 0.17448, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1753 - val_loss: 0.1745\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1738\n","Epoch 84: val_loss improved from 0.17448 to 0.17315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1738 - val_loss: 0.1731\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1723\n","Epoch 85: val_loss improved from 0.17315 to 0.17183, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1723 - val_loss: 0.1718\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1709\n","Epoch 86: val_loss improved from 0.17183 to 0.17052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1709 - val_loss: 0.1705\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1694\n","Epoch 87: val_loss improved from 0.17052 to 0.16923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1694 - val_loss: 0.1692\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1680\n","Epoch 88: val_loss improved from 0.16923 to 0.16795, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1680 - val_loss: 0.1679\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1666\n","Epoch 89: val_loss improved from 0.16795 to 0.16668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1666 - val_loss: 0.1667\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1652\n","Epoch 90: val_loss improved from 0.16668 to 0.16543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1652 - val_loss: 0.1654\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1638\n","Epoch 91: val_loss improved from 0.16543 to 0.16420, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1638 - val_loss: 0.1642\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1624\n","Epoch 92: val_loss improved from 0.16420 to 0.16297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1624 - val_loss: 0.1630\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1610\n","Epoch 93: val_loss improved from 0.16297 to 0.16176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1610 - val_loss: 0.1618\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1597\n","Epoch 94: val_loss improved from 0.16176 to 0.16056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1597 - val_loss: 0.1606\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1584\n","Epoch 95: val_loss improved from 0.16056 to 0.15937, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1584 - val_loss: 0.1594\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1570\n","Epoch 96: val_loss improved from 0.15937 to 0.15820, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1570 - val_loss: 0.1582\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1557\n","Epoch 97: val_loss improved from 0.15820 to 0.15704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1557 - val_loss: 0.1570\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1544\n","Epoch 98: val_loss improved from 0.15704 to 0.15589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1544 - val_loss: 0.1559\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1532\n","Epoch 99: val_loss improved from 0.15589 to 0.15475, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1532 - val_loss: 0.1548\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1519\n","Epoch 100: val_loss improved from 0.15475 to 0.15363, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1519 - val_loss: 0.1536\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1506\n","Epoch 101: val_loss improved from 0.15363 to 0.15251, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1506 - val_loss: 0.1525\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1494\n","Epoch 102: val_loss improved from 0.15251 to 0.15141, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1494 - val_loss: 0.1514\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1482\n","Epoch 103: val_loss improved from 0.15141 to 0.15032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1482 - val_loss: 0.1503\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1470\n","Epoch 104: val_loss improved from 0.15032 to 0.14925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.1470 - val_loss: 0.1492\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1458\n","Epoch 105: val_loss improved from 0.14925 to 0.14818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1458 - val_loss: 0.1482\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1446\n","Epoch 106: val_loss improved from 0.14818 to 0.14713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1446 - val_loss: 0.1471\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1434\n","Epoch 107: val_loss improved from 0.14713 to 0.14608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1434 - val_loss: 0.1461\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1422\n","Epoch 108: val_loss improved from 0.14608 to 0.14505, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1422 - val_loss: 0.1451\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1411\n","Epoch 109: val_loss improved from 0.14505 to 0.14403, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1411 - val_loss: 0.1440\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1399\n","Epoch 110: val_loss improved from 0.14403 to 0.14302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1399 - val_loss: 0.1430\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1388\n","Epoch 111: val_loss improved from 0.14302 to 0.14202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1388 - val_loss: 0.1420\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1377\n","Epoch 112: val_loss improved from 0.14202 to 0.14103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1377 - val_loss: 0.1410\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1365\n","Epoch 113: val_loss improved from 0.14103 to 0.14005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1365 - val_loss: 0.1401\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1354\n","Epoch 114: val_loss improved from 0.14005 to 0.13908, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1354 - val_loss: 0.1391\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1344\n","Epoch 115: val_loss improved from 0.13908 to 0.13813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1344 - val_loss: 0.1381\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1333\n","Epoch 116: val_loss improved from 0.13813 to 0.13718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1333 - val_loss: 0.1372\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1322\n","Epoch 117: val_loss improved from 0.13718 to 0.13624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1322 - val_loss: 0.1362\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1312\n","Epoch 118: val_loss improved from 0.13624 to 0.13531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.1312 - val_loss: 0.1353\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1301\n","Epoch 119: val_loss improved from 0.13531 to 0.13440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1301 - val_loss: 0.1344\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1291\n","Epoch 120: val_loss improved from 0.13440 to 0.13349, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1291 - val_loss: 0.1335\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1281\n","Epoch 121: val_loss improved from 0.13349 to 0.13259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1281 - val_loss: 0.1326\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1270\n","Epoch 122: val_loss improved from 0.13259 to 0.13170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.1270 - val_loss: 0.1317\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1260\n","Epoch 123: val_loss improved from 0.13170 to 0.13083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1260 - val_loss: 0.1308\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1250\n","Epoch 124: val_loss improved from 0.13083 to 0.12996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1250 - val_loss: 0.1300\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1241\n","Epoch 125: val_loss improved from 0.12996 to 0.12910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1241 - val_loss: 0.1291\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1231\n","Epoch 126: val_loss improved from 0.12910 to 0.12825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1231 - val_loss: 0.1282\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1221\n","Epoch 127: val_loss improved from 0.12825 to 0.12741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.1221 - val_loss: 0.1274\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1212\n","Epoch 128: val_loss improved from 0.12741 to 0.12657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1212 - val_loss: 0.1266\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1202\n","Epoch 129: val_loss improved from 0.12657 to 0.12575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.1202 - val_loss: 0.1258\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1193\n","Epoch 130: val_loss improved from 0.12575 to 0.12494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1193 - val_loss: 0.1249\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1184\n","Epoch 131: val_loss improved from 0.12494 to 0.12413, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1184 - val_loss: 0.1241\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1174\n","Epoch 132: val_loss improved from 0.12413 to 0.12333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1174 - val_loss: 0.1233\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1165\n","Epoch 133: val_loss improved from 0.12333 to 0.12254, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1165 - val_loss: 0.1225\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1156\n","Epoch 134: val_loss improved from 0.12254 to 0.12176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1156 - val_loss: 0.1218\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1148\n","Epoch 135: val_loss improved from 0.12176 to 0.12099, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1148 - val_loss: 0.1210\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1139\n","Epoch 136: val_loss improved from 0.12099 to 0.12023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1139 - val_loss: 0.1202\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1130\n","Epoch 137: val_loss improved from 0.12023 to 0.11947, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1130 - val_loss: 0.1195\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1121\n","Epoch 138: val_loss improved from 0.11947 to 0.11873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1121 - val_loss: 0.1187\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1113\n","Epoch 139: val_loss improved from 0.11873 to 0.11799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1113 - val_loss: 0.1180\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1104\n","Epoch 140: val_loss improved from 0.11799 to 0.11726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1104 - val_loss: 0.1173\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1096\n","Epoch 141: val_loss improved from 0.11726 to 0.11654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1096 - val_loss: 0.1165\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1088\n","Epoch 142: val_loss improved from 0.11654 to 0.11582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1088 - val_loss: 0.1158\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1080\n","Epoch 143: val_loss improved from 0.11582 to 0.11511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1080 - val_loss: 0.1151\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 144: val_loss improved from 0.11511 to 0.11441, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1071 - val_loss: 0.1144\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 145: val_loss improved from 0.11441 to 0.11372, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1063 - val_loss: 0.1137\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1055\n","Epoch 146: val_loss improved from 0.11372 to 0.11304, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1055 - val_loss: 0.1130\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1048\n","Epoch 147: val_loss improved from 0.11304 to 0.11236, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1048 - val_loss: 0.1124\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1040\n","Epoch 148: val_loss improved from 0.11236 to 0.11169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1040 - val_loss: 0.1117\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1032\n","Epoch 149: val_loss improved from 0.11169 to 0.11103, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1032 - val_loss: 0.1110\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1024\n","Epoch 150: val_loss improved from 0.11103 to 0.11037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1024 - val_loss: 0.1104\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1017\n","Epoch 151: val_loss improved from 0.11037 to 0.10972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1017 - val_loss: 0.1097\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1009\n","Epoch 152: val_loss improved from 0.10972 to 0.10908, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1009 - val_loss: 0.1091\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1002\n","Epoch 153: val_loss improved from 0.10908 to 0.10845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1002 - val_loss: 0.1084\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0995\n","Epoch 154: val_loss improved from 0.10845 to 0.10782, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0995 - val_loss: 0.1078\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0987\n","Epoch 155: val_loss improved from 0.10782 to 0.10720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0987 - val_loss: 0.1072\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0980\n","Epoch 156: val_loss improved from 0.10720 to 0.10659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0980 - val_loss: 0.1066\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0973\n","Epoch 157: val_loss improved from 0.10659 to 0.10598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0973 - val_loss: 0.1060\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0966\n","Epoch 158: val_loss improved from 0.10598 to 0.10538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0966 - val_loss: 0.1054\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0959\n","Epoch 159: val_loss improved from 0.10538 to 0.10479, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0959 - val_loss: 0.1048\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0952\n","Epoch 160: val_loss improved from 0.10479 to 0.10420, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0952 - val_loss: 0.1042\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0945\n","Epoch 161: val_loss improved from 0.10420 to 0.10362, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0945 - val_loss: 0.1036\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0939\n","Epoch 162: val_loss improved from 0.10362 to 0.10304, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0939 - val_loss: 0.1030\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0932\n","Epoch 163: val_loss improved from 0.10304 to 0.10248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0932 - val_loss: 0.1025\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0925\n","Epoch 164: val_loss improved from 0.10248 to 0.10191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0925 - val_loss: 0.1019\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0919\n","Epoch 165: val_loss improved from 0.10191 to 0.10136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0919 - val_loss: 0.1014\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0912\n","Epoch 166: val_loss improved from 0.10136 to 0.10081, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0912 - val_loss: 0.1008\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0906\n","Epoch 167: val_loss improved from 0.10081 to 0.10026, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0906 - val_loss: 0.1003\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0899\n","Epoch 168: val_loss improved from 0.10026 to 0.09973, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0899 - val_loss: 0.0997\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0893\n","Epoch 169: val_loss improved from 0.09973 to 0.09920, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0893 - val_loss: 0.0992\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0887\n","Epoch 170: val_loss improved from 0.09920 to 0.09867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0887 - val_loss: 0.0987\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0881\n","Epoch 171: val_loss improved from 0.09867 to 0.09815, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0881 - val_loss: 0.0981\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0875\n","Epoch 172: val_loss improved from 0.09815 to 0.09763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0875 - val_loss: 0.0976\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0869\n","Epoch 173: val_loss improved from 0.09763 to 0.09713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0869 - val_loss: 0.0971\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 174: val_loss improved from 0.09713 to 0.09662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0863 - val_loss: 0.0966\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0857\n","Epoch 175: val_loss improved from 0.09662 to 0.09612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0857 - val_loss: 0.0961\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0851\n","Epoch 176: val_loss improved from 0.09612 to 0.09563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0851 - val_loss: 0.0956\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0845\n","Epoch 177: val_loss improved from 0.09563 to 0.09515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0845 - val_loss: 0.0951\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0839\n","Epoch 178: val_loss improved from 0.09515 to 0.09466, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0839 - val_loss: 0.0947\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0834\n","Epoch 179: val_loss improved from 0.09466 to 0.09419, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0834 - val_loss: 0.0942\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0828\n","Epoch 180: val_loss improved from 0.09419 to 0.09372, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0828 - val_loss: 0.0937\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0822\n","Epoch 181: val_loss improved from 0.09372 to 0.09325, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0822 - val_loss: 0.0933\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0817\n","Epoch 182: val_loss improved from 0.09325 to 0.09279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0817 - val_loss: 0.0928\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0811\n","Epoch 183: val_loss improved from 0.09279 to 0.09234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0811 - val_loss: 0.0923\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0806\n","Epoch 184: val_loss improved from 0.09234 to 0.09189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0806 - val_loss: 0.0919\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0801\n","Epoch 185: val_loss improved from 0.09189 to 0.09144, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0801 - val_loss: 0.0914\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0795\n","Epoch 186: val_loss improved from 0.09144 to 0.09100, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0795 - val_loss: 0.0910\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 187: val_loss improved from 0.09100 to 0.09056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0790 - val_loss: 0.0906\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0785\n","Epoch 188: val_loss improved from 0.09056 to 0.09013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0785 - val_loss: 0.0901\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0780\n","Epoch 189: val_loss improved from 0.09013 to 0.08971, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0780 - val_loss: 0.0897\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0775\n","Epoch 190: val_loss improved from 0.08971 to 0.08929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0775 - val_loss: 0.0893\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0770\n","Epoch 191: val_loss improved from 0.08929 to 0.08887, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0770 - val_loss: 0.0889\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0765\n","Epoch 192: val_loss improved from 0.08887 to 0.08846, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0765 - val_loss: 0.0885\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0760\n","Epoch 193: val_loss improved from 0.08846 to 0.08805, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0760 - val_loss: 0.0881\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 194: val_loss improved from 0.08805 to 0.08765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0755 - val_loss: 0.0876\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0750\n","Epoch 195: val_loss improved from 0.08765 to 0.08725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0750 - val_loss: 0.0872\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0745\n","Epoch 196: val_loss improved from 0.08725 to 0.08686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0745 - val_loss: 0.0869\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0741\n","Epoch 197: val_loss improved from 0.08686 to 0.08647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0741 - val_loss: 0.0865\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0736\n","Epoch 198: val_loss improved from 0.08647 to 0.08608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0736 - val_loss: 0.0861\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0731\n","Epoch 199: val_loss improved from 0.08608 to 0.08570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0731 - val_loss: 0.0857\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 200: val_loss improved from 0.08570 to 0.08532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0727 - val_loss: 0.0853\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0722\n","Epoch 201: val_loss improved from 0.08532 to 0.08495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0722 - val_loss: 0.0850\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0718\n","Epoch 202: val_loss improved from 0.08495 to 0.08458, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0718 - val_loss: 0.0846\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0713\n","Epoch 203: val_loss improved from 0.08458 to 0.08422, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0713 - val_loss: 0.0842\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0709\n","Epoch 204: val_loss improved from 0.08422 to 0.08386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0709 - val_loss: 0.0839\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0704\n","Epoch 205: val_loss improved from 0.08386 to 0.08350, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0704 - val_loss: 0.0835\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0700\n","Epoch 206: val_loss improved from 0.08350 to 0.08315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0700 - val_loss: 0.0832\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 207: val_loss improved from 0.08315 to 0.08280, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0696 - val_loss: 0.0828\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0692\n","Epoch 208: val_loss improved from 0.08280 to 0.08246, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0692 - val_loss: 0.0825\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0687\n","Epoch 209: val_loss improved from 0.08246 to 0.08212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0687 - val_loss: 0.0821\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0683\n","Epoch 210: val_loss improved from 0.08212 to 0.08178, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0683 - val_loss: 0.0818\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0679\n","Epoch 211: val_loss improved from 0.08178 to 0.08145, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0679 - val_loss: 0.0815\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0675\n","Epoch 212: val_loss improved from 0.08145 to 0.08112, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0675 - val_loss: 0.0811\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 213: val_loss improved from 0.08112 to 0.08080, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0671 - val_loss: 0.0808\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0667\n","Epoch 214: val_loss improved from 0.08080 to 0.08048, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0667 - val_loss: 0.0805\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0663\n","Epoch 215: val_loss improved from 0.08048 to 0.08016, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0663 - val_loss: 0.0802\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0659\n","Epoch 216: val_loss improved from 0.08016 to 0.07984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0659 - val_loss: 0.0798\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0655\n","Epoch 217: val_loss improved from 0.07984 to 0.07953, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0655 - val_loss: 0.0795\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 218: val_loss improved from 0.07953 to 0.07923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0651 - val_loss: 0.0792\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 219: val_loss improved from 0.07923 to 0.07892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0648 - val_loss: 0.0789\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0644\n","Epoch 220: val_loss improved from 0.07892 to 0.07862, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0644 - val_loss: 0.0786\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0640\n","Epoch 221: val_loss improved from 0.07862 to 0.07833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0640 - val_loss: 0.0783\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0637\n","Epoch 222: val_loss improved from 0.07833 to 0.07803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0637 - val_loss: 0.0780\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0633\n","Epoch 223: val_loss improved from 0.07803 to 0.07774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0633 - val_loss: 0.0777\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 224: val_loss improved from 0.07774 to 0.07746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0629 - val_loss: 0.0775\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0626\n","Epoch 225: val_loss improved from 0.07746 to 0.07717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0626 - val_loss: 0.0772\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 226: val_loss improved from 0.07717 to 0.07689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0622 - val_loss: 0.0769\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0619\n","Epoch 227: val_loss improved from 0.07689 to 0.07662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0619 - val_loss: 0.0766\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0615\n","Epoch 228: val_loss improved from 0.07662 to 0.07634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0615 - val_loss: 0.0763\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0612\n","Epoch 229: val_loss improved from 0.07634 to 0.07607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0612 - val_loss: 0.0761\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0609\n","Epoch 230: val_loss improved from 0.07607 to 0.07580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0609 - val_loss: 0.0758\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0605\n","Epoch 231: val_loss improved from 0.07580 to 0.07554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0605 - val_loss: 0.0755\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0602\n","Epoch 232: val_loss improved from 0.07554 to 0.07528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0602 - val_loss: 0.0753\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 233: val_loss improved from 0.07528 to 0.07502, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0599 - val_loss: 0.0750\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0595\n","Epoch 234: val_loss improved from 0.07502 to 0.07476, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0595 - val_loss: 0.0748\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0592\n","Epoch 235: val_loss improved from 0.07476 to 0.07451, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0592 - val_loss: 0.0745\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 236: val_loss improved from 0.07451 to 0.07426, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0589 - val_loss: 0.0743\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 237: val_loss improved from 0.07426 to 0.07402, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0586 - val_loss: 0.0740\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0583\n","Epoch 238: val_loss improved from 0.07402 to 0.07377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0583 - val_loss: 0.0738\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0580\n","Epoch 239: val_loss improved from 0.07377 to 0.07353, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0580 - val_loss: 0.0735\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 240: val_loss improved from 0.07353 to 0.07329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0577 - val_loss: 0.0733\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0574\n","Epoch 241: val_loss improved from 0.07329 to 0.07306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0574 - val_loss: 0.0731\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 242: val_loss improved from 0.07306 to 0.07282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0571 - val_loss: 0.0728\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0568\n","Epoch 243: val_loss improved from 0.07282 to 0.07259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0568 - val_loss: 0.0726\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 244: val_loss improved from 0.07259 to 0.07237, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0565 - val_loss: 0.0724\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0562\n","Epoch 245: val_loss improved from 0.07237 to 0.07214, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0562 - val_loss: 0.0721\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 246: val_loss improved from 0.07214 to 0.07192, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0559 - val_loss: 0.0719\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0556\n","Epoch 247: val_loss improved from 0.07192 to 0.07170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0556 - val_loss: 0.0717\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0553\n","Epoch 248: val_loss improved from 0.07170 to 0.07148, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0553 - val_loss: 0.0715\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0550\n","Epoch 249: val_loss improved from 0.07148 to 0.07127, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0550 - val_loss: 0.0713\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0548\n","Epoch 250: val_loss improved from 0.07127 to 0.07106, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0548 - val_loss: 0.0711\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0545\n","Epoch 251: val_loss improved from 0.07106 to 0.07085, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0545 - val_loss: 0.0708\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 252: val_loss improved from 0.07085 to 0.07064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0542 - val_loss: 0.0706\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 253: val_loss improved from 0.07064 to 0.07043, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0540 - val_loss: 0.0704\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 254: val_loss improved from 0.07043 to 0.07023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0537 - val_loss: 0.0702\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 255: val_loss improved from 0.07023 to 0.07003, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0534 - val_loss: 0.0700\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0532\n","Epoch 256: val_loss improved from 0.07003 to 0.06983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0532 - val_loss: 0.0698\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0529\n","Epoch 257: val_loss improved from 0.06983 to 0.06964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0529 - val_loss: 0.0696\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0527\n","Epoch 258: val_loss improved from 0.06964 to 0.06945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0527 - val_loss: 0.0694\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 259: val_loss improved from 0.06945 to 0.06925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0524 - val_loss: 0.0693\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0522\n","Epoch 260: val_loss improved from 0.06925 to 0.06907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0522 - val_loss: 0.0691\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 261: val_loss improved from 0.06907 to 0.06888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0519 - val_loss: 0.0689\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0517\n","Epoch 262: val_loss improved from 0.06888 to 0.06870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0517 - val_loss: 0.0687\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 263: val_loss improved from 0.06870 to 0.06851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0514 - val_loss: 0.0685\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 264: val_loss improved from 0.06851 to 0.06833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0512 - val_loss: 0.0683\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0510\n","Epoch 265: val_loss improved from 0.06833 to 0.06816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0510 - val_loss: 0.0682\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0507\n","Epoch 266: val_loss improved from 0.06816 to 0.06798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0507 - val_loss: 0.0680\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0505\n","Epoch 267: val_loss improved from 0.06798 to 0.06781, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0505 - val_loss: 0.0678\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 268: val_loss improved from 0.06781 to 0.06764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0503 - val_loss: 0.0676\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 269: val_loss improved from 0.06764 to 0.06747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0500 - val_loss: 0.0675\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 270: val_loss improved from 0.06747 to 0.06730, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0498 - val_loss: 0.0673\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0496\n","Epoch 271: val_loss improved from 0.06730 to 0.06713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0496 - val_loss: 0.0671\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 272: val_loss improved from 0.06713 to 0.06697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0494 - val_loss: 0.0670\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0491\n","Epoch 273: val_loss improved from 0.06697 to 0.06681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0491 - val_loss: 0.0668\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 274: val_loss improved from 0.06681 to 0.06665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0489 - val_loss: 0.0666\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 275: val_loss improved from 0.06665 to 0.06649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0487 - val_loss: 0.0665\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0485\n","Epoch 276: val_loss improved from 0.06649 to 0.06634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0485 - val_loss: 0.0663\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 277: val_loss improved from 0.06634 to 0.06618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0483 - val_loss: 0.0662\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 278: val_loss improved from 0.06618 to 0.06603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0481 - val_loss: 0.0660\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 279: val_loss improved from 0.06603 to 0.06588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0479 - val_loss: 0.0659\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 280: val_loss improved from 0.06588 to 0.06573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0477 - val_loss: 0.0657\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0475\n","Epoch 281: val_loss improved from 0.06573 to 0.06558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0475 - val_loss: 0.0656\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0473\n","Epoch 282: val_loss improved from 0.06558 to 0.06544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0473 - val_loss: 0.0654\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 283: val_loss improved from 0.06544 to 0.06530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0471 - val_loss: 0.0653\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 284: val_loss improved from 0.06530 to 0.06516, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0469 - val_loss: 0.0652\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 285: val_loss improved from 0.06516 to 0.06502, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0467 - val_loss: 0.0650\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0465\n","Epoch 286: val_loss improved from 0.06502 to 0.06488, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0465 - val_loss: 0.0649\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0463\n","Epoch 287: val_loss improved from 0.06488 to 0.06474, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0463 - val_loss: 0.0647\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0461\n","Epoch 288: val_loss improved from 0.06474 to 0.06461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0461 - val_loss: 0.0646\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 289: val_loss improved from 0.06461 to 0.06447, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0460 - val_loss: 0.0645\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 290: val_loss improved from 0.06447 to 0.06434, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0458 - val_loss: 0.0643\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 291: val_loss improved from 0.06434 to 0.06421, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0456 - val_loss: 0.0642\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 292: val_loss improved from 0.06421 to 0.06408, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0454 - val_loss: 0.0641\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 293: val_loss improved from 0.06408 to 0.06396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0452 - val_loss: 0.0640\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 294: val_loss improved from 0.06396 to 0.06383, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0451 - val_loss: 0.0638\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0449\n","Epoch 295: val_loss improved from 0.06383 to 0.06371, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0449 - val_loss: 0.0637\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 296: val_loss improved from 0.06371 to 0.06359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0447 - val_loss: 0.0636\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 297: val_loss improved from 0.06359 to 0.06347, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0445 - val_loss: 0.0635\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 298: val_loss improved from 0.06347 to 0.06335, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0444 - val_loss: 0.0633\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 299: val_loss improved from 0.06335 to 0.06323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0442 - val_loss: 0.0632\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 300: val_loss improved from 0.06323 to 0.06311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0440 - val_loss: 0.0631\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 301: val_loss improved from 0.06311 to 0.06300, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0439 - val_loss: 0.0630\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 302: val_loss improved from 0.06300 to 0.06289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0437 - val_loss: 0.0629\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 303: val_loss improved from 0.06289 to 0.06277, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0436 - val_loss: 0.0628\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 304: val_loss improved from 0.06277 to 0.06266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0434 - val_loss: 0.0627\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 305: val_loss improved from 0.06266 to 0.06255, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0432 - val_loss: 0.0626\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 306: val_loss improved from 0.06255 to 0.06245, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0431 - val_loss: 0.0624\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 307: val_loss improved from 0.06245 to 0.06234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0429 - val_loss: 0.0623\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 308: val_loss improved from 0.06234 to 0.06223, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0428 - val_loss: 0.0622\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 309: val_loss improved from 0.06223 to 0.06213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0426 - val_loss: 0.0621\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 310: val_loss improved from 0.06213 to 0.06203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0425 - val_loss: 0.0620\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 311: val_loss improved from 0.06203 to 0.06193, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0423 - val_loss: 0.0619\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 312: val_loss improved from 0.06193 to 0.06183, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0422 - val_loss: 0.0618\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 313: val_loss improved from 0.06183 to 0.06173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0420 - val_loss: 0.0617\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 314: val_loss improved from 0.06173 to 0.06163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0418\n","Epoch 315: val_loss improved from 0.06163 to 0.06153, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0418 - val_loss: 0.0615\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 316: val_loss improved from 0.06153 to 0.06144, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0416 - val_loss: 0.0614\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 317: val_loss improved from 0.06144 to 0.06135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0415 - val_loss: 0.0613\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 318: val_loss improved from 0.06135 to 0.06125, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0413 - val_loss: 0.0613\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 319: val_loss improved from 0.06125 to 0.06116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0412 - val_loss: 0.0612\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 320: val_loss improved from 0.06116 to 0.06107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0411 - val_loss: 0.0611\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 321: val_loss improved from 0.06107 to 0.06098, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0409 - val_loss: 0.0610\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 322: val_loss improved from 0.06098 to 0.06089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0408 - val_loss: 0.0609\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 323: val_loss improved from 0.06089 to 0.06081, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0407 - val_loss: 0.0608\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 324: val_loss improved from 0.06081 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0406 - val_loss: 0.0607\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 325: val_loss improved from 0.06072 to 0.06064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0404 - val_loss: 0.0606\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 326: val_loss improved from 0.06064 to 0.06055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0403 - val_loss: 0.0606\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 327: val_loss improved from 0.06055 to 0.06047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 328: val_loss improved from 0.06047 to 0.06039, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0401 - val_loss: 0.0604\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 329: val_loss improved from 0.06039 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0399 - val_loss: 0.0603\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 330: val_loss improved from 0.06031 to 0.06023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0398 - val_loss: 0.0602\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 331: val_loss improved from 0.06023 to 0.06015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0397 - val_loss: 0.0601\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 332: val_loss improved from 0.06015 to 0.06007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 333: val_loss improved from 0.06007 to 0.06000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0395 - val_loss: 0.0600\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 334: val_loss improved from 0.06000 to 0.05992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0393 - val_loss: 0.0599\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 335: val_loss improved from 0.05992 to 0.05985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0392 - val_loss: 0.0598\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 336: val_loss improved from 0.05985 to 0.05977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0391 - val_loss: 0.0598\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 337: val_loss improved from 0.05977 to 0.05970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0390 - val_loss: 0.0597\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 338: val_loss improved from 0.05970 to 0.05963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0389 - val_loss: 0.0596\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 339: val_loss improved from 0.05963 to 0.05956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0388 - val_loss: 0.0596\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 340: val_loss improved from 0.05956 to 0.05949, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0387 - val_loss: 0.0595\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 341: val_loss improved from 0.05949 to 0.05942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0386 - val_loss: 0.0594\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 342: val_loss improved from 0.05942 to 0.05935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0384 - val_loss: 0.0594\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 343: val_loss improved from 0.05935 to 0.05928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0383 - val_loss: 0.0593\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 344: val_loss improved from 0.05928 to 0.05922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0382 - val_loss: 0.0592\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 345: val_loss improved from 0.05922 to 0.05915, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0381 - val_loss: 0.0592\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 346: val_loss improved from 0.05915 to 0.05909, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0380 - val_loss: 0.0591\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 347: val_loss improved from 0.05909 to 0.05903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0379 - val_loss: 0.0590\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 348: val_loss improved from 0.05903 to 0.05896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0378 - val_loss: 0.0590\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 349: val_loss improved from 0.05896 to 0.05890, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0377 - val_loss: 0.0589\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 350: val_loss improved from 0.05890 to 0.05884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 351: val_loss improved from 0.05884 to 0.05878, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0375 - val_loss: 0.0588\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 352: val_loss improved from 0.05878 to 0.05872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0374 - val_loss: 0.0587\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 353: val_loss improved from 0.05872 to 0.05866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0373 - val_loss: 0.0587\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 354: val_loss improved from 0.05866 to 0.05860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0372 - val_loss: 0.0586\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 355: val_loss improved from 0.05860 to 0.05855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0372 - val_loss: 0.0585\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 356: val_loss improved from 0.05855 to 0.05849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0371 - val_loss: 0.0585\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 357: val_loss improved from 0.05849 to 0.05844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0370 - val_loss: 0.0584\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 358: val_loss improved from 0.05844 to 0.05838, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 359: val_loss improved from 0.05838 to 0.05833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0368 - val_loss: 0.0583\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 360: val_loss improved from 0.05833 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0367 - val_loss: 0.0583\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 361: val_loss improved from 0.05827 to 0.05822, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0366 - val_loss: 0.0582\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 362: val_loss improved from 0.05822 to 0.05817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 363: val_loss improved from 0.05817 to 0.05812, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 364: val_loss improved from 0.05812 to 0.05807, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 365: val_loss improved from 0.05807 to 0.05802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0363 - val_loss: 0.0580\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 366: val_loss improved from 0.05802 to 0.05797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0362 - val_loss: 0.0580\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 367: val_loss improved from 0.05797 to 0.05792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0361 - val_loss: 0.0579\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 368: val_loss improved from 0.05792 to 0.05787, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0360 - val_loss: 0.0579\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 369: val_loss improved from 0.05787 to 0.05783, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 370: val_loss improved from 0.05783 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 371: val_loss improved from 0.05778 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0358 - val_loss: 0.0577\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 372: val_loss improved from 0.05773 to 0.05769, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 373: val_loss improved from 0.05769 to 0.05764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 374: val_loss improved from 0.05764 to 0.05760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 375: val_loss improved from 0.05760 to 0.05756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 376: val_loss improved from 0.05756 to 0.05752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 377: val_loss improved from 0.05752 to 0.05747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0353 - val_loss: 0.0575\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 378: val_loss improved from 0.05747 to 0.05743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0353 - val_loss: 0.0574\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 379: val_loss improved from 0.05743 to 0.05739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 380: val_loss improved from 0.05739 to 0.05735, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0351 - val_loss: 0.0574\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 381: val_loss improved from 0.05735 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 382: val_loss improved from 0.05731 to 0.05727, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 383: val_loss improved from 0.05727 to 0.05723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0349 - val_loss: 0.0572\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 384: val_loss improved from 0.05723 to 0.05720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 385: val_loss improved from 0.05720 to 0.05716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 386: val_loss improved from 0.05716 to 0.05712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 387: val_loss improved from 0.05712 to 0.05709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 388: val_loss improved from 0.05709 to 0.05705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 389: val_loss improved from 0.05705 to 0.05701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0345 - val_loss: 0.0570\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 390: val_loss improved from 0.05701 to 0.05698, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 391: val_loss improved from 0.05698 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0344 - val_loss: 0.0569\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 392: val_loss improved from 0.05695 to 0.05691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 393: val_loss improved from 0.05691 to 0.05688, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0342 - val_loss: 0.0569\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 394: val_loss improved from 0.05688 to 0.05685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0342 - val_loss: 0.0568\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 395: val_loss improved from 0.05685 to 0.05681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 396: val_loss improved from 0.05681 to 0.05678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 397: val_loss improved from 0.05678 to 0.05675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 398: val_loss improved from 0.05675 to 0.05672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 399: val_loss improved from 0.05672 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 400: val_loss improved from 0.05669 to 0.05666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 401: val_loss improved from 0.05666 to 0.05663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0338 - val_loss: 0.0566\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 402: val_loss improved from 0.05663 to 0.05660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 403: val_loss improved from 0.05660 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0336 - val_loss: 0.0566\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 404: val_loss improved from 0.05657 to 0.05655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 405: val_loss improved from 0.05655 to 0.05652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 406: val_loss improved from 0.05652 to 0.05649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 407: val_loss improved from 0.05649 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 408: val_loss improved from 0.05646 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0334 - val_loss: 0.0564\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 409: val_loss improved from 0.05644 to 0.05641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 410: val_loss improved from 0.05641 to 0.05639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 411: val_loss improved from 0.05639 to 0.05636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 412: val_loss improved from 0.05636 to 0.05634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0332 - val_loss: 0.0563\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 413: val_loss improved from 0.05634 to 0.05631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 414: val_loss improved from 0.05631 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 415: val_loss improved from 0.05629 to 0.05626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 416: val_loss improved from 0.05626 to 0.05624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0330 - val_loss: 0.0562\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 417: val_loss improved from 0.05624 to 0.05622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 418: val_loss improved from 0.05622 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 419: val_loss improved from 0.05620 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 420: val_loss improved from 0.05617 to 0.05615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 421: val_loss improved from 0.05615 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 422: val_loss improved from 0.05613 to 0.05611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 423: val_loss improved from 0.05611 to 0.05609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 424: val_loss improved from 0.05609 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 425: val_loss improved from 0.05607 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 426: val_loss improved from 0.05605 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 427: val_loss improved from 0.05603 to 0.05601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 428: val_loss improved from 0.05601 to 0.05599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 429: val_loss improved from 0.05599 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 430: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 431: val_loss improved from 0.05595 to 0.05594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0323 - val_loss: 0.0559\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 432: val_loss improved from 0.05594 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 433: val_loss improved from 0.05592 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 434: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 435: val_loss improved from 0.05588 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 436: val_loss improved from 0.05587 to 0.05585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0321 - val_loss: 0.0558\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 437: val_loss improved from 0.05585 to 0.05583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 438: val_loss improved from 0.05583 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 439: val_loss improved from 0.05582 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 440: val_loss improved from 0.05580 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 441: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 442: val_loss improved from 0.05577 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 443: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 444: val_loss improved from 0.05574 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 146ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 445: val_loss improved from 0.05573 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 446: val_loss improved from 0.05571 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 447: val_loss improved from 0.05570 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 448: val_loss improved from 0.05569 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 449: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 450: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 451: val_loss improved from 0.05565 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 452: val_loss improved from 0.05564 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 453: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 454: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 455: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 456: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 457: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 458: val_loss improved from 0.05557 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 459: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 460: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 461: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 462: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 463: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 464: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 465: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 466: val_loss improved from 0.05548 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 467: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 468: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 469: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 470: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 471: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 472: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 473: val_loss improved from 0.05542 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 474: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 475: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 476: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 477: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 478: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 479: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 480: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 481: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 482: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 483: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 484: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 485: val_loss improved from 0.05534 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 486: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 487: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 488: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 489: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 490: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 491: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 492: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 493: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 494: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 495: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 496: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 497: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 498: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 499: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 500: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 501: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 502: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 503: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 504: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 505: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 506: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 507: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 508: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 509: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 510: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 511: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 512: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 513: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 514: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 515: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 516: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 517: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 518: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 519: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 520: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 521: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 522: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 523: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 524: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 525: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 526: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 527: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 528: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 529: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 530: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 531: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 532/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 532: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 533/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 533: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 534/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 534: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 535/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 535: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 536/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 536: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 537/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 537: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 538/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 538: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 539/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 539: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 540/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 540: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 541/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 541: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 542/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 542: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 543/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 543: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 544/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 544: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 545/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 545: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 546/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 546: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 547/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 547: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 548/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 548: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 549/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 549: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 550/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 550: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 551/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 551: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 552/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 552: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 553/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 553: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 554/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 554: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 555/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 555: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 556/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 556: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 557/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 557: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 558/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 558: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 559/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 559: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 560/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 560: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 561/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 561: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 562/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 562: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 563/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 563: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 564/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 564: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 565/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 565: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 566/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 566: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 567/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 567: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 568/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 568: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 569/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 569: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 570/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 570: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 571/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 571: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 572/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 572: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 573/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 573: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 574/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 574: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 575/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 575: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 576/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 576: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 577/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 577: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 578/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 578: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 579/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 579: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 580/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 580: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 581/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 581: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 582/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 582: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 583/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 583: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 584/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 584: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 585/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 585: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 586/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 586: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 587/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 587: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 588/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 588: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 589/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 589: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 590/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 590: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 591/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 591: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 592/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 592: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 593/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 593: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 594/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 594: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 595/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 595: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 596/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 596: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 597/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 597: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 598/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 598: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 599/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 599: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 600/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 600: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 601/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 601: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 602/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 602: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 603/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 603: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 604/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 604: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 605/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 605: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 606/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 606: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 607/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 607: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 608/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 608: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 609/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 609: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 610/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 610: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 611/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 611: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 612/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 612: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 613/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 613: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 614/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 614: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 615/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 615: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 616/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 616: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 617/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 617: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 618/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 618: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 619/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 619: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 620/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 620: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 621/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 621: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 622/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 622: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 623/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 623: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 624/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 624: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 625/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 625: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 626/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 626: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 627/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 627: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 628/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 628: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 629/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 629: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 630/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 630: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 631/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 631: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 632/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 632: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 633/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 633: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 634/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 634: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 635/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 635: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 636/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 636: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 637/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 637: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0287 - val_loss: 0.0554\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0734\n","loss_and_metrics : 0.0733594298362732\n","1/1 [==============================] - 0s 78ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=5, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=5, activataion=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}