{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNLfAo09/b4qKEcrbZ6Fji"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3DHbFJugy7ck","executionInfo":{"status":"ok","timestamp":1695016071673,"user_tz":-540,"elapsed":96117,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"c3c999af-309e-4eb4-e271-f9ce62188836"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3663\n","Epoch 1: val_loss improved from inf to 0.34936, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3663 - val_loss: 0.3494\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3629\n","Epoch 2: val_loss improved from 0.34936 to 0.34619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3629 - val_loss: 0.3462\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3595\n","Epoch 3: val_loss improved from 0.34619 to 0.34306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3595 - val_loss: 0.3431\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3562\n","Epoch 4: val_loss improved from 0.34306 to 0.33996, saving model to hl5_0100.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 69ms/step - loss: 0.3562 - val_loss: 0.3400\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3529\n","Epoch 5: val_loss improved from 0.33996 to 0.33689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3529 - val_loss: 0.3369\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3496\n","Epoch 6: val_loss improved from 0.33689 to 0.33386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3496 - val_loss: 0.3339\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3464\n","Epoch 7: val_loss improved from 0.33386 to 0.33086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3464 - val_loss: 0.3309\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3432\n","Epoch 8: val_loss improved from 0.33086 to 0.32789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3432 - val_loss: 0.3279\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3400\n","Epoch 9: val_loss improved from 0.32789 to 0.32495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3400 - val_loss: 0.3249\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3368\n","Epoch 10: val_loss improved from 0.32495 to 0.32204, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3368 - val_loss: 0.3220\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3337\n","Epoch 11: val_loss improved from 0.32204 to 0.31916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3337 - val_loss: 0.3192\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3306\n","Epoch 12: val_loss improved from 0.31916 to 0.31631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3306 - val_loss: 0.3163\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3276\n","Epoch 13: val_loss improved from 0.31631 to 0.31349, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3276 - val_loss: 0.3135\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3246\n","Epoch 14: val_loss improved from 0.31349 to 0.31070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3246 - val_loss: 0.3107\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3216\n","Epoch 15: val_loss improved from 0.31070 to 0.30794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3216 - val_loss: 0.3079\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3187\n","Epoch 16: val_loss improved from 0.30794 to 0.30521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3187 - val_loss: 0.3052\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3157\n","Epoch 17: val_loss improved from 0.30521 to 0.30251, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3157 - val_loss: 0.3025\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3128\n","Epoch 18: val_loss improved from 0.30251 to 0.29983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3128 - val_loss: 0.2998\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3099\n","Epoch 19: val_loss improved from 0.29983 to 0.29718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3099 - val_loss: 0.2972\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3071\n","Epoch 20: val_loss improved from 0.29718 to 0.29456, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3071 - val_loss: 0.2946\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3043\n","Epoch 21: val_loss improved from 0.29456 to 0.29197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3043 - val_loss: 0.2920\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3015\n","Epoch 22: val_loss improved from 0.29197 to 0.28941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3015 - val_loss: 0.2894\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2987\n","Epoch 23: val_loss improved from 0.28941 to 0.28687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2987 - val_loss: 0.2869\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2960\n","Epoch 24: val_loss improved from 0.28687 to 0.28436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2960 - val_loss: 0.2844\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2932\n","Epoch 25: val_loss improved from 0.28436 to 0.28187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2932 - val_loss: 0.2819\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2905\n","Epoch 26: val_loss improved from 0.28187 to 0.27941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2905 - val_loss: 0.2794\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2880\n","Epoch 27: val_loss improved from 0.27941 to 0.27697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2880 - val_loss: 0.2770\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2853\n","Epoch 28: val_loss improved from 0.27697 to 0.27456, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2853 - val_loss: 0.2746\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2828\n","Epoch 29: val_loss improved from 0.27456 to 0.27218, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2828 - val_loss: 0.2722\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2802\n","Epoch 30: val_loss improved from 0.27218 to 0.26982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2802 - val_loss: 0.2698\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2777\n","Epoch 31: val_loss improved from 0.26982 to 0.26749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2777 - val_loss: 0.2675\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2750\n","Epoch 32: val_loss improved from 0.26749 to 0.26518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2750 - val_loss: 0.2652\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2726\n","Epoch 33: val_loss improved from 0.26518 to 0.26289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2726 - val_loss: 0.2629\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2701\n","Epoch 34: val_loss improved from 0.26289 to 0.26063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2701 - val_loss: 0.2606\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2677\n","Epoch 35: val_loss improved from 0.26063 to 0.25839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2677 - val_loss: 0.2584\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2653\n","Epoch 36: val_loss improved from 0.25839 to 0.25617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2653 - val_loss: 0.2562\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2629\n","Epoch 37: val_loss improved from 0.25617 to 0.25398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2629 - val_loss: 0.2540\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2606\n","Epoch 38: val_loss improved from 0.25398 to 0.25181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2606 - val_loss: 0.2518\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2582\n","Epoch 39: val_loss improved from 0.25181 to 0.24967, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2582 - val_loss: 0.2497\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2561\n","Epoch 40: val_loss improved from 0.24967 to 0.24755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2561 - val_loss: 0.2475\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2536\n","Epoch 41: val_loss improved from 0.24755 to 0.24545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2536 - val_loss: 0.2454\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2512\n","Epoch 42: val_loss improved from 0.24545 to 0.24337, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2512 - val_loss: 0.2434\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2490\n","Epoch 43: val_loss improved from 0.24337 to 0.24131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2490 - val_loss: 0.2413\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2470\n","Epoch 44: val_loss improved from 0.24131 to 0.23928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2470 - val_loss: 0.2393\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2448\n","Epoch 45: val_loss improved from 0.23928 to 0.23726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2448 - val_loss: 0.2373\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2425\n","Epoch 46: val_loss improved from 0.23726 to 0.23527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2425 - val_loss: 0.2353\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2403\n","Epoch 47: val_loss improved from 0.23527 to 0.23330, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2403 - val_loss: 0.2333\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2381\n","Epoch 48: val_loss improved from 0.23330 to 0.23135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2381 - val_loss: 0.2313\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2361\n","Epoch 49: val_loss improved from 0.23135 to 0.22942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2361 - val_loss: 0.2294\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2339\n","Epoch 50: val_loss improved from 0.22942 to 0.22751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2339 - val_loss: 0.2275\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2318\n","Epoch 51: val_loss improved from 0.22751 to 0.22561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2318 - val_loss: 0.2256\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2299\n","Epoch 52: val_loss improved from 0.22561 to 0.22374, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2299 - val_loss: 0.2237\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2276\n","Epoch 53: val_loss improved from 0.22374 to 0.22189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2276 - val_loss: 0.2219\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2258\n","Epoch 54: val_loss improved from 0.22189 to 0.22006, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2258 - val_loss: 0.2201\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2236\n","Epoch 55: val_loss improved from 0.22006 to 0.21825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2236 - val_loss: 0.2182\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2217\n","Epoch 56: val_loss improved from 0.21825 to 0.21645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2217 - val_loss: 0.2165\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2197\n","Epoch 57: val_loss improved from 0.21645 to 0.21467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2197 - val_loss: 0.2147\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2179\n","Epoch 58: val_loss improved from 0.21467 to 0.21292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2179 - val_loss: 0.2129\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2160\n","Epoch 59: val_loss improved from 0.21292 to 0.21118, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2160 - val_loss: 0.2112\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2140\n","Epoch 60: val_loss improved from 0.21118 to 0.20946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2140 - val_loss: 0.2095\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2123\n","Epoch 61: val_loss improved from 0.20946 to 0.20776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2123 - val_loss: 0.2078\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 62: val_loss improved from 0.20776 to 0.20608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2102 - val_loss: 0.2061\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 63: val_loss improved from 0.20608 to 0.20441, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2085 - val_loss: 0.2044\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2066\n","Epoch 64: val_loss improved from 0.20441 to 0.20276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2066 - val_loss: 0.2028\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2048\n","Epoch 65: val_loss improved from 0.20276 to 0.20113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2048 - val_loss: 0.2011\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2029\n","Epoch 66: val_loss improved from 0.20113 to 0.19952, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2029 - val_loss: 0.1995\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2013\n","Epoch 67: val_loss improved from 0.19952 to 0.19792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2013 - val_loss: 0.1979\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1996\n","Epoch 68: val_loss improved from 0.19792 to 0.19634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1996 - val_loss: 0.1963\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1977\n","Epoch 69: val_loss improved from 0.19634 to 0.19477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1977 - val_loss: 0.1948\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1959\n","Epoch 70: val_loss improved from 0.19477 to 0.19323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1959 - val_loss: 0.1932\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1945\n","Epoch 71: val_loss improved from 0.19323 to 0.19170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1945 - val_loss: 0.1917\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1927\n","Epoch 72: val_loss improved from 0.19170 to 0.19018, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1927 - val_loss: 0.1902\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1910\n","Epoch 73: val_loss improved from 0.19018 to 0.18868, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1910 - val_loss: 0.1887\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1895\n","Epoch 74: val_loss improved from 0.18868 to 0.18720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1895 - val_loss: 0.1872\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1877\n","Epoch 75: val_loss improved from 0.18720 to 0.18574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1877 - val_loss: 0.1857\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1863\n","Epoch 76: val_loss improved from 0.18574 to 0.18428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1863 - val_loss: 0.1843\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1845\n","Epoch 77: val_loss improved from 0.18428 to 0.18285, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1845 - val_loss: 0.1828\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1828\n","Epoch 78: val_loss improved from 0.18285 to 0.18143, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1828 - val_loss: 0.1814\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1815\n","Epoch 79: val_loss improved from 0.18143 to 0.18002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1815 - val_loss: 0.1800\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1797\n","Epoch 80: val_loss improved from 0.18002 to 0.17863, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1797 - val_loss: 0.1786\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1783\n","Epoch 81: val_loss improved from 0.17863 to 0.17725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1783 - val_loss: 0.1773\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1767\n","Epoch 82: val_loss improved from 0.17725 to 0.17589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1767 - val_loss: 0.1759\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1752\n","Epoch 83: val_loss improved from 0.17589 to 0.17454, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1752 - val_loss: 0.1745\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1739\n","Epoch 84: val_loss improved from 0.17454 to 0.17321, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1739 - val_loss: 0.1732\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1725\n","Epoch 85: val_loss improved from 0.17321 to 0.17189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1725 - val_loss: 0.1719\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1710\n","Epoch 86: val_loss improved from 0.17189 to 0.17058, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1710 - val_loss: 0.1706\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1694\n","Epoch 87: val_loss improved from 0.17058 to 0.16929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1694 - val_loss: 0.1693\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1680\n","Epoch 88: val_loss improved from 0.16929 to 0.16802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1680 - val_loss: 0.1680\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1665\n","Epoch 89: val_loss improved from 0.16802 to 0.16675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1665 - val_loss: 0.1668\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1652\n","Epoch 90: val_loss improved from 0.16675 to 0.16550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1652 - val_loss: 0.1655\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1639\n","Epoch 91: val_loss improved from 0.16550 to 0.16426, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1639 - val_loss: 0.1643\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1623\n","Epoch 92: val_loss improved from 0.16426 to 0.16304, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1623 - val_loss: 0.1630\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1609\n","Epoch 93: val_loss improved from 0.16304 to 0.16183, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1609 - val_loss: 0.1618\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1600\n","Epoch 94: val_loss improved from 0.16183 to 0.16063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1600 - val_loss: 0.1606\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1585\n","Epoch 95: val_loss improved from 0.16063 to 0.15944, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1585 - val_loss: 0.1594\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1572\n","Epoch 96: val_loss improved from 0.15944 to 0.15827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1572 - val_loss: 0.1583\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1559\n","Epoch 97: val_loss improved from 0.15827 to 0.15711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1559 - val_loss: 0.1571\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1545\n","Epoch 98: val_loss improved from 0.15711 to 0.15596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1545 - val_loss: 0.1560\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1532\n","Epoch 99: val_loss improved from 0.15596 to 0.15483, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1532 - val_loss: 0.1548\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1519\n","Epoch 100: val_loss improved from 0.15483 to 0.15370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1519 - val_loss: 0.1537\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1506\n","Epoch 101: val_loss improved from 0.15370 to 0.15259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1506 - val_loss: 0.1526\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1495\n","Epoch 102: val_loss improved from 0.15259 to 0.15149, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1495 - val_loss: 0.1515\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1481\n","Epoch 103: val_loss improved from 0.15149 to 0.15040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1481 - val_loss: 0.1504\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1468\n","Epoch 104: val_loss improved from 0.15040 to 0.14932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1468 - val_loss: 0.1493\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1459\n","Epoch 105: val_loss improved from 0.14932 to 0.14826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1459 - val_loss: 0.1483\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1447\n","Epoch 106: val_loss improved from 0.14826 to 0.14720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1447 - val_loss: 0.1472\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1434\n","Epoch 107: val_loss improved from 0.14720 to 0.14616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1434 - val_loss: 0.1462\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1425\n","Epoch 108: val_loss improved from 0.14616 to 0.14513, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1425 - val_loss: 0.1451\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1409\n","Epoch 109: val_loss improved from 0.14513 to 0.14411, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.1409 - val_loss: 0.1441\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1400\n","Epoch 110: val_loss improved from 0.14411 to 0.14310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.1400 - val_loss: 0.1431\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1388\n","Epoch 111: val_loss improved from 0.14310 to 0.14210, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1388 - val_loss: 0.1421\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1378\n","Epoch 112: val_loss improved from 0.14210 to 0.14111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1378 - val_loss: 0.1411\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1366\n","Epoch 113: val_loss improved from 0.14111 to 0.14013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1366 - val_loss: 0.1401\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1357\n","Epoch 114: val_loss improved from 0.14013 to 0.13917, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1357 - val_loss: 0.1392\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1344\n","Epoch 115: val_loss improved from 0.13917 to 0.13821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1344 - val_loss: 0.1382\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1338\n","Epoch 116: val_loss improved from 0.13821 to 0.13726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1338 - val_loss: 0.1373\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1323\n","Epoch 117: val_loss improved from 0.13726 to 0.13633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1323 - val_loss: 0.1363\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1312\n","Epoch 118: val_loss improved from 0.13633 to 0.13540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.1312 - val_loss: 0.1354\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1300\n","Epoch 119: val_loss improved from 0.13540 to 0.13449, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.1300 - val_loss: 0.1345\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1291\n","Epoch 120: val_loss improved from 0.13449 to 0.13358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1291 - val_loss: 0.1336\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1281\n","Epoch 121: val_loss improved from 0.13358 to 0.13268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1281 - val_loss: 0.1327\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1270\n","Epoch 122: val_loss improved from 0.13268 to 0.13179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.1270 - val_loss: 0.1318\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1260\n","Epoch 123: val_loss improved from 0.13179 to 0.13092, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1260 - val_loss: 0.1309\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1252\n","Epoch 124: val_loss improved from 0.13092 to 0.13005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1252 - val_loss: 0.1300\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1240\n","Epoch 125: val_loss improved from 0.13005 to 0.12919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1240 - val_loss: 0.1292\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1230\n","Epoch 126: val_loss improved from 0.12919 to 0.12834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.1230 - val_loss: 0.1283\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1223\n","Epoch 127: val_loss improved from 0.12834 to 0.12750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.1223 - val_loss: 0.1275\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1212\n","Epoch 128: val_loss improved from 0.12750 to 0.12666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1212 - val_loss: 0.1267\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1203\n","Epoch 129: val_loss improved from 0.12666 to 0.12584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1203 - val_loss: 0.1258\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1192\n","Epoch 130: val_loss improved from 0.12584 to 0.12503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.1192 - val_loss: 0.1250\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1182\n","Epoch 131: val_loss improved from 0.12503 to 0.12422, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.1182 - val_loss: 0.1242\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1173\n","Epoch 132: val_loss improved from 0.12422 to 0.12342, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.1173 - val_loss: 0.1234\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1165\n","Epoch 133: val_loss improved from 0.12342 to 0.12264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.1165 - val_loss: 0.1226\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1157\n","Epoch 134: val_loss improved from 0.12264 to 0.12186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1157 - val_loss: 0.1219\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1148\n","Epoch 135: val_loss improved from 0.12186 to 0.12108, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1148 - val_loss: 0.1211\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1140\n","Epoch 136: val_loss improved from 0.12108 to 0.12032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1140 - val_loss: 0.1203\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1129\n","Epoch 137: val_loss improved from 0.12032 to 0.11957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1129 - val_loss: 0.1196\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1120\n","Epoch 138: val_loss improved from 0.11957 to 0.11882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1120 - val_loss: 0.1188\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1114\n","Epoch 139: val_loss improved from 0.11882 to 0.11808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1114 - val_loss: 0.1181\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1105\n","Epoch 140: val_loss improved from 0.11808 to 0.11735, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1105 - val_loss: 0.1174\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1096\n","Epoch 141: val_loss improved from 0.11735 to 0.11663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1096 - val_loss: 0.1166\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1089\n","Epoch 142: val_loss improved from 0.11663 to 0.11592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1089 - val_loss: 0.1159\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1081\n","Epoch 143: val_loss improved from 0.11592 to 0.11521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1081 - val_loss: 0.1152\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1070\n","Epoch 144: val_loss improved from 0.11521 to 0.11451, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1070 - val_loss: 0.1145\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1065\n","Epoch 145: val_loss improved from 0.11451 to 0.11382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1065 - val_loss: 0.1138\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1055\n","Epoch 146: val_loss improved from 0.11382 to 0.11313, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1055 - val_loss: 0.1131\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1047\n","Epoch 147: val_loss improved from 0.11313 to 0.11246, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1047 - val_loss: 0.1125\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1040\n","Epoch 148: val_loss improved from 0.11246 to 0.11179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1040 - val_loss: 0.1118\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1032\n","Epoch 149: val_loss improved from 0.11179 to 0.11112, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1032 - val_loss: 0.1111\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1029\n","Epoch 150: val_loss improved from 0.11112 to 0.11047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1029 - val_loss: 0.1105\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1018\n","Epoch 151: val_loss improved from 0.11047 to 0.10982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1018 - val_loss: 0.1098\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1009\n","Epoch 152: val_loss improved from 0.10982 to 0.10918, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1009 - val_loss: 0.1092\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1006\n","Epoch 153: val_loss improved from 0.10918 to 0.10855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1006 - val_loss: 0.1085\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0995\n","Epoch 154: val_loss improved from 0.10855 to 0.10792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0995 - val_loss: 0.1079\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0989\n","Epoch 155: val_loss improved from 0.10792 to 0.10730, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0989 - val_loss: 0.1073\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0979\n","Epoch 156: val_loss improved from 0.10730 to 0.10669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0979 - val_loss: 0.1067\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0974\n","Epoch 157: val_loss improved from 0.10669 to 0.10608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0974 - val_loss: 0.1061\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0967\n","Epoch 158: val_loss improved from 0.10608 to 0.10548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0967 - val_loss: 0.1055\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0958\n","Epoch 159: val_loss improved from 0.10548 to 0.10489, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0958 - val_loss: 0.1049\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0952\n","Epoch 160: val_loss improved from 0.10489 to 0.10430, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0952 - val_loss: 0.1043\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0946\n","Epoch 161: val_loss improved from 0.10430 to 0.10372, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0946 - val_loss: 0.1037\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0937\n","Epoch 162: val_loss improved from 0.10372 to 0.10315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0937 - val_loss: 0.1031\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0934\n","Epoch 163: val_loss improved from 0.10315 to 0.10258, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0934 - val_loss: 0.1026\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0924\n","Epoch 164: val_loss improved from 0.10258 to 0.10202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0924 - val_loss: 0.1020\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0920\n","Epoch 165: val_loss improved from 0.10202 to 0.10146, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0920 - val_loss: 0.1015\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0913\n","Epoch 166: val_loss improved from 0.10146 to 0.10091, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0913 - val_loss: 0.1009\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0906\n","Epoch 167: val_loss improved from 0.10091 to 0.10037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0906 - val_loss: 0.1004\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0901\n","Epoch 168: val_loss improved from 0.10037 to 0.09983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0901 - val_loss: 0.0998\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0894\n","Epoch 169: val_loss improved from 0.09983 to 0.09930, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0894 - val_loss: 0.0993\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0889\n","Epoch 170: val_loss improved from 0.09930 to 0.09877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0889 - val_loss: 0.0988\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0881\n","Epoch 171: val_loss improved from 0.09877 to 0.09825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0881 - val_loss: 0.0983\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0874\n","Epoch 172: val_loss improved from 0.09825 to 0.09774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0874 - val_loss: 0.0977\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0870\n","Epoch 173: val_loss improved from 0.09774 to 0.09723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0870 - val_loss: 0.0972\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0864\n","Epoch 174: val_loss improved from 0.09723 to 0.09673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0864 - val_loss: 0.0967\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0856\n","Epoch 175: val_loss improved from 0.09673 to 0.09623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0856 - val_loss: 0.0962\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0851\n","Epoch 176: val_loss improved from 0.09623 to 0.09574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0851 - val_loss: 0.0957\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0845\n","Epoch 177: val_loss improved from 0.09574 to 0.09525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0845 - val_loss: 0.0953\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0839\n","Epoch 178: val_loss improved from 0.09525 to 0.09477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0839 - val_loss: 0.0948\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0834\n","Epoch 179: val_loss improved from 0.09477 to 0.09429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0834 - val_loss: 0.0943\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0830\n","Epoch 180: val_loss improved from 0.09429 to 0.09382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0830 - val_loss: 0.0938\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0823\n","Epoch 181: val_loss improved from 0.09382 to 0.09336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0823 - val_loss: 0.0934\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0816\n","Epoch 182: val_loss improved from 0.09336 to 0.09290, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0816 - val_loss: 0.0929\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0811\n","Epoch 183: val_loss improved from 0.09290 to 0.09244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0811 - val_loss: 0.0924\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0812\n","Epoch 184: val_loss improved from 0.09244 to 0.09199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0812 - val_loss: 0.0920\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0800\n","Epoch 185: val_loss improved from 0.09199 to 0.09155, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0800 - val_loss: 0.0915\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0796\n","Epoch 186: val_loss improved from 0.09155 to 0.09111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0796 - val_loss: 0.0911\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 187: val_loss improved from 0.09111 to 0.09067, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0790 - val_loss: 0.0907\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0784\n","Epoch 188: val_loss improved from 0.09067 to 0.09024, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0784 - val_loss: 0.0902\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0781\n","Epoch 189: val_loss improved from 0.09024 to 0.08981, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0781 - val_loss: 0.0898\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0774\n","Epoch 190: val_loss improved from 0.08981 to 0.08939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0774 - val_loss: 0.0894\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0769\n","Epoch 191: val_loss improved from 0.08939 to 0.08898, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0769 - val_loss: 0.0890\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 192: val_loss improved from 0.08898 to 0.08856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0766 - val_loss: 0.0886\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 193: val_loss improved from 0.08856 to 0.08816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0761 - val_loss: 0.0882\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0754\n","Epoch 194: val_loss improved from 0.08816 to 0.08775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0754 - val_loss: 0.0878\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0752\n","Epoch 195: val_loss improved from 0.08775 to 0.08736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0752 - val_loss: 0.0874\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0749\n","Epoch 196: val_loss improved from 0.08736 to 0.08696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0749 - val_loss: 0.0870\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 197: val_loss improved from 0.08696 to 0.08657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0744 - val_loss: 0.0866\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0735\n","Epoch 198: val_loss improved from 0.08657 to 0.08619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0735 - val_loss: 0.0862\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0736\n","Epoch 199: val_loss improved from 0.08619 to 0.08581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0736 - val_loss: 0.0858\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0728\n","Epoch 200: val_loss improved from 0.08581 to 0.08543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0728 - val_loss: 0.0854\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0724\n","Epoch 201: val_loss improved from 0.08543 to 0.08506, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0724 - val_loss: 0.0851\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0716\n","Epoch 202: val_loss improved from 0.08506 to 0.08469, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0716 - val_loss: 0.0847\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0713\n","Epoch 203: val_loss improved from 0.08469 to 0.08433, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0713 - val_loss: 0.0843\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0711\n","Epoch 204: val_loss improved from 0.08433 to 0.08397, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0711 - val_loss: 0.0840\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0703\n","Epoch 205: val_loss improved from 0.08397 to 0.08361, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0703 - val_loss: 0.0836\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0702\n","Epoch 206: val_loss improved from 0.08361 to 0.08326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0702 - val_loss: 0.0833\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0699\n","Epoch 207: val_loss improved from 0.08326 to 0.08291, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0699 - val_loss: 0.0829\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 208: val_loss improved from 0.08291 to 0.08257, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0691 - val_loss: 0.0826\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0689\n","Epoch 209: val_loss improved from 0.08257 to 0.08223, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0689 - val_loss: 0.0822\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0682\n","Epoch 210: val_loss improved from 0.08223 to 0.08189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0682 - val_loss: 0.0819\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0679\n","Epoch 211: val_loss improved from 0.08189 to 0.08156, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0679 - val_loss: 0.0816\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0674\n","Epoch 212: val_loss improved from 0.08156 to 0.08123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0674 - val_loss: 0.0812\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0673\n","Epoch 213: val_loss improved from 0.08123 to 0.08090, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0673 - val_loss: 0.0809\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0668\n","Epoch 214: val_loss improved from 0.08090 to 0.08058, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0668 - val_loss: 0.0806\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0668\n","Epoch 215: val_loss improved from 0.08058 to 0.08027, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0668 - val_loss: 0.0803\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0658\n","Epoch 216: val_loss improved from 0.08027 to 0.07995, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0658 - val_loss: 0.0800\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 217: val_loss improved from 0.07995 to 0.07964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0657 - val_loss: 0.0796\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0652\n","Epoch 218: val_loss improved from 0.07964 to 0.07933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0652 - val_loss: 0.0793\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0649\n","Epoch 219: val_loss improved from 0.07933 to 0.07903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0649 - val_loss: 0.0790\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 220: val_loss improved from 0.07903 to 0.07873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0648 - val_loss: 0.0787\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 221: val_loss improved from 0.07873 to 0.07843, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0641 - val_loss: 0.0784\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0639\n","Epoch 222: val_loss improved from 0.07843 to 0.07814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0639 - val_loss: 0.0781\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0637\n","Epoch 223: val_loss improved from 0.07814 to 0.07785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0637 - val_loss: 0.0778\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0632\n","Epoch 224: val_loss improved from 0.07785 to 0.07756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0632 - val_loss: 0.0776\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0626\n","Epoch 225: val_loss improved from 0.07756 to 0.07728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0626 - val_loss: 0.0773\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0624\n","Epoch 226: val_loss improved from 0.07728 to 0.07700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0624 - val_loss: 0.0770\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0624\n","Epoch 227: val_loss improved from 0.07700 to 0.07672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0624 - val_loss: 0.0767\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0618\n","Epoch 228: val_loss improved from 0.07672 to 0.07645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0618 - val_loss: 0.0764\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0615\n","Epoch 229: val_loss improved from 0.07645 to 0.07618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0615 - val_loss: 0.0762\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0610\n","Epoch 230: val_loss improved from 0.07618 to 0.07591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0610 - val_loss: 0.0759\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 231: val_loss improved from 0.07591 to 0.07565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0608 - val_loss: 0.0756\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0603\n","Epoch 232: val_loss improved from 0.07565 to 0.07538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0603 - val_loss: 0.0754\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0598\n","Epoch 233: val_loss improved from 0.07538 to 0.07513, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0598 - val_loss: 0.0751\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0597\n","Epoch 234: val_loss improved from 0.07513 to 0.07487, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0597 - val_loss: 0.0749\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 235: val_loss improved from 0.07487 to 0.07462, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0600 - val_loss: 0.0746\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 236: val_loss improved from 0.07462 to 0.07437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0589 - val_loss: 0.0744\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0588\n","Epoch 237: val_loss improved from 0.07437 to 0.07412, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0588 - val_loss: 0.0741\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0581\n","Epoch 238: val_loss improved from 0.07412 to 0.07388, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0581 - val_loss: 0.0739\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0583\n","Epoch 239: val_loss improved from 0.07388 to 0.07364, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0583 - val_loss: 0.0736\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 240: val_loss improved from 0.07364 to 0.07340, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0577 - val_loss: 0.0734\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0574\n","Epoch 241: val_loss improved from 0.07340 to 0.07316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0574 - val_loss: 0.0732\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 242: val_loss improved from 0.07316 to 0.07293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0569 - val_loss: 0.0729\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 243: val_loss improved from 0.07293 to 0.07270, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0570 - val_loss: 0.0727\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0567\n","Epoch 244: val_loss improved from 0.07270 to 0.07247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0567 - val_loss: 0.0725\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 245: val_loss improved from 0.07247 to 0.07224, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0565 - val_loss: 0.0722\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0561\n","Epoch 246: val_loss improved from 0.07224 to 0.07202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0561 - val_loss: 0.0720\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 247: val_loss improved from 0.07202 to 0.07180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0558 - val_loss: 0.0718\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0555\n","Epoch 248: val_loss improved from 0.07180 to 0.07159, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0555 - val_loss: 0.0716\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 249: val_loss improved from 0.07159 to 0.07137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0557 - val_loss: 0.0714\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 250: val_loss improved from 0.07137 to 0.07116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0551 - val_loss: 0.0712\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0546\n","Epoch 251: val_loss improved from 0.07116 to 0.07095, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0546 - val_loss: 0.0709\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 252: val_loss improved from 0.07095 to 0.07074, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0542 - val_loss: 0.0707\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 253: val_loss improved from 0.07074 to 0.07054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0540 - val_loss: 0.0705\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 254: val_loss improved from 0.07054 to 0.07033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0537 - val_loss: 0.0703\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0535\n","Epoch 255: val_loss improved from 0.07033 to 0.07013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0535 - val_loss: 0.0701\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 256: val_loss improved from 0.07013 to 0.06994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0536 - val_loss: 0.0699\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0529\n","Epoch 257: val_loss improved from 0.06994 to 0.06974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0529 - val_loss: 0.0697\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0531\n","Epoch 258: val_loss improved from 0.06974 to 0.06955, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0531 - val_loss: 0.0695\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0526\n","Epoch 259: val_loss improved from 0.06955 to 0.06936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0526 - val_loss: 0.0694\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0523\n","Epoch 260: val_loss improved from 0.06936 to 0.06917, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0523 - val_loss: 0.0692\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0522\n","Epoch 261: val_loss improved from 0.06917 to 0.06898, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0522 - val_loss: 0.0690\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0520\n","Epoch 262: val_loss improved from 0.06898 to 0.06880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0520 - val_loss: 0.0688\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 263: val_loss improved from 0.06880 to 0.06861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0515 - val_loss: 0.0686\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 264: val_loss improved from 0.06861 to 0.06843, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0513 - val_loss: 0.0684\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 265: val_loss improved from 0.06843 to 0.06826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0513 - val_loss: 0.0683\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 266: val_loss improved from 0.06826 to 0.06808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0509 - val_loss: 0.0681\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0505\n","Epoch 267: val_loss improved from 0.06808 to 0.06791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0505 - val_loss: 0.0679\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 268: val_loss improved from 0.06791 to 0.06773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0503 - val_loss: 0.0677\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 269: val_loss improved from 0.06773 to 0.06757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0504 - val_loss: 0.0676\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 270: val_loss improved from 0.06757 to 0.06740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0497 - val_loss: 0.0674\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 271: val_loss improved from 0.06740 to 0.06723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0497 - val_loss: 0.0672\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 272: val_loss improved from 0.06723 to 0.06707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0494 - val_loss: 0.0671\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 273: val_loss improved from 0.06707 to 0.06691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0494 - val_loss: 0.0669\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 274: val_loss improved from 0.06691 to 0.06675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0488 - val_loss: 0.0667\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 275: val_loss improved from 0.06675 to 0.06659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0488 - val_loss: 0.0666\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 276: val_loss improved from 0.06659 to 0.06643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0487 - val_loss: 0.0664\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 277: val_loss improved from 0.06643 to 0.06628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0482 - val_loss: 0.0663\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 278: val_loss improved from 0.06628 to 0.06613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0483 - val_loss: 0.0661\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0478\n","Epoch 279: val_loss improved from 0.06613 to 0.06598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0478 - val_loss: 0.0660\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 280: val_loss improved from 0.06598 to 0.06583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0479 - val_loss: 0.0658\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 281: val_loss improved from 0.06583 to 0.06568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0474 - val_loss: 0.0657\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 282: val_loss improved from 0.06568 to 0.06554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0474 - val_loss: 0.0655\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0473\n","Epoch 283: val_loss improved from 0.06554 to 0.06539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0473 - val_loss: 0.0654\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 284: val_loss improved from 0.06539 to 0.06525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0469 - val_loss: 0.0653\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 285: val_loss improved from 0.06525 to 0.06511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0467 - val_loss: 0.0651\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 286: val_loss improved from 0.06511 to 0.06497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0467 - val_loss: 0.0650\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0464\n","Epoch 287: val_loss improved from 0.06497 to 0.06484, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0464 - val_loss: 0.0648\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0464\n","Epoch 288: val_loss improved from 0.06484 to 0.06470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0464 - val_loss: 0.0647\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 289: val_loss improved from 0.06470 to 0.06457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0462 - val_loss: 0.0646\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 290: val_loss improved from 0.06457 to 0.06443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0457 - val_loss: 0.0644\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 291: val_loss improved from 0.06443 to 0.06430, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0458 - val_loss: 0.0643\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 292: val_loss improved from 0.06430 to 0.06418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0454 - val_loss: 0.0642\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 293: val_loss improved from 0.06418 to 0.06405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0455 - val_loss: 0.0640\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 294: val_loss improved from 0.06405 to 0.06392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0457 - val_loss: 0.0639\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 295: val_loss improved from 0.06392 to 0.06380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0452 - val_loss: 0.0638\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 296: val_loss improved from 0.06380 to 0.06368, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0446 - val_loss: 0.0637\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 297: val_loss improved from 0.06368 to 0.06356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0446 - val_loss: 0.0636\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 298: val_loss improved from 0.06356 to 0.06344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0444 - val_loss: 0.0634\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 299: val_loss improved from 0.06344 to 0.06332, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0442 - val_loss: 0.0633\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 300: val_loss improved from 0.06332 to 0.06320, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0440 - val_loss: 0.0632\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 301: val_loss improved from 0.06320 to 0.06309, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0440 - val_loss: 0.0631\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0443\n","Epoch 302: val_loss improved from 0.06309 to 0.06297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0443 - val_loss: 0.0630\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 303: val_loss improved from 0.06297 to 0.06286, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0435 - val_loss: 0.0629\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 304: val_loss improved from 0.06286 to 0.06275, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0435 - val_loss: 0.0628\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 305: val_loss improved from 0.06275 to 0.06264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0437 - val_loss: 0.0626\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 306: val_loss improved from 0.06264 to 0.06253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0430 - val_loss: 0.0625\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 307: val_loss improved from 0.06253 to 0.06243, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0429 - val_loss: 0.0624\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 308: val_loss improved from 0.06243 to 0.06232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0427 - val_loss: 0.0623\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 309: val_loss improved from 0.06232 to 0.06222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0426 - val_loss: 0.0622\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 310: val_loss improved from 0.06222 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0429 - val_loss: 0.0621\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 311: val_loss improved from 0.06212 to 0.06201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0424 - val_loss: 0.0620\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 312: val_loss improved from 0.06201 to 0.06191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0420 - val_loss: 0.0619\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 313: val_loss improved from 0.06191 to 0.06181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0424 - val_loss: 0.0618\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 314: val_loss improved from 0.06181 to 0.06172, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0421 - val_loss: 0.0617\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 315: val_loss improved from 0.06172 to 0.06162, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 316: val_loss improved from 0.06162 to 0.06152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0416 - val_loss: 0.0615\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 317: val_loss improved from 0.06152 to 0.06143, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0417 - val_loss: 0.0614\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 318: val_loss improved from 0.06143 to 0.06134, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0413 - val_loss: 0.0613\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 319: val_loss improved from 0.06134 to 0.06124, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0415 - val_loss: 0.0612\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 320: val_loss improved from 0.06124 to 0.06115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0409 - val_loss: 0.0612\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 321: val_loss improved from 0.06115 to 0.06106, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0411 - val_loss: 0.0611\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 322: val_loss improved from 0.06106 to 0.06098, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0409 - val_loss: 0.0610\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 323: val_loss improved from 0.06098 to 0.06089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0408 - val_loss: 0.0609\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 324: val_loss improved from 0.06089 to 0.06080, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0406 - val_loss: 0.0608\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 325: val_loss improved from 0.06080 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0406 - val_loss: 0.0607\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 326: val_loss improved from 0.06072 to 0.06063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0403 - val_loss: 0.0606\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 327: val_loss improved from 0.06063 to 0.06055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0406 - val_loss: 0.0605\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 328: val_loss improved from 0.06055 to 0.06047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0399 - val_loss: 0.0605\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 329: val_loss improved from 0.06047 to 0.06039, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0399 - val_loss: 0.0604\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 330: val_loss improved from 0.06039 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0400 - val_loss: 0.0603\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 331: val_loss improved from 0.06031 to 0.06023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0398 - val_loss: 0.0602\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 332: val_loss improved from 0.06023 to 0.06015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0394 - val_loss: 0.0602\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 333: val_loss improved from 0.06015 to 0.06007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 334: val_loss improved from 0.06007 to 0.06000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0395 - val_loss: 0.0600\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 335: val_loss improved from 0.06000 to 0.05992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0393 - val_loss: 0.0599\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 336: val_loss improved from 0.05992 to 0.05985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0390 - val_loss: 0.0598\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 337: val_loss improved from 0.05985 to 0.05978, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0390 - val_loss: 0.0598\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 338: val_loss improved from 0.05978 to 0.05970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0390 - val_loss: 0.0597\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 339: val_loss improved from 0.05970 to 0.05963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0388 - val_loss: 0.0596\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 340: val_loss improved from 0.05963 to 0.05956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0389 - val_loss: 0.0596\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 341: val_loss improved from 0.05956 to 0.05949, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0386 - val_loss: 0.0595\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 342: val_loss improved from 0.05949 to 0.05943, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0384 - val_loss: 0.0594\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 343: val_loss improved from 0.05943 to 0.05936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0385 - val_loss: 0.0594\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 344: val_loss improved from 0.05936 to 0.05929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0382 - val_loss: 0.0593\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 345: val_loss improved from 0.05929 to 0.05923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0382 - val_loss: 0.0592\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 346: val_loss improved from 0.05923 to 0.05916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0379 - val_loss: 0.0592\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 347: val_loss improved from 0.05916 to 0.05910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0383 - val_loss: 0.0591\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 348: val_loss improved from 0.05910 to 0.05903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0380 - val_loss: 0.0590\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 349: val_loss improved from 0.05903 to 0.05897, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0378 - val_loss: 0.0590\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 350: val_loss improved from 0.05897 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0378 - val_loss: 0.0589\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 351: val_loss improved from 0.05891 to 0.05885, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0380 - val_loss: 0.0588\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 352: val_loss improved from 0.05885 to 0.05879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0377 - val_loss: 0.0588\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 353: val_loss improved from 0.05879 to 0.05873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0372 - val_loss: 0.0587\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 354: val_loss improved from 0.05873 to 0.05867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0372 - val_loss: 0.0587\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 355: val_loss improved from 0.05867 to 0.05862, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0371 - val_loss: 0.0586\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 356: val_loss improved from 0.05862 to 0.05856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0373 - val_loss: 0.0586\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 357: val_loss improved from 0.05856 to 0.05850, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0370 - val_loss: 0.0585\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 358: val_loss improved from 0.05850 to 0.05845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 359: val_loss improved from 0.05845 to 0.05839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0367 - val_loss: 0.0584\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 360: val_loss improved from 0.05839 to 0.05834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0367 - val_loss: 0.0583\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 361: val_loss improved from 0.05834 to 0.05829, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0368 - val_loss: 0.0583\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 362: val_loss improved from 0.05829 to 0.05824, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0364 - val_loss: 0.0582\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 363: val_loss improved from 0.05824 to 0.05818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0363 - val_loss: 0.0582\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 364: val_loss improved from 0.05818 to 0.05813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0363 - val_loss: 0.0581\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 365: val_loss improved from 0.05813 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0362 - val_loss: 0.0581\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 366: val_loss improved from 0.05808 to 0.05803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0361 - val_loss: 0.0580\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 367: val_loss improved from 0.05803 to 0.05798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0359 - val_loss: 0.0580\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 368: val_loss improved from 0.05798 to 0.05794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0358 - val_loss: 0.0579\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 369: val_loss improved from 0.05794 to 0.05789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0362 - val_loss: 0.0579\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 370: val_loss improved from 0.05789 to 0.05784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0358 - val_loss: 0.0578\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 371: val_loss improved from 0.05784 to 0.05779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0357 - val_loss: 0.0578\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 372: val_loss improved from 0.05779 to 0.05775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0361 - val_loss: 0.0577\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 373: val_loss improved from 0.05775 to 0.05770, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 374: val_loss improved from 0.05770 to 0.05766, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0361 - val_loss: 0.0577\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 375: val_loss improved from 0.05766 to 0.05762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0354 - val_loss: 0.0576\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 376: val_loss improved from 0.05762 to 0.05757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0353 - val_loss: 0.0576\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 377: val_loss improved from 0.05757 to 0.05753, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0359 - val_loss: 0.0575\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 378: val_loss improved from 0.05753 to 0.05749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0353 - val_loss: 0.0575\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 379: val_loss improved from 0.05749 to 0.05745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 380: val_loss improved from 0.05745 to 0.05741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0351 - val_loss: 0.0574\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 381: val_loss improved from 0.05741 to 0.05737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0350 - val_loss: 0.0574\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 382: val_loss improved from 0.05737 to 0.05733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 383: val_loss improved from 0.05733 to 0.05729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0347 - val_loss: 0.0573\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 384: val_loss improved from 0.05729 to 0.05725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0348 - val_loss: 0.0573\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 385: val_loss improved from 0.05725 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0347 - val_loss: 0.0572\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 386: val_loss improved from 0.05721 to 0.05717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0346 - val_loss: 0.0572\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 387: val_loss improved from 0.05717 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 388: val_loss improved from 0.05714 to 0.05710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0348 - val_loss: 0.0571\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 389: val_loss improved from 0.05710 to 0.05707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 390: val_loss improved from 0.05707 to 0.05703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 391: val_loss improved from 0.05703 to 0.05700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 392: val_loss improved from 0.05700 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0343 - val_loss: 0.0570\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 393: val_loss improved from 0.05696 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0341 - val_loss: 0.0569\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 394: val_loss improved from 0.05693 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0341 - val_loss: 0.0569\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 395: val_loss improved from 0.05689 to 0.05686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0341 - val_loss: 0.0569\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 396: val_loss improved from 0.05686 to 0.05683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 397: val_loss improved from 0.05683 to 0.05680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0345 - val_loss: 0.0568\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 398: val_loss improved from 0.05680 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 399: val_loss improved from 0.05677 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0341 - val_loss: 0.0567\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 400: val_loss improved from 0.05674 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 401: val_loss improved from 0.05671 to 0.05668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0337 - val_loss: 0.0567\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 402: val_loss improved from 0.05668 to 0.05665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0336 - val_loss: 0.0566\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 403: val_loss improved from 0.05665 to 0.05662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0339 - val_loss: 0.0566\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 404: val_loss improved from 0.05662 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0338 - val_loss: 0.0566\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 405: val_loss improved from 0.05659 to 0.05656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0335 - val_loss: 0.0566\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 406: val_loss improved from 0.05656 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 407: val_loss improved from 0.05653 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 408: val_loss improved from 0.05651 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 409: val_loss improved from 0.05648 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 410: val_loss improved from 0.05645 to 0.05643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0334 - val_loss: 0.0564\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 411: val_loss improved from 0.05643 to 0.05640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0336 - val_loss: 0.0564\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 412: val_loss improved from 0.05640 to 0.05638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 413: val_loss improved from 0.05638 to 0.05635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0334 - val_loss: 0.0564\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 414: val_loss improved from 0.05635 to 0.05633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 415: val_loss improved from 0.05633 to 0.05630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0333 - val_loss: 0.0563\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 416: val_loss improved from 0.05630 to 0.05628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 417: val_loss improved from 0.05628 to 0.05626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 418: val_loss improved from 0.05626 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0330 - val_loss: 0.0562\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 419: val_loss improved from 0.05623 to 0.05621, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 420: val_loss improved from 0.05621 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 421: val_loss improved from 0.05619 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0327 - val_loss: 0.0562\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 422: val_loss improved from 0.05617 to 0.05615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0329 - val_loss: 0.0561\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 423: val_loss improved from 0.05615 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0329 - val_loss: 0.0561\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 424: val_loss improved from 0.05613 to 0.05611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0328 - val_loss: 0.0561\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 425: val_loss improved from 0.05611 to 0.05609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 426: val_loss improved from 0.05609 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0329 - val_loss: 0.0561\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 427: val_loss improved from 0.05607 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 428: val_loss improved from 0.05605 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 429: val_loss improved from 0.05603 to 0.05601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 430: val_loss improved from 0.05601 to 0.05599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0322 - val_loss: 0.0560\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 431: val_loss improved from 0.05599 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0328 - val_loss: 0.0560\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 432: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0322 - val_loss: 0.0560\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 433: val_loss improved from 0.05595 to 0.05594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0326 - val_loss: 0.0559\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 434: val_loss improved from 0.05594 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 435: val_loss improved from 0.05592 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 436: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 437: val_loss improved from 0.05588 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0320 - val_loss: 0.0559\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 438: val_loss improved from 0.05587 to 0.05585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 439: val_loss improved from 0.05585 to 0.05583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 440: val_loss improved from 0.05583 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 441: val_loss improved from 0.05582 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 442: val_loss improved from 0.05580 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0317 - val_loss: 0.0558\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 443: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0317 - val_loss: 0.0558\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 444: val_loss improved from 0.05577 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0321 - val_loss: 0.0558\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 445: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0313 - val_loss: 0.0557\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 446: val_loss improved from 0.05574 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 447: val_loss improved from 0.05573 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0319 - val_loss: 0.0557\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 448: val_loss improved from 0.05572 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 449: val_loss improved from 0.05570 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 450: val_loss improved from 0.05569 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 451: val_loss improved from 0.05568 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0315 - val_loss: 0.0557\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 452: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 453: val_loss improved from 0.05565 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 454: val_loss improved from 0.05564 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 455: val_loss improved from 0.05563 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0312 - val_loss: 0.0556\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 456: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 457: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0317 - val_loss: 0.0556\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 458: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0312 - val_loss: 0.0556\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 459: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0312 - val_loss: 0.0556\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 460: val_loss improved from 0.05557 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0308 - val_loss: 0.0556\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 461: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 462: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0314 - val_loss: 0.0555\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 463: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 464: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 465: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 466: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 467: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 468: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 469: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 470: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 471: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 472: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 473: val_loss improved from 0.05544 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 474: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 475: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 476: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0312 - val_loss: 0.0554\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 477: val_loss improved from 0.05541 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 478: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 479: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 480: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 481: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 482: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 483: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 484: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0311 - val_loss: 0.0554\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 485: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 486: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 487: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 488: val_loss improved from 0.05534 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 489: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0307 - val_loss: 0.0553\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 490: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 491: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 492: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 493: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 494: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 495: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 496: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 497: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 498: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 499: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 500: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 501: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 502: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 503: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 504: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 505: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 506: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0308 - val_loss: 0.0553\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 507: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 508: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 509: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 510: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 511: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 512: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 513: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 514: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0302 - val_loss: 0.0552\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 515: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 516: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 517: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0303 - val_loss: 0.0552\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 518: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 519: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0302 - val_loss: 0.0552\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 520: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 521: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 522: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 523: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 524: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 525: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 526: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 527: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 528: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 529: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 530: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 531: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 532/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 532: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 533/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 533: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 534/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 534: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 535/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 535: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 536/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 536: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 537/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 537: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 538/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 538: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 539/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 539: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 540/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 540: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 541/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 541: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 542/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 542: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 543/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 543: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 544/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 544: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 545/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 545: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 546/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 546: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 547/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 547: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 548/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 548: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 549/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 549: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 550/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 550: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 551/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 551: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 552/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 552: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 553/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 553: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 554/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 554: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 555/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 555: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 556/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 556: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 557/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 557: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 558/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 558: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 559/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 559: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 560/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 560: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 561/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 561: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 562/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 562: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 563/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 563: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 564/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 564: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 565/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 565: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 566/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 566: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 567/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 567: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 568/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 568: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0289 - val_loss: 0.0552\n","Epoch 569/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 569: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 570/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 570: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 571/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 571: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 572/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 572: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0552\n","Epoch 573/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 573: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - val_loss: 0.0552\n","Epoch 574/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 574: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 575/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 575: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 576/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 576: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 577/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 577: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 578/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 578: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 579/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 579: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 580/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 580: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 581/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 581: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 582/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 582: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 583/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 583: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 584/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 584: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 585/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 585: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 586/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 586: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 587/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 587: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 588/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 588: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 589/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 589: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 590/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 590: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 591/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 591: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 592/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 592: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 593/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 593: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 594/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 594: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 595/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 595: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 596/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 596: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 597/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 597: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 598/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 598: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 599/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 599: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 600/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 600: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 601/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 601: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 602/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 602: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 603/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 603: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 604/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 604: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 605/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 605: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 606/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 606: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 607/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 607: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0284 - val_loss: 0.0553\n","Epoch 608/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 608: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 609/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 609: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 610/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 610: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 611/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 611: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 612/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 612: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 613/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 613: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 614/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 614: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 615/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 615: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 616/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 616: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 617/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 617: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 618/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 618: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0284 - val_loss: 0.0554\n","Epoch 619/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 619: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 620/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 620: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 621/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 621: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 622/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 622: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 623/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 623: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0283 - val_loss: 0.0554\n","Epoch 624/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 624: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 625/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 625: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 626/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 626: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 627/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 627: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0284 - val_loss: 0.0554\n","Epoch 628/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 628: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 629/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 629: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 630/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 630: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 631/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 631: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 632/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 632: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 633/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 633: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 634/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 634: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 635/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 635: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 636/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 636: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 637/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 637: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 638/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 638: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 639/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 639: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 640/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 640: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0288 - val_loss: 0.0554\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0734\n","loss_and_metrics : 0.07335667312145233\n","1/1 [==============================] - 0s 117ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=5, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(units=5, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}