{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAG4wi9m1D5WdRqslzGfXu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZVvJtj2Dm1vN","executionInfo":{"status":"ok","timestamp":1695182472330,"user_tz":-540,"elapsed":42491,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"d3d368b9-9c60-4d06-ebd3-b605f55add07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3665\n","Epoch 1: val_loss improved from inf to 0.34934, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 592ms/step - loss: 0.3665 - val_loss: 0.3493\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3632\n","Epoch 2: val_loss improved from 0.34934 to 0.34619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3632 - val_loss: 0.3462\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3595\n","Epoch 3: val_loss improved from 0.34619 to 0.34307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3595 - val_loss: 0.3431\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3564\n","Epoch 4: val_loss improved from 0.34307 to 0.33999, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3564 - val_loss: 0.3400\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3530\n","Epoch 5: val_loss improved from 0.33999 to 0.33694, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3530 - val_loss: 0.3369\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3497\n","Epoch 6: val_loss improved from 0.33694 to 0.33391, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.3497 - val_loss: 0.3339\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3462\n","Epoch 7: val_loss improved from 0.33391 to 0.33092, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3462 - val_loss: 0.3309\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3432\n","Epoch 8: val_loss improved from 0.33092 to 0.32797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.3432 - val_loss: 0.3280\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3404\n","Epoch 9: val_loss improved from 0.32797 to 0.32504, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3404 - val_loss: 0.3250\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3374\n","Epoch 10: val_loss improved from 0.32504 to 0.32214, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3374 - val_loss: 0.3221\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3339\n","Epoch 11: val_loss improved from 0.32214 to 0.31928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3339 - val_loss: 0.3193\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3308\n","Epoch 12: val_loss improved from 0.31928 to 0.31644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3308 - val_loss: 0.3164\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3278\n","Epoch 13: val_loss improved from 0.31644 to 0.31363, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3278 - val_loss: 0.3136\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3246\n","Epoch 14: val_loss improved from 0.31363 to 0.31085, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3246 - val_loss: 0.3109\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3220\n","Epoch 15: val_loss improved from 0.31085 to 0.30810, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3220 - val_loss: 0.3081\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3190\n","Epoch 16: val_loss improved from 0.30810 to 0.30538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3190 - val_loss: 0.3054\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3160\n","Epoch 17: val_loss improved from 0.30538 to 0.30269, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3160 - val_loss: 0.3027\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3131\n","Epoch 18: val_loss improved from 0.30269 to 0.30002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3131 - val_loss: 0.3000\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3102\n","Epoch 19: val_loss improved from 0.30002 to 0.29739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.3102 - val_loss: 0.2974\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3076\n","Epoch 20: val_loss improved from 0.29739 to 0.29478, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3076 - val_loss: 0.2948\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3048\n","Epoch 21: val_loss improved from 0.29478 to 0.29220, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.3048 - val_loss: 0.2922\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3019\n","Epoch 22: val_loss improved from 0.29220 to 0.28964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.3019 - val_loss: 0.2896\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2989\n","Epoch 23: val_loss improved from 0.28964 to 0.28712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2989 - val_loss: 0.2871\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2965\n","Epoch 24: val_loss improved from 0.28712 to 0.28461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2965 - val_loss: 0.2846\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2938\n","Epoch 25: val_loss improved from 0.28461 to 0.28214, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2938 - val_loss: 0.2821\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2907\n","Epoch 26: val_loss improved from 0.28214 to 0.27968, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2907 - val_loss: 0.2797\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2883\n","Epoch 27: val_loss improved from 0.27968 to 0.27726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2883 - val_loss: 0.2773\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2856\n","Epoch 28: val_loss improved from 0.27726 to 0.27486, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2856 - val_loss: 0.2749\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2831\n","Epoch 29: val_loss improved from 0.27486 to 0.27248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2831 - val_loss: 0.2725\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2807\n","Epoch 30: val_loss improved from 0.27248 to 0.27013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2807 - val_loss: 0.2701\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2781\n","Epoch 31: val_loss improved from 0.27013 to 0.26780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2781 - val_loss: 0.2678\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2755\n","Epoch 32: val_loss improved from 0.26780 to 0.26550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2755 - val_loss: 0.2655\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2729\n","Epoch 33: val_loss improved from 0.26550 to 0.26323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2729 - val_loss: 0.2632\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2709\n","Epoch 34: val_loss improved from 0.26323 to 0.26097, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2709 - val_loss: 0.2610\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2684\n","Epoch 35: val_loss improved from 0.26097 to 0.25874, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2684 - val_loss: 0.2587\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2657\n","Epoch 36: val_loss improved from 0.25874 to 0.25653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2657 - val_loss: 0.2565\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2634\n","Epoch 37: val_loss improved from 0.25653 to 0.25435, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2634 - val_loss: 0.2543\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2607\n","Epoch 38: val_loss improved from 0.25435 to 0.25219, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2607 - val_loss: 0.2522\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2589\n","Epoch 39: val_loss improved from 0.25219 to 0.25005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2589 - val_loss: 0.2500\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2565\n","Epoch 40: val_loss improved from 0.25005 to 0.24793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2565 - val_loss: 0.2479\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2540\n","Epoch 41: val_loss improved from 0.24793 to 0.24583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2540 - val_loss: 0.2458\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2518\n","Epoch 42: val_loss improved from 0.24583 to 0.24376, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2518 - val_loss: 0.2438\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2496\n","Epoch 43: val_loss improved from 0.24376 to 0.24171, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2496 - val_loss: 0.2417\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2474\n","Epoch 44: val_loss improved from 0.24171 to 0.23968, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2474 - val_loss: 0.2397\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2451\n","Epoch 45: val_loss improved from 0.23968 to 0.23767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2451 - val_loss: 0.2377\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2428\n","Epoch 46: val_loss improved from 0.23767 to 0.23568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2428 - val_loss: 0.2357\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2407\n","Epoch 47: val_loss improved from 0.23568 to 0.23371, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2407 - val_loss: 0.2337\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2388\n","Epoch 48: val_loss improved from 0.23371 to 0.23177, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.2388 - val_loss: 0.2318\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2368\n","Epoch 49: val_loss improved from 0.23177 to 0.22984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2368 - val_loss: 0.2298\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2346\n","Epoch 50: val_loss improved from 0.22984 to 0.22793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2346 - val_loss: 0.2279\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2324\n","Epoch 51: val_loss improved from 0.22793 to 0.22605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2324 - val_loss: 0.2260\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2303\n","Epoch 52: val_loss improved from 0.22605 to 0.22418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2303 - val_loss: 0.2242\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2282\n","Epoch 53: val_loss improved from 0.22418 to 0.22234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2282 - val_loss: 0.2223\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2266\n","Epoch 54: val_loss improved from 0.22234 to 0.22051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2266 - val_loss: 0.2205\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2242\n","Epoch 55: val_loss improved from 0.22051 to 0.21870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2242 - val_loss: 0.2187\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2223\n","Epoch 56: val_loss improved from 0.21870 to 0.21691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2223 - val_loss: 0.2169\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2204\n","Epoch 57: val_loss improved from 0.21691 to 0.21514, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2204 - val_loss: 0.2151\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2183\n","Epoch 58: val_loss improved from 0.21514 to 0.21339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.2183 - val_loss: 0.2134\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2167\n","Epoch 59: val_loss improved from 0.21339 to 0.21166, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2167 - val_loss: 0.2117\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2145\n","Epoch 60: val_loss improved from 0.21166 to 0.20994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2145 - val_loss: 0.2099\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2129\n","Epoch 61: val_loss improved from 0.20994 to 0.20824, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2129 - val_loss: 0.2082\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2110\n","Epoch 62: val_loss improved from 0.20824 to 0.20657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2110 - val_loss: 0.2066\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 63: val_loss improved from 0.20657 to 0.20490, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2091 - val_loss: 0.2049\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2074\n","Epoch 64: val_loss improved from 0.20490 to 0.20326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2074 - val_loss: 0.2033\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2051\n","Epoch 65: val_loss improved from 0.20326 to 0.20163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2051 - val_loss: 0.2016\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2040\n","Epoch 66: val_loss improved from 0.20163 to 0.20002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2040 - val_loss: 0.2000\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2021\n","Epoch 67: val_loss improved from 0.20002 to 0.19842, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2021 - val_loss: 0.1984\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2001\n","Epoch 68: val_loss improved from 0.19842 to 0.19685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2001 - val_loss: 0.1968\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1985\n","Epoch 69: val_loss improved from 0.19685 to 0.19529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1985 - val_loss: 0.1953\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1968\n","Epoch 70: val_loss improved from 0.19529 to 0.19374, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1968 - val_loss: 0.1937\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1950\n","Epoch 71: val_loss improved from 0.19374 to 0.19221, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1950 - val_loss: 0.1922\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1933\n","Epoch 72: val_loss improved from 0.19221 to 0.19070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1933 - val_loss: 0.1907\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1920\n","Epoch 73: val_loss improved from 0.19070 to 0.18921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1920 - val_loss: 0.1892\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1901\n","Epoch 74: val_loss improved from 0.18921 to 0.18773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1901 - val_loss: 0.1877\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1884\n","Epoch 75: val_loss improved from 0.18773 to 0.18626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1884 - val_loss: 0.1863\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1866\n","Epoch 76: val_loss improved from 0.18626 to 0.18481, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1866 - val_loss: 0.1848\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1849\n","Epoch 77: val_loss improved from 0.18481 to 0.18338, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1849 - val_loss: 0.1834\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1837\n","Epoch 78: val_loss improved from 0.18338 to 0.18196, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1837 - val_loss: 0.1820\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1822\n","Epoch 79: val_loss improved from 0.18196 to 0.18055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1822 - val_loss: 0.1806\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1807\n","Epoch 80: val_loss improved from 0.18055 to 0.17916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1807 - val_loss: 0.1792\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1788\n","Epoch 81: val_loss improved from 0.17916 to 0.17779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1788 - val_loss: 0.1778\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1773\n","Epoch 82: val_loss improved from 0.17779 to 0.17643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1773 - val_loss: 0.1764\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1757\n","Epoch 83: val_loss improved from 0.17643 to 0.17508, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1757 - val_loss: 0.1751\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1748\n","Epoch 84: val_loss improved from 0.17508 to 0.17375, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1748 - val_loss: 0.1737\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1733\n","Epoch 85: val_loss improved from 0.17375 to 0.17243, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1733 - val_loss: 0.1724\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1714\n","Epoch 86: val_loss improved from 0.17243 to 0.17113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1714 - val_loss: 0.1711\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1701\n","Epoch 87: val_loss improved from 0.17113 to 0.16984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1701 - val_loss: 0.1698\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1687\n","Epoch 88: val_loss improved from 0.16984 to 0.16856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1687 - val_loss: 0.1686\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1671\n","Epoch 89: val_loss improved from 0.16856 to 0.16730, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1671 - val_loss: 0.1673\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1660\n","Epoch 90: val_loss improved from 0.16730 to 0.16605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1660 - val_loss: 0.1660\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1644\n","Epoch 91: val_loss improved from 0.16605 to 0.16481, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1644 - val_loss: 0.1648\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1630\n","Epoch 92: val_loss improved from 0.16481 to 0.16359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1630 - val_loss: 0.1636\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1621\n","Epoch 93: val_loss improved from 0.16359 to 0.16238, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1621 - val_loss: 0.1624\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1603\n","Epoch 94: val_loss improved from 0.16238 to 0.16118, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1603 - val_loss: 0.1612\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1592\n","Epoch 95: val_loss improved from 0.16118 to 0.16000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1592 - val_loss: 0.1600\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1580\n","Epoch 96: val_loss improved from 0.16000 to 0.15882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1580 - val_loss: 0.1588\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1564\n","Epoch 97: val_loss improved from 0.15882 to 0.15767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1564 - val_loss: 0.1577\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1551\n","Epoch 98: val_loss improved from 0.15767 to 0.15652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1551 - val_loss: 0.1565\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1537\n","Epoch 99: val_loss improved from 0.15652 to 0.15538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1537 - val_loss: 0.1554\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1531\n","Epoch 100: val_loss improved from 0.15538 to 0.15426, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1531 - val_loss: 0.1543\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1515\n","Epoch 101: val_loss improved from 0.15426 to 0.15315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1515 - val_loss: 0.1531\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1502\n","Epoch 102: val_loss improved from 0.15315 to 0.15205, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1502 - val_loss: 0.1520\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1489\n","Epoch 103: val_loss improved from 0.15205 to 0.15096, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1489 - val_loss: 0.1510\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1476\n","Epoch 104: val_loss improved from 0.15096 to 0.14988, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1476 - val_loss: 0.1499\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1467\n","Epoch 105: val_loss improved from 0.14988 to 0.14882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1467 - val_loss: 0.1488\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1454\n","Epoch 106: val_loss improved from 0.14882 to 0.14776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1454 - val_loss: 0.1478\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1441\n","Epoch 107: val_loss improved from 0.14776 to 0.14672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1441 - val_loss: 0.1467\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1432\n","Epoch 108: val_loss improved from 0.14672 to 0.14569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1432 - val_loss: 0.1457\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1418\n","Epoch 109: val_loss improved from 0.14569 to 0.14467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1418 - val_loss: 0.1447\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1404\n","Epoch 110: val_loss improved from 0.14467 to 0.14366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1404 - val_loss: 0.1437\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1395\n","Epoch 111: val_loss improved from 0.14366 to 0.14266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1395 - val_loss: 0.1427\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1385\n","Epoch 112: val_loss improved from 0.14266 to 0.14167, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1385 - val_loss: 0.1417\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1375\n","Epoch 113: val_loss improved from 0.14167 to 0.14070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1375 - val_loss: 0.1407\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1363\n","Epoch 114: val_loss improved from 0.14070 to 0.13973, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1363 - val_loss: 0.1397\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1351\n","Epoch 115: val_loss improved from 0.13973 to 0.13877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1351 - val_loss: 0.1388\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1339\n","Epoch 116: val_loss improved from 0.13877 to 0.13782, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1339 - val_loss: 0.1378\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1333\n","Epoch 117: val_loss improved from 0.13782 to 0.13689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1333 - val_loss: 0.1369\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1318\n","Epoch 118: val_loss improved from 0.13689 to 0.13596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1318 - val_loss: 0.1360\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1310\n","Epoch 119: val_loss improved from 0.13596 to 0.13504, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1310 - val_loss: 0.1350\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1298\n","Epoch 120: val_loss improved from 0.13504 to 0.13414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1298 - val_loss: 0.1341\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1291\n","Epoch 121: val_loss improved from 0.13414 to 0.13324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1291 - val_loss: 0.1332\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1279\n","Epoch 122: val_loss improved from 0.13324 to 0.13235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1279 - val_loss: 0.1324\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1269\n","Epoch 123: val_loss improved from 0.13235 to 0.13147, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1269 - val_loss: 0.1315\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1257\n","Epoch 124: val_loss improved from 0.13147 to 0.13060, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1257 - val_loss: 0.1306\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1248\n","Epoch 125: val_loss improved from 0.13060 to 0.12974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1248 - val_loss: 0.1297\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1241\n","Epoch 126: val_loss improved from 0.12974 to 0.12889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1241 - val_loss: 0.1289\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1231\n","Epoch 127: val_loss improved from 0.12889 to 0.12805, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1231 - val_loss: 0.1281\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1221\n","Epoch 128: val_loss improved from 0.12805 to 0.12722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1221 - val_loss: 0.1272\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1209\n","Epoch 129: val_loss improved from 0.12722 to 0.12639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1209 - val_loss: 0.1264\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1202\n","Epoch 130: val_loss improved from 0.12639 to 0.12558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1202 - val_loss: 0.1256\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1191\n","Epoch 131: val_loss improved from 0.12558 to 0.12477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1191 - val_loss: 0.1248\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1183\n","Epoch 132: val_loss improved from 0.12477 to 0.12398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1183 - val_loss: 0.1240\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1171\n","Epoch 133: val_loss improved from 0.12398 to 0.12319, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1171 - val_loss: 0.1232\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1162\n","Epoch 134: val_loss improved from 0.12319 to 0.12241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1162 - val_loss: 0.1224\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1158\n","Epoch 135: val_loss improved from 0.12241 to 0.12163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1158 - val_loss: 0.1216\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1147\n","Epoch 136: val_loss improved from 0.12163 to 0.12087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1147 - val_loss: 0.1209\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1137\n","Epoch 137: val_loss improved from 0.12087 to 0.12011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1137 - val_loss: 0.1201\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1132\n","Epoch 138: val_loss improved from 0.12011 to 0.11937, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1132 - val_loss: 0.1194\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1122\n","Epoch 139: val_loss improved from 0.11937 to 0.11863, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1122 - val_loss: 0.1186\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1113\n","Epoch 140: val_loss improved from 0.11863 to 0.11790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1113 - val_loss: 0.1179\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1102\n","Epoch 141: val_loss improved from 0.11790 to 0.11717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1102 - val_loss: 0.1172\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1095\n","Epoch 142: val_loss improved from 0.11717 to 0.11646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1095 - val_loss: 0.1165\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1085\n","Epoch 143: val_loss improved from 0.11646 to 0.11575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1085 - val_loss: 0.1157\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1079\n","Epoch 144: val_loss improved from 0.11575 to 0.11505, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1079 - val_loss: 0.1150\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 145: val_loss improved from 0.11505 to 0.11435, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1071 - val_loss: 0.1144\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 146: val_loss improved from 0.11435 to 0.11367, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1063 - val_loss: 0.1137\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1057\n","Epoch 147: val_loss improved from 0.11367 to 0.11299, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1057 - val_loss: 0.1130\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1048\n","Epoch 148: val_loss improved from 0.11299 to 0.11232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1048 - val_loss: 0.1123\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1040\n","Epoch 149: val_loss improved from 0.11232 to 0.11166, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1040 - val_loss: 0.1117\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1035\n","Epoch 150: val_loss improved from 0.11166 to 0.11100, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1035 - val_loss: 0.1110\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1023\n","Epoch 151: val_loss improved from 0.11100 to 0.11035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1023 - val_loss: 0.1104\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1020\n","Epoch 152: val_loss improved from 0.11035 to 0.10971, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1020 - val_loss: 0.1097\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1011\n","Epoch 153: val_loss improved from 0.10971 to 0.10907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1011 - val_loss: 0.1091\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1002\n","Epoch 154: val_loss improved from 0.10907 to 0.10845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1002 - val_loss: 0.1084\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0997\n","Epoch 155: val_loss improved from 0.10845 to 0.10783, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0997 - val_loss: 0.1078\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0988\n","Epoch 156: val_loss improved from 0.10783 to 0.10721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0988 - val_loss: 0.1072\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0982\n","Epoch 157: val_loss improved from 0.10721 to 0.10660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0982 - val_loss: 0.1066\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 158: val_loss improved from 0.10660 to 0.10600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0975 - val_loss: 0.1060\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0965\n","Epoch 159: val_loss improved from 0.10600 to 0.10541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0965 - val_loss: 0.1054\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0961\n","Epoch 160: val_loss improved from 0.10541 to 0.10482, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0961 - val_loss: 0.1048\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0956\n","Epoch 161: val_loss improved from 0.10482 to 0.10423, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0956 - val_loss: 0.1042\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0947\n","Epoch 162: val_loss improved from 0.10423 to 0.10366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0947 - val_loss: 0.1037\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0939\n","Epoch 163: val_loss improved from 0.10366 to 0.10309, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0939 - val_loss: 0.1031\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0930\n","Epoch 164: val_loss improved from 0.10309 to 0.10253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0930 - val_loss: 0.1025\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0927\n","Epoch 165: val_loss improved from 0.10253 to 0.10197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0927 - val_loss: 0.1020\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0919\n","Epoch 166: val_loss improved from 0.10197 to 0.10142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0919 - val_loss: 0.1014\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0914\n","Epoch 167: val_loss improved from 0.10142 to 0.10087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0914 - val_loss: 0.1009\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0905\n","Epoch 168: val_loss improved from 0.10087 to 0.10033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0905 - val_loss: 0.1003\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0901\n","Epoch 169: val_loss improved from 0.10033 to 0.09980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0901 - val_loss: 0.0998\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0895\n","Epoch 170: val_loss improved from 0.09980 to 0.09927, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0895 - val_loss: 0.0993\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0887\n","Epoch 171: val_loss improved from 0.09927 to 0.09875, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0887 - val_loss: 0.0987\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0883\n","Epoch 172: val_loss improved from 0.09875 to 0.09823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0883 - val_loss: 0.0982\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0876\n","Epoch 173: val_loss improved from 0.09823 to 0.09772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0876 - val_loss: 0.0977\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0872\n","Epoch 174: val_loss improved from 0.09772 to 0.09722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0872 - val_loss: 0.0972\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 175: val_loss improved from 0.09722 to 0.09672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0863 - val_loss: 0.0967\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0861\n","Epoch 176: val_loss improved from 0.09672 to 0.09623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0861 - val_loss: 0.0962\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0851\n","Epoch 177: val_loss improved from 0.09623 to 0.09574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0851 - val_loss: 0.0957\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0847\n","Epoch 178: val_loss improved from 0.09574 to 0.09525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0847 - val_loss: 0.0953\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0839\n","Epoch 179: val_loss improved from 0.09525 to 0.09478, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0839 - val_loss: 0.0948\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0836\n","Epoch 180: val_loss improved from 0.09478 to 0.09430, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0836 - val_loss: 0.0943\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0831\n","Epoch 181: val_loss improved from 0.09430 to 0.09384, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0831 - val_loss: 0.0938\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0824\n","Epoch 182: val_loss improved from 0.09384 to 0.09337, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0824 - val_loss: 0.0934\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0818\n","Epoch 183: val_loss improved from 0.09337 to 0.09291, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0818 - val_loss: 0.0929\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0813\n","Epoch 184: val_loss improved from 0.09291 to 0.09246, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0813 - val_loss: 0.0925\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0807\n","Epoch 185: val_loss improved from 0.09246 to 0.09201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0807 - val_loss: 0.0920\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0804\n","Epoch 186: val_loss improved from 0.09201 to 0.09157, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0804 - val_loss: 0.0916\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0794\n","Epoch 187: val_loss improved from 0.09157 to 0.09113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0794 - val_loss: 0.0911\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0793\n","Epoch 188: val_loss improved from 0.09113 to 0.09070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0793 - val_loss: 0.0907\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 189: val_loss improved from 0.09070 to 0.09027, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0790 - val_loss: 0.0903\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0781\n","Epoch 190: val_loss improved from 0.09027 to 0.08985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0781 - val_loss: 0.0899\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0777\n","Epoch 191: val_loss improved from 0.08985 to 0.08943, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0777 - val_loss: 0.0894\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0771\n","Epoch 192: val_loss improved from 0.08943 to 0.08902, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0771 - val_loss: 0.0890\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 193: val_loss improved from 0.08902 to 0.08861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0766 - val_loss: 0.0886\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0765\n","Epoch 194: val_loss improved from 0.08861 to 0.08820, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0765 - val_loss: 0.0882\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 195: val_loss improved from 0.08820 to 0.08780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0761 - val_loss: 0.0878\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0753\n","Epoch 196: val_loss improved from 0.08780 to 0.08741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0753 - val_loss: 0.0874\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0745\n","Epoch 197: val_loss improved from 0.08741 to 0.08702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0745 - val_loss: 0.0870\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0745\n","Epoch 198: val_loss improved from 0.08702 to 0.08663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0745 - val_loss: 0.0866\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0739\n","Epoch 199: val_loss improved from 0.08663 to 0.08625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0739 - val_loss: 0.0862\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0739\n","Epoch 200: val_loss improved from 0.08625 to 0.08587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0739 - val_loss: 0.0859\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 201: val_loss improved from 0.08587 to 0.08549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0727 - val_loss: 0.0855\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0725\n","Epoch 202: val_loss improved from 0.08549 to 0.08512, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0725 - val_loss: 0.0851\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0719\n","Epoch 203: val_loss improved from 0.08512 to 0.08475, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0719 - val_loss: 0.0848\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0721\n","Epoch 204: val_loss improved from 0.08475 to 0.08439, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0721 - val_loss: 0.0844\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0708\n","Epoch 205: val_loss improved from 0.08439 to 0.08403, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0708 - val_loss: 0.0840\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0710\n","Epoch 206: val_loss improved from 0.08403 to 0.08368, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0710 - val_loss: 0.0837\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0706\n","Epoch 207: val_loss improved from 0.08368 to 0.08333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0706 - val_loss: 0.0833\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0700\n","Epoch 208: val_loss improved from 0.08333 to 0.08298, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0700 - val_loss: 0.0830\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 209: val_loss improved from 0.08298 to 0.08264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0696 - val_loss: 0.0826\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0690\n","Epoch 210: val_loss improved from 0.08264 to 0.08230, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0690 - val_loss: 0.0823\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0684\n","Epoch 211: val_loss improved from 0.08230 to 0.08197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0684 - val_loss: 0.0820\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 212: val_loss improved from 0.08197 to 0.08164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0681 - val_loss: 0.0816\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 213: val_loss improved from 0.08164 to 0.08131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0681 - val_loss: 0.0813\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0673\n","Epoch 214: val_loss improved from 0.08131 to 0.08098, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0673 - val_loss: 0.0810\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 215: val_loss improved from 0.08098 to 0.08066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0671 - val_loss: 0.0807\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0663\n","Epoch 216: val_loss improved from 0.08066 to 0.08035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0663 - val_loss: 0.0803\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0663\n","Epoch 217: val_loss improved from 0.08035 to 0.08003, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0663 - val_loss: 0.0800\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0658\n","Epoch 218: val_loss improved from 0.08003 to 0.07972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0658 - val_loss: 0.0797\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 219: val_loss improved from 0.07972 to 0.07942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0651 - val_loss: 0.0794\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0650\n","Epoch 220: val_loss improved from 0.07942 to 0.07912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0650 - val_loss: 0.0791\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0650\n","Epoch 221: val_loss improved from 0.07912 to 0.07882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0650 - val_loss: 0.0788\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 222: val_loss improved from 0.07882 to 0.07852, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0641 - val_loss: 0.0785\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 223: val_loss improved from 0.07852 to 0.07823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0641 - val_loss: 0.0782\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0636\n","Epoch 224: val_loss improved from 0.07823 to 0.07794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0636 - val_loss: 0.0779\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0631\n","Epoch 225: val_loss improved from 0.07794 to 0.07765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0631 - val_loss: 0.0777\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 226: val_loss improved from 0.07765 to 0.07737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0629 - val_loss: 0.0774\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0626\n","Epoch 227: val_loss improved from 0.07737 to 0.07709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0626 - val_loss: 0.0771\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0623\n","Epoch 228: val_loss improved from 0.07709 to 0.07682, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0623 - val_loss: 0.0768\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0620\n","Epoch 229: val_loss improved from 0.07682 to 0.07654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0620 - val_loss: 0.0765\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 230: val_loss improved from 0.07654 to 0.07627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0616 - val_loss: 0.0763\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0612\n","Epoch 231: val_loss improved from 0.07627 to 0.07601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0612 - val_loss: 0.0760\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0609\n","Epoch 232: val_loss improved from 0.07601 to 0.07574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0609 - val_loss: 0.0757\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 233: val_loss improved from 0.07574 to 0.07548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0608 - val_loss: 0.0755\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 234: val_loss improved from 0.07548 to 0.07523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0599 - val_loss: 0.0752\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 235: val_loss improved from 0.07523 to 0.07497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0600 - val_loss: 0.0750\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0596\n","Epoch 236: val_loss improved from 0.07497 to 0.07472, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0596 - val_loss: 0.0747\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0594\n","Epoch 237: val_loss improved from 0.07472 to 0.07447, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0594 - val_loss: 0.0745\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0587\n","Epoch 238: val_loss improved from 0.07447 to 0.07422, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0587 - val_loss: 0.0742\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0584\n","Epoch 239: val_loss improved from 0.07422 to 0.07398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0584 - val_loss: 0.0740\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0584\n","Epoch 240: val_loss improved from 0.07398 to 0.07374, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0584 - val_loss: 0.0737\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 241: val_loss improved from 0.07374 to 0.07350, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0577 - val_loss: 0.0735\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0574\n","Epoch 242: val_loss improved from 0.07350 to 0.07327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0574 - val_loss: 0.0733\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 243: val_loss improved from 0.07327 to 0.07303, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0576 - val_loss: 0.0730\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 244: val_loss improved from 0.07303 to 0.07281, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0571 - val_loss: 0.0728\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 245: val_loss improved from 0.07281 to 0.07258, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0569 - val_loss: 0.0726\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0563\n","Epoch 246: val_loss improved from 0.07258 to 0.07235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0563 - val_loss: 0.0724\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0561\n","Epoch 247: val_loss improved from 0.07235 to 0.07213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0561 - val_loss: 0.0721\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 248: val_loss improved from 0.07213 to 0.07191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0559 - val_loss: 0.0719\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 249: val_loss improved from 0.07191 to 0.07169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0558 - val_loss: 0.0717\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0552\n","Epoch 250: val_loss improved from 0.07169 to 0.07148, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0552 - val_loss: 0.0715\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0552\n","Epoch 251: val_loss improved from 0.07148 to 0.07127, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0552 - val_loss: 0.0713\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 252: val_loss improved from 0.07127 to 0.07106, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0547 - val_loss: 0.0711\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0546\n","Epoch 253: val_loss improved from 0.07106 to 0.07085, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0546 - val_loss: 0.0708\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 254: val_loss improved from 0.07085 to 0.07065, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0543 - val_loss: 0.0706\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 255: val_loss improved from 0.07065 to 0.07044, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0542 - val_loss: 0.0704\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 256: val_loss improved from 0.07044 to 0.07024, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0537 - val_loss: 0.0702\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 257: val_loss improved from 0.07024 to 0.07005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0536 - val_loss: 0.0700\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0531\n","Epoch 258: val_loss improved from 0.07005 to 0.06985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0531 - val_loss: 0.0698\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0532\n","Epoch 259: val_loss improved from 0.06985 to 0.06966, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0532 - val_loss: 0.0697\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0526\n","Epoch 260: val_loss improved from 0.06966 to 0.06947, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0526 - val_loss: 0.0695\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0525\n","Epoch 261: val_loss improved from 0.06947 to 0.06928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0525 - val_loss: 0.0693\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0525\n","Epoch 262: val_loss improved from 0.06928 to 0.06909, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0525 - val_loss: 0.0691\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0520\n","Epoch 263: val_loss improved from 0.06909 to 0.06891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0520 - val_loss: 0.0689\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 264: val_loss improved from 0.06891 to 0.06872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0515 - val_loss: 0.0687\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 265: val_loss improved from 0.06872 to 0.06854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0513 - val_loss: 0.0685\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 266: val_loss improved from 0.06854 to 0.06837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0514 - val_loss: 0.0684\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0511\n","Epoch 267: val_loss improved from 0.06837 to 0.06819, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0511 - val_loss: 0.0682\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0508\n","Epoch 268: val_loss improved from 0.06819 to 0.06802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0508 - val_loss: 0.0680\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0508\n","Epoch 269: val_loss improved from 0.06802 to 0.06784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0508 - val_loss: 0.0678\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 270: val_loss improved from 0.06784 to 0.06767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0504 - val_loss: 0.0677\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 271: val_loss improved from 0.06767 to 0.06751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0504 - val_loss: 0.0675\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 272: val_loss improved from 0.06751 to 0.06734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0497 - val_loss: 0.0673\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 273: val_loss improved from 0.06734 to 0.06718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0497 - val_loss: 0.0672\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 274: val_loss improved from 0.06718 to 0.06701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0492 - val_loss: 0.0670\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 275: val_loss improved from 0.06701 to 0.06685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0494 - val_loss: 0.0669\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 276: val_loss improved from 0.06685 to 0.06669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0489 - val_loss: 0.0667\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0491\n","Epoch 277: val_loss improved from 0.06669 to 0.06654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0491 - val_loss: 0.0665\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 278: val_loss improved from 0.06654 to 0.06638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0487 - val_loss: 0.0664\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 279: val_loss improved from 0.06638 to 0.06623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0487 - val_loss: 0.0662\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 280: val_loss improved from 0.06623 to 0.06608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0483 - val_loss: 0.0661\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0480\n","Epoch 281: val_loss improved from 0.06608 to 0.06593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0480 - val_loss: 0.0659\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 282: val_loss improved from 0.06593 to 0.06578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0477 - val_loss: 0.0658\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 283: val_loss improved from 0.06578 to 0.06564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0476 - val_loss: 0.0656\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 284: val_loss improved from 0.06564 to 0.06549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0470 - val_loss: 0.0655\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 285: val_loss improved from 0.06549 to 0.06535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0471 - val_loss: 0.0654\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 286: val_loss improved from 0.06535 to 0.06521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0469 - val_loss: 0.0652\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 287: val_loss improved from 0.06521 to 0.06507, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0468 - val_loss: 0.0651\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 288: val_loss improved from 0.06507 to 0.06494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0467 - val_loss: 0.0649\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 289: val_loss improved from 0.06494 to 0.06480, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0468 - val_loss: 0.0648\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 290: val_loss improved from 0.06480 to 0.06467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0462 - val_loss: 0.0647\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 291: val_loss improved from 0.06467 to 0.06454, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0460 - val_loss: 0.0645\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 292: val_loss improved from 0.06454 to 0.06440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0456 - val_loss: 0.0644\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0459\n","Epoch 293: val_loss improved from 0.06440 to 0.06428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0459 - val_loss: 0.0643\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 294: val_loss improved from 0.06428 to 0.06415, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0457 - val_loss: 0.0641\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 295: val_loss improved from 0.06415 to 0.06402, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0451 - val_loss: 0.0640\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 296: val_loss improved from 0.06402 to 0.06390, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0455 - val_loss: 0.0639\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 297: val_loss improved from 0.06390 to 0.06378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0452 - val_loss: 0.0638\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0453\n","Epoch 298: val_loss improved from 0.06378 to 0.06365, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0453 - val_loss: 0.0637\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 299: val_loss improved from 0.06365 to 0.06353, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0450 - val_loss: 0.0635\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0443\n","Epoch 300: val_loss improved from 0.06353 to 0.06342, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0443 - val_loss: 0.0634\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 301: val_loss improved from 0.06342 to 0.06330, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0446 - val_loss: 0.0633\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 302: val_loss improved from 0.06330 to 0.06318, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0445 - val_loss: 0.0632\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0443\n","Epoch 303: val_loss improved from 0.06318 to 0.06307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0443 - val_loss: 0.0631\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 304: val_loss improved from 0.06307 to 0.06295, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0438 - val_loss: 0.0630\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 305: val_loss improved from 0.06295 to 0.06284, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0437 - val_loss: 0.0628\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 306: val_loss improved from 0.06284 to 0.06273, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0435 - val_loss: 0.0627\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 307: val_loss improved from 0.06273 to 0.06263, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0435 - val_loss: 0.0626\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0433\n","Epoch 308: val_loss improved from 0.06263 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0433 - val_loss: 0.0625\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 309: val_loss improved from 0.06252 to 0.06241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0428 - val_loss: 0.0624\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 310: val_loss improved from 0.06241 to 0.06231, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0430 - val_loss: 0.0623\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 311: val_loss improved from 0.06231 to 0.06220, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0426 - val_loss: 0.0622\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 312: val_loss improved from 0.06220 to 0.06210, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0426 - val_loss: 0.0621\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 313: val_loss improved from 0.06210 to 0.06200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0427 - val_loss: 0.0620\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 314: val_loss improved from 0.06200 to 0.06190, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0423 - val_loss: 0.0619\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 315: val_loss improved from 0.06190 to 0.06180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0424 - val_loss: 0.0618\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 316: val_loss improved from 0.06180 to 0.06170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0421 - val_loss: 0.0617\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 317: val_loss improved from 0.06170 to 0.06160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 318: val_loss improved from 0.06160 to 0.06151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0416 - val_loss: 0.0615\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 319: val_loss improved from 0.06151 to 0.06142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0415 - val_loss: 0.0614\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 320: val_loss improved from 0.06142 to 0.06132, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0417 - val_loss: 0.0613\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 321: val_loss improved from 0.06132 to 0.06123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0414 - val_loss: 0.0612\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 322: val_loss improved from 0.06123 to 0.06114, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0414 - val_loss: 0.0611\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 323: val_loss improved from 0.06114 to 0.06105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0407 - val_loss: 0.0611\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 324: val_loss improved from 0.06105 to 0.06096, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0408 - val_loss: 0.0610\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 325: val_loss improved from 0.06096 to 0.06088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0409 - val_loss: 0.0609\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 326: val_loss improved from 0.06088 to 0.06079, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0406 - val_loss: 0.0608\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 327: val_loss improved from 0.06079 to 0.06071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0408 - val_loss: 0.0607\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 328: val_loss improved from 0.06071 to 0.06062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0406 - val_loss: 0.0606\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 329: val_loss improved from 0.06062 to 0.06054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0403 - val_loss: 0.0605\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 330: val_loss improved from 0.06054 to 0.06046, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 331: val_loss improved from 0.06046 to 0.06038, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0398 - val_loss: 0.0604\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 332: val_loss improved from 0.06038 to 0.06030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0403 - val_loss: 0.0603\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 333: val_loss improved from 0.06030 to 0.06022, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0396 - val_loss: 0.0602\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 334: val_loss improved from 0.06022 to 0.06014, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 335: val_loss improved from 0.06014 to 0.06006, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 336: val_loss improved from 0.06006 to 0.05999, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0392 - val_loss: 0.0600\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 337: val_loss improved from 0.05999 to 0.05992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0391 - val_loss: 0.0599\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 338: val_loss improved from 0.05992 to 0.05984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0391 - val_loss: 0.0598\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 339: val_loss improved from 0.05984 to 0.05977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0392 - val_loss: 0.0598\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 340: val_loss improved from 0.05977 to 0.05970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0391 - val_loss: 0.0597\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 341: val_loss improved from 0.05970 to 0.05963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0389 - val_loss: 0.0596\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 342: val_loss improved from 0.05963 to 0.05956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0386 - val_loss: 0.0596\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 343: val_loss improved from 0.05956 to 0.05949, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0386 - val_loss: 0.0595\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 344: val_loss improved from 0.05949 to 0.05942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0387 - val_loss: 0.0594\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 345: val_loss improved from 0.05942 to 0.05935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0389 - val_loss: 0.0594\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 346: val_loss improved from 0.05935 to 0.05929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0385 - val_loss: 0.0593\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 347: val_loss improved from 0.05929 to 0.05922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0384 - val_loss: 0.0592\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 348: val_loss improved from 0.05922 to 0.05916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0378 - val_loss: 0.0592\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 349: val_loss improved from 0.05916 to 0.05909, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0380 - val_loss: 0.0591\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 350: val_loss improved from 0.05909 to 0.05903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0379 - val_loss: 0.0590\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 351: val_loss improved from 0.05903 to 0.05897, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0377 - val_loss: 0.0590\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 352: val_loss improved from 0.05897 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0381 - val_loss: 0.0589\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 353: val_loss improved from 0.05891 to 0.05885, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0373 - val_loss: 0.0588\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 354: val_loss improved from 0.05885 to 0.05879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0377 - val_loss: 0.0588\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 355: val_loss improved from 0.05879 to 0.05873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0374 - val_loss: 0.0587\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 356: val_loss improved from 0.05873 to 0.05867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0373 - val_loss: 0.0587\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 357: val_loss improved from 0.05867 to 0.05861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0376 - val_loss: 0.0586\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 358: val_loss improved from 0.05861 to 0.05855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0374 - val_loss: 0.0586\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 359: val_loss improved from 0.05855 to 0.05850, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0374 - val_loss: 0.0585\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 360: val_loss improved from 0.05850 to 0.05844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0371 - val_loss: 0.0584\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 361: val_loss improved from 0.05844 to 0.05839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0371 - val_loss: 0.0584\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 362: val_loss improved from 0.05839 to 0.05834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0368 - val_loss: 0.0583\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 363: val_loss improved from 0.05834 to 0.05828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0369 - val_loss: 0.0583\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 364: val_loss improved from 0.05828 to 0.05823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0366 - val_loss: 0.0582\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 365: val_loss improved from 0.05823 to 0.05818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0367 - val_loss: 0.0582\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 366: val_loss improved from 0.05818 to 0.05813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0363 - val_loss: 0.0581\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 367: val_loss improved from 0.05813 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0363 - val_loss: 0.0581\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 368: val_loss improved from 0.05808 to 0.05803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0362 - val_loss: 0.0580\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 369: val_loss improved from 0.05803 to 0.05798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0366 - val_loss: 0.0580\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 370: val_loss improved from 0.05798 to 0.05793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0358 - val_loss: 0.0579\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 371: val_loss improved from 0.05793 to 0.05789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0361 - val_loss: 0.0579\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 372: val_loss improved from 0.05789 to 0.05784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0358 - val_loss: 0.0578\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 373: val_loss improved from 0.05784 to 0.05779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0360 - val_loss: 0.0578\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 374: val_loss improved from 0.05779 to 0.05775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0361 - val_loss: 0.0577\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 375: val_loss improved from 0.05775 to 0.05770, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0356 - val_loss: 0.0577\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 376: val_loss improved from 0.05770 to 0.05766, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0354 - val_loss: 0.0577\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 377: val_loss improved from 0.05766 to 0.05761, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 378: val_loss improved from 0.05761 to 0.05757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0357 - val_loss: 0.0576\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 379: val_loss improved from 0.05757 to 0.05753, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 380: val_loss improved from 0.05753 to 0.05749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0356 - val_loss: 0.0575\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 381: val_loss improved from 0.05749 to 0.05745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0355 - val_loss: 0.0574\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 382: val_loss improved from 0.05745 to 0.05741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0357 - val_loss: 0.0574\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 383: val_loss improved from 0.05741 to 0.05737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0354 - val_loss: 0.0574\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 384: val_loss improved from 0.05737 to 0.05733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0354 - val_loss: 0.0573\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 385: val_loss improved from 0.05733 to 0.05729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0356 - val_loss: 0.0573\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 386: val_loss improved from 0.05729 to 0.05725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 387: val_loss improved from 0.05725 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0352 - val_loss: 0.0572\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 388: val_loss improved from 0.05721 to 0.05717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0346 - val_loss: 0.0572\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 389: val_loss improved from 0.05717 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0349 - val_loss: 0.0571\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 390: val_loss improved from 0.05714 to 0.05710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0350 - val_loss: 0.0571\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 391: val_loss improved from 0.05710 to 0.05707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 392: val_loss improved from 0.05707 to 0.05703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0346 - val_loss: 0.0570\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 393: val_loss improved from 0.05703 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0346 - val_loss: 0.0570\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 394: val_loss improved from 0.05699 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 395: val_loss improved from 0.05696 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0346 - val_loss: 0.0569\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 396: val_loss improved from 0.05693 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 397: val_loss improved from 0.05689 to 0.05686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0341 - val_loss: 0.0569\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 398: val_loss improved from 0.05686 to 0.05683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0338 - val_loss: 0.0568\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 399: val_loss improved from 0.05683 to 0.05680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0344 - val_loss: 0.0568\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 400: val_loss improved from 0.05680 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0342 - val_loss: 0.0568\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 401: val_loss improved from 0.05677 to 0.05673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0340 - val_loss: 0.0567\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 402: val_loss improved from 0.05673 to 0.05670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 403: val_loss improved from 0.05670 to 0.05667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 404: val_loss improved from 0.05667 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0340 - val_loss: 0.0566\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 405: val_loss improved from 0.05664 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0340 - val_loss: 0.0566\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 406: val_loss improved from 0.05661 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0339 - val_loss: 0.0566\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 407: val_loss improved from 0.05659 to 0.05656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0340 - val_loss: 0.0566\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 408: val_loss improved from 0.05656 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 409: val_loss improved from 0.05653 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 410: val_loss improved from 0.05650 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 411: val_loss improved from 0.05648 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0334 - val_loss: 0.0564\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 412: val_loss improved from 0.05645 to 0.05642, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0335 - val_loss: 0.0564\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 413: val_loss improved from 0.05642 to 0.05640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 414: val_loss improved from 0.05640 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 415: val_loss improved from 0.05637 to 0.05635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 416: val_loss improved from 0.05635 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 417: val_loss improved from 0.05632 to 0.05630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 418: val_loss improved from 0.05630 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0332 - val_loss: 0.0563\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 419: val_loss improved from 0.05627 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 420: val_loss improved from 0.05625 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0334 - val_loss: 0.0562\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 421: val_loss improved from 0.05623 to 0.05621, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0327 - val_loss: 0.0562\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 422: val_loss improved from 0.05621 to 0.05618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0330 - val_loss: 0.0562\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 423: val_loss improved from 0.05618 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0326 - val_loss: 0.0562\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 424: val_loss improved from 0.05616 to 0.05614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0333 - val_loss: 0.0561\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 425: val_loss improved from 0.05614 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0333 - val_loss: 0.0561\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 426: val_loss improved from 0.05612 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 427: val_loss improved from 0.05610 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 428: val_loss improved from 0.05608 to 0.05606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 429: val_loss improved from 0.05606 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0327 - val_loss: 0.0560\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 430: val_loss improved from 0.05604 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 431: val_loss improved from 0.05602 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 432: val_loss improved from 0.05600 to 0.05598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 433: val_loss improved from 0.05598 to 0.05596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 434: val_loss improved from 0.05596 to 0.05594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 435: val_loss improved from 0.05594 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0323 - val_loss: 0.0559\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 436: val_loss improved from 0.05592 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 437: val_loss improved from 0.05591 to 0.05589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 438: val_loss improved from 0.05589 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0326 - val_loss: 0.0559\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 439: val_loss improved from 0.05587 to 0.05585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0320 - val_loss: 0.0559\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 440: val_loss improved from 0.05585 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 441: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0326 - val_loss: 0.0558\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 442: val_loss improved from 0.05582 to 0.05581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0323 - val_loss: 0.0558\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 443: val_loss improved from 0.05581 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0321 - val_loss: 0.0558\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 444: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 445: val_loss improved from 0.05577 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0322 - val_loss: 0.0558\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 446: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0319 - val_loss: 0.0557\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 447: val_loss improved from 0.05574 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 448: val_loss improved from 0.05573 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0325 - val_loss: 0.0557\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 449: val_loss improved from 0.05571 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 450: val_loss improved from 0.05570 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0322 - val_loss: 0.0557\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 451: val_loss improved from 0.05569 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 452: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0319 - val_loss: 0.0557\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 453: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 454: val_loss improved from 0.05565 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0318 - val_loss: 0.0556\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 455: val_loss improved from 0.05563 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 456: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0316 - val_loss: 0.0556\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 457: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0317 - val_loss: 0.0556\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 458: val_loss improved from 0.05560 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0316 - val_loss: 0.0556\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 459: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0312 - val_loss: 0.0556\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 460: val_loss improved from 0.05557 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 461: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0316 - val_loss: 0.0555\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 462: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0316 - val_loss: 0.0555\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 463: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0314 - val_loss: 0.0555\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 464: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 465: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0315 - val_loss: 0.0555\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 466: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0313 - val_loss: 0.0555\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 467: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 468: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 469: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 470: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0313 - val_loss: 0.0555\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 471: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0313 - val_loss: 0.0554\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 472: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 473: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 474: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0311 - val_loss: 0.0554\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 475: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 476: val_loss improved from 0.05541 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 477: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 478: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 479: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 480: val_loss improved from 0.05538 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 481: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 482: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 483: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 484: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 485: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0310 - val_loss: 0.0553\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 486: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 487: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 488: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 489: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0307 - val_loss: 0.0553\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 490: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0307 - val_loss: 0.0553\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 491: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 492: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 493: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 494: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 495: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 496: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 497: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0308 - val_loss: 0.0553\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 498: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 499: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 500: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 501: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 502: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 503: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0303 - val_loss: 0.0552\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 504: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0306 - val_loss: 0.0552\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 505: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0305 - val_loss: 0.0552\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 506: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0304 - val_loss: 0.0552\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 507: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 508: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 509: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 510: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 511: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 512: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 513: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 514: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 515: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 516: val_loss improved from 0.05521 to 0.05520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 517: val_loss improved from 0.05520 to 0.05520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 518: val_loss improved from 0.05520 to 0.05520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 519: val_loss improved from 0.05520 to 0.05520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 520: val_loss improved from 0.05520 to 0.05520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0303 - val_loss: 0.0552\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 521: val_loss improved from 0.05520 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 522: val_loss improved from 0.05519 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 523: val_loss improved from 0.05519 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0303 - val_loss: 0.0552\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 524: val_loss improved from 0.05519 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 525: val_loss improved from 0.05519 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 526: val_loss improved from 0.05519 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 527: val_loss improved from 0.05519 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 528: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 529: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 530: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 531: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 532/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 532: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 533/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 533: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 534/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 534: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 535/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 535: val_loss improved from 0.05518 to 0.05518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 536/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 536: val_loss improved from 0.05518 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 537/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 537: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 538/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 538: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 539/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 539: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 540/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 540: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0302 - val_loss: 0.0552\n","Epoch 541/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 541: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 542/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 542: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 543/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 543: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 544/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 544: val_loss improved from 0.05517 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 545/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 545: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 546/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 546: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 547/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 547: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 548/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 548: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 549/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 549: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 550/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 550: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 551/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 551: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 552/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 552: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 553/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 553: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 554/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 554: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 555/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 555: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 556/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 556: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 557/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 557: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 558/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 558: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 559/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 559: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 560/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 560: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 561/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 561: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 562/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 562: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 563/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 563: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 564/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 564: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 565/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 565: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 566/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 566: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 567/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 567: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 568/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 568: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 569/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 569: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 570/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 570: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 571/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 571: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 572/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 572: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 573/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 573: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 574/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 574: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 575/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 575: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 576/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 576: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 577/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 577: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - val_loss: 0.0552\n","Epoch 578/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 578: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 579/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 579: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 580/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 580: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 581/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 581: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 582/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 582: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 583/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 583: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0288 - val_loss: 0.0552\n","Epoch 584/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 584: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 585/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 585: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0290 - val_loss: 0.0552\n","Epoch 586/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 586: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 587/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 587: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 588/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 588: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0290 - val_loss: 0.0552\n","Epoch 589/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 589: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 590/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 590: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0552\n","Epoch 591/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 591: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0552\n","Epoch 592/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 592: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 593/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 593: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0284 - val_loss: 0.0552\n","Epoch 594/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 594: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0552\n","Epoch 595/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 595: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 596/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 596: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 597/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 597: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0289 - val_loss: 0.0552\n","Epoch 598/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 598: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 599/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 599: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 600/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 600: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 601/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 601: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 602/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 602: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 603/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 603: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 604/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 604: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 605/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 605: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 606/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 606: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 607/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 607: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 608/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 608: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 609/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 609: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 610/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 610: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 611/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 611: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 612/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 612: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 613/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 613: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 614/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 614: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 615/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 615: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 616/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 616: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 617/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 617: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 618/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 618: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 619/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 619: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 620/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 620: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 621/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 621: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 622/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 622: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 623/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 623: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0284 - val_loss: 0.0553\n","Epoch 624/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 624: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 625/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 625: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 626/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 626: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0285 - val_loss: 0.0553\n","Epoch 627/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 627: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0285 - val_loss: 0.0553\n","Epoch 628/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 628: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 629/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 629: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 630/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 630: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0285 - val_loss: 0.0553\n","Epoch 631/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 631: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0284 - val_loss: 0.0553\n","Epoch 632/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 632: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 633/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 633: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 634/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 634: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0287 - val_loss: 0.0553\n","Epoch 635/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 635: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 636/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 636: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 637/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 637: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 638/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 638: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 639/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 639: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 640/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 640: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 641/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 641: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 642/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 642: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 643/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 643: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 644/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 644: val_loss did not improve from 0.05517\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0291 - val_loss: 0.0554\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0734\n","loss_and_metrics : 0.07337834686040878\n","1/1 [==============================] - 0s 59ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=20, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.8))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.5))\n","#model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.2))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.2))\n","model.add(Dense(units=4, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.2))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}