{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPZ1IDBmoJGNBQrT6kWL25E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rJdWY366qRno","executionInfo":{"status":"ok","timestamp":1695013668158,"user_tz":-540,"elapsed":15901,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"032af528-0ac7-4d07-d3a5-c1bb9d889f25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3713\n","Epoch 1: val_loss improved from inf to 0.34559, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 831ms/step - loss: 0.3713 - val_loss: 0.3456\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3576\n","Epoch 2: val_loss improved from 0.34559 to 0.33334, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3576 - val_loss: 0.3333\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3444\n","Epoch 3: val_loss improved from 0.33334 to 0.32158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3444 - val_loss: 0.3216\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3317\n","Epoch 4: val_loss improved from 0.32158 to 0.31028, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.3317 - val_loss: 0.3103\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3195\n","Epoch 5: val_loss improved from 0.31028 to 0.29940, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.3195 - val_loss: 0.2994\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3078\n","Epoch 6: val_loss improved from 0.29940 to 0.28892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.3078 - val_loss: 0.2889\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2964\n","Epoch 7: val_loss improved from 0.28892 to 0.27880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 184ms/step - loss: 0.2964 - val_loss: 0.2788\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2854\n","Epoch 8: val_loss improved from 0.27880 to 0.26902, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 150ms/step - loss: 0.2854 - val_loss: 0.2690\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2748\n","Epoch 9: val_loss improved from 0.26902 to 0.25957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 131ms/step - loss: 0.2748 - val_loss: 0.2596\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2645\n","Epoch 10: val_loss improved from 0.25957 to 0.25043, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.2645 - val_loss: 0.2504\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2546\n","Epoch 11: val_loss improved from 0.25043 to 0.24158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 244ms/step - loss: 0.2546 - val_loss: 0.2416\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2449\n","Epoch 12: val_loss improved from 0.24158 to 0.23301, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 165ms/step - loss: 0.2449 - val_loss: 0.2330\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2355\n","Epoch 13: val_loss improved from 0.23301 to 0.22470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.2355 - val_loss: 0.2247\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2264\n","Epoch 14: val_loss improved from 0.22470 to 0.21666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 136ms/step - loss: 0.2264 - val_loss: 0.2167\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2176\n","Epoch 15: val_loss improved from 0.21666 to 0.20886, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.2176 - val_loss: 0.2089\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 16: val_loss improved from 0.20886 to 0.20131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.2090 - val_loss: 0.2013\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2006\n","Epoch 17: val_loss improved from 0.20131 to 0.19400, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 131ms/step - loss: 0.2006 - val_loss: 0.1940\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1925\n","Epoch 18: val_loss improved from 0.19400 to 0.18692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.1925 - val_loss: 0.1869\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1847\n","Epoch 19: val_loss improved from 0.18692 to 0.18006, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 158ms/step - loss: 0.1847 - val_loss: 0.1801\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1771\n","Epoch 20: val_loss improved from 0.18006 to 0.17343, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.1771 - val_loss: 0.1734\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1697\n","Epoch 21: val_loss improved from 0.17343 to 0.16703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1697 - val_loss: 0.1670\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1625\n","Epoch 22: val_loss improved from 0.16703 to 0.16084, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 169ms/step - loss: 0.1625 - val_loss: 0.1608\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1556\n","Epoch 23: val_loss improved from 0.16084 to 0.15488, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 146ms/step - loss: 0.1556 - val_loss: 0.1549\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1488\n","Epoch 24: val_loss improved from 0.15488 to 0.14913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 237ms/step - loss: 0.1488 - val_loss: 0.1491\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1423\n","Epoch 25: val_loss improved from 0.14913 to 0.14359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1423 - val_loss: 0.1436\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1361\n","Epoch 26: val_loss improved from 0.14359 to 0.13826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1361 - val_loss: 0.1383\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1300\n","Epoch 27: val_loss improved from 0.13826 to 0.13315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.1300 - val_loss: 0.1332\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1242\n","Epoch 28: val_loss improved from 0.13315 to 0.12825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.1242 - val_loss: 0.1282\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1186\n","Epoch 29: val_loss improved from 0.12825 to 0.12356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.1186 - val_loss: 0.1236\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1132\n","Epoch 30: val_loss improved from 0.12356 to 0.11907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1132 - val_loss: 0.1191\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1080\n","Epoch 31: val_loss improved from 0.11907 to 0.11479, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1080 - val_loss: 0.1148\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1030\n","Epoch 32: val_loss improved from 0.11479 to 0.11071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1030 - val_loss: 0.1107\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0982\n","Epoch 33: val_loss improved from 0.11071 to 0.10683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0982 - val_loss: 0.1068\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0937\n","Epoch 34: val_loss improved from 0.10683 to 0.10314, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0937 - val_loss: 0.1031\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0893\n","Epoch 35: val_loss improved from 0.10314 to 0.09965, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0893 - val_loss: 0.0997\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0852\n","Epoch 36: val_loss improved from 0.09965 to 0.09636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0852 - val_loss: 0.0964\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0812\n","Epoch 37: val_loss improved from 0.09636 to 0.09324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.0812 - val_loss: 0.0932\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0775\n","Epoch 38: val_loss improved from 0.09324 to 0.09031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0775 - val_loss: 0.0903\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0739\n","Epoch 39: val_loss improved from 0.09031 to 0.08756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0739 - val_loss: 0.0876\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0705\n","Epoch 40: val_loss improved from 0.08756 to 0.08498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0705 - val_loss: 0.0850\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0674\n","Epoch 41: val_loss improved from 0.08498 to 0.08257, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0674 - val_loss: 0.0826\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0644\n","Epoch 42: val_loss improved from 0.08257 to 0.08032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0644 - val_loss: 0.0803\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0615\n","Epoch 43: val_loss improved from 0.08032 to 0.07823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0615 - val_loss: 0.0782\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 44: val_loss improved from 0.07823 to 0.07628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0589 - val_loss: 0.0763\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 45: val_loss improved from 0.07628 to 0.07448, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0564 - val_loss: 0.0745\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 46: val_loss improved from 0.07448 to 0.07282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0540 - val_loss: 0.0728\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 47: val_loss improved from 0.07282 to 0.07129, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0519 - val_loss: 0.0713\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 48: val_loss improved from 0.07129 to 0.06989, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0498 - val_loss: 0.0699\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 49: val_loss improved from 0.06989 to 0.06860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0479 - val_loss: 0.0686\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0461\n","Epoch 50: val_loss improved from 0.06860 to 0.06742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0461 - val_loss: 0.0674\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 51: val_loss improved from 0.06742 to 0.06635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0445 - val_loss: 0.0663\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 52: val_loss improved from 0.06635 to 0.06538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0430 - val_loss: 0.0654\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 53: val_loss improved from 0.06538 to 0.06450, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0416 - val_loss: 0.0645\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 54: val_loss improved from 0.06450 to 0.06370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0403 - val_loss: 0.0637\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 55: val_loss improved from 0.06370 to 0.06299, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0391 - val_loss: 0.0630\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 56: val_loss improved from 0.06299 to 0.06235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0379 - val_loss: 0.0623\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 57: val_loss improved from 0.06235 to 0.06178, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0369 - val_loss: 0.0618\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 58: val_loss improved from 0.06178 to 0.06127, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0360 - val_loss: 0.0613\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 59: val_loss improved from 0.06127 to 0.06083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0351 - val_loss: 0.0608\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 60: val_loss improved from 0.06083 to 0.06043, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0343 - val_loss: 0.0604\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 61: val_loss improved from 0.06043 to 0.06009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0336 - val_loss: 0.0601\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 62: val_loss improved from 0.06009 to 0.05979, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0329 - val_loss: 0.0598\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 63: val_loss improved from 0.05979 to 0.05953, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0323 - val_loss: 0.0595\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 64: val_loss improved from 0.05953 to 0.05931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0318 - val_loss: 0.0593\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 65: val_loss improved from 0.05931 to 0.05912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0313 - val_loss: 0.0591\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 66: val_loss improved from 0.05912 to 0.05897, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0308 - val_loss: 0.0590\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 67: val_loss improved from 0.05897 to 0.05884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0304 - val_loss: 0.0588\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 68: val_loss improved from 0.05884 to 0.05873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0300 - val_loss: 0.0587\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 69: val_loss improved from 0.05873 to 0.05865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0297 - val_loss: 0.0586\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 70: val_loss improved from 0.05865 to 0.05858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0294 - val_loss: 0.0586\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 71: val_loss improved from 0.05858 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0291 - val_loss: 0.0585\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 72: val_loss improved from 0.05854 to 0.05850, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0288 - val_loss: 0.0585\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 73: val_loss improved from 0.05850 to 0.05849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0286 - val_loss: 0.0585\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 74: val_loss improved from 0.05849 to 0.05848, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0284 - val_loss: 0.0585\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 75: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0282 - val_loss: 0.0585\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 76: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0280 - val_loss: 0.0585\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 77: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0278 - val_loss: 0.0585\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 78: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0277 - val_loss: 0.0585\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 79: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0275 - val_loss: 0.0586\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 80: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0274 - val_loss: 0.0586\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 81: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0273 - val_loss: 0.0587\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 82: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0272 - val_loss: 0.0587\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0271\n","Epoch 83: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0271 - val_loss: 0.0587\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0270\n","Epoch 84: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0270 - val_loss: 0.0588\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0270\n","Epoch 85: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0270 - val_loss: 0.0588\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0269\n","Epoch 86: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0269 - val_loss: 0.0589\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 87: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0268 - val_loss: 0.0589\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 88: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0268 - val_loss: 0.0590\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 89: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0267 - val_loss: 0.0591\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 90: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0266 - val_loss: 0.0591\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 91: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0266 - val_loss: 0.0592\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 92: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0266 - val_loss: 0.0592\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 93: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0265 - val_loss: 0.0593\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 94: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0265 - val_loss: 0.0593\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 95: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0264 - val_loss: 0.0594\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 96: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0264 - val_loss: 0.0594\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 97: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0264 - val_loss: 0.0595\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 98: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0263 - val_loss: 0.0595\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 99: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0263 - val_loss: 0.0596\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 100: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0263 - val_loss: 0.0596\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 101: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0263 - val_loss: 0.0597\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 102: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0262 - val_loss: 0.0597\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 103: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0262 - val_loss: 0.0597\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 104: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0262 - val_loss: 0.0598\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 105: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0262 - val_loss: 0.0598\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 106: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0261 - val_loss: 0.0599\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 107: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - val_loss: 0.0599\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 108: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0261 - val_loss: 0.0600\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 109: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - val_loss: 0.0600\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 110: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0261 - val_loss: 0.0600\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 111: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0260 - val_loss: 0.0601\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 112: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0260 - val_loss: 0.0601\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 113: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0260 - val_loss: 0.0601\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 114: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0260 - val_loss: 0.0602\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 115: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0260 - val_loss: 0.0602\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 116: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0259 - val_loss: 0.0602\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 117: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0259 - val_loss: 0.0603\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 118: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0259 - val_loss: 0.0603\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 119: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0259 - val_loss: 0.0603\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 120: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0259 - val_loss: 0.0604\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 121: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0259 - val_loss: 0.0604\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 122: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0258 - val_loss: 0.0604\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 123: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0258 - val_loss: 0.0605\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 124: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0258 - val_loss: 0.0605\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 125: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0258 - val_loss: 0.0605\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 126: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0258 - val_loss: 0.0606\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 127: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0258 - val_loss: 0.0606\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 128: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0257 - val_loss: 0.0606\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 129: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0257 - val_loss: 0.0606\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 130: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0257 - val_loss: 0.0607\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 131: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0257 - val_loss: 0.0607\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 132: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0257 - val_loss: 0.0607\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 133: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0257 - val_loss: 0.0608\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 134: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0256 - val_loss: 0.0608\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 135: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0256 - val_loss: 0.0608\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 136: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0256 - val_loss: 0.0608\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 137: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0256 - val_loss: 0.0609\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 138: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0256 - val_loss: 0.0609\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 139: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0256 - val_loss: 0.0609\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 140: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0255 - val_loss: 0.0609\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 141: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0255 - val_loss: 0.0610\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 142: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0255 - val_loss: 0.0610\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 143: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0255 - val_loss: 0.0610\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 144: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0255 - val_loss: 0.0610\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 145: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0255 - val_loss: 0.0611\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 146: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0254 - val_loss: 0.0611\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 147: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0254 - val_loss: 0.0611\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 148: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0254 - val_loss: 0.0611\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 149: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0254 - val_loss: 0.0612\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 150: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0254 - val_loss: 0.0612\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 151: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0254 - val_loss: 0.0612\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 152: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0253 - val_loss: 0.0612\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 153: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0253 - val_loss: 0.0612\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 154: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0253 - val_loss: 0.0613\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 155: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0253 - val_loss: 0.0613\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 156: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0253 - val_loss: 0.0613\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 157: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0253 - val_loss: 0.0613\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0253\n","Epoch 158: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0253 - val_loss: 0.0614\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 159: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0252 - val_loss: 0.0614\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 160: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0252 - val_loss: 0.0614\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 161: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0252 - val_loss: 0.0614\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 162: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0252 - val_loss: 0.0615\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 163: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0252 - val_loss: 0.0615\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0252\n","Epoch 164: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0252 - val_loss: 0.0615\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 165: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0251 - val_loss: 0.0615\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 166: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0251 - val_loss: 0.0616\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 167: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0251 - val_loss: 0.0616\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 168: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0251 - val_loss: 0.0616\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 169: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0251 - val_loss: 0.0616\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 170: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0251 - val_loss: 0.0616\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0251\n","Epoch 171: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0251 - val_loss: 0.0617\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0250\n","Epoch 172: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0250 - val_loss: 0.0617\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0250\n","Epoch 173: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0250 - val_loss: 0.0617\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0250\n","Epoch 174: val_loss did not improve from 0.05848\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0250 - val_loss: 0.0617\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0745\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e794c65a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["loss_and_metrics : 0.07445994019508362\n","1/1 [==============================] - 0s 86ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=200, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=200, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=200, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=200, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=200, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}