{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0enWhwSL0vgiSuAtsqtuM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9zKmVRrsii3a","executionInfo":{"status":"ok","timestamp":1695179995014,"user_tz":-540,"elapsed":84468,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"94ec04d6-98bd-42db-8f68-33dd6861dbd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3663\n","Epoch 1: val_loss improved from inf to 0.34939, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3663 - val_loss: 0.3494\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3629\n","Epoch 2: val_loss improved from 0.34939 to 0.34626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3629 - val_loss: 0.3463\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3596\n","Epoch 3: val_loss improved from 0.34626 to 0.34316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.3596 - val_loss: 0.3432\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3563\n","Epoch 4: val_loss improved from 0.34316 to 0.34009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3563 - val_loss: 0.3401\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3530\n","Epoch 5: val_loss improved from 0.34009 to 0.33706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3530 - val_loss: 0.3371\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3498\n","Epoch 6: val_loss improved from 0.33706 to 0.33405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3498 - val_loss: 0.3341\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3466\n","Epoch 7: val_loss improved from 0.33405 to 0.33108, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3466 - val_loss: 0.3311\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3434\n","Epoch 8: val_loss improved from 0.33108 to 0.32813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3434 - val_loss: 0.3281\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3402\n","Epoch 9: val_loss improved from 0.32813 to 0.32522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3402 - val_loss: 0.3252\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3371\n","Epoch 10: val_loss improved from 0.32522 to 0.32234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3371 - val_loss: 0.3223\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3341\n","Epoch 11: val_loss improved from 0.32234 to 0.31948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3341 - val_loss: 0.3195\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3310\n","Epoch 12: val_loss improved from 0.31948 to 0.31666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3310 - val_loss: 0.3167\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3279\n","Epoch 13: val_loss improved from 0.31666 to 0.31386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3279 - val_loss: 0.3139\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3250\n","Epoch 14: val_loss improved from 0.31386 to 0.31110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3250 - val_loss: 0.3111\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3220\n","Epoch 15: val_loss improved from 0.31110 to 0.30836, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3220 - val_loss: 0.3084\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3191\n","Epoch 16: val_loss improved from 0.30836 to 0.30565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3191 - val_loss: 0.3057\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3162\n","Epoch 17: val_loss improved from 0.30565 to 0.30297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3162 - val_loss: 0.3030\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3133\n","Epoch 18: val_loss improved from 0.30297 to 0.30032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.3133 - val_loss: 0.3003\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3105\n","Epoch 19: val_loss improved from 0.30032 to 0.29770, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3105 - val_loss: 0.2977\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3076\n","Epoch 20: val_loss improved from 0.29770 to 0.29510, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.3076 - val_loss: 0.2951\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3048\n","Epoch 21: val_loss improved from 0.29510 to 0.29252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3048 - val_loss: 0.2925\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3021\n","Epoch 22: val_loss improved from 0.29252 to 0.28998, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3021 - val_loss: 0.2900\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2993\n","Epoch 23: val_loss improved from 0.28998 to 0.28746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2993 - val_loss: 0.2875\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2966\n","Epoch 24: val_loss improved from 0.28746 to 0.28497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2966 - val_loss: 0.2850\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2940\n","Epoch 25: val_loss improved from 0.28497 to 0.28250, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2940 - val_loss: 0.2825\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2913\n","Epoch 26: val_loss improved from 0.28250 to 0.28006, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2913 - val_loss: 0.2801\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2887\n","Epoch 27: val_loss improved from 0.28006 to 0.27765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2887 - val_loss: 0.2776\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2861\n","Epoch 28: val_loss improved from 0.27765 to 0.27526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2861 - val_loss: 0.2753\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2835\n","Epoch 29: val_loss improved from 0.27526 to 0.27289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2835 - val_loss: 0.2729\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2810\n","Epoch 30: val_loss improved from 0.27289 to 0.27055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2810 - val_loss: 0.2705\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2784\n","Epoch 31: val_loss improved from 0.27055 to 0.26823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2784 - val_loss: 0.2682\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2759\n","Epoch 32: val_loss improved from 0.26823 to 0.26594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2759 - val_loss: 0.2659\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2735\n","Epoch 33: val_loss improved from 0.26594 to 0.26367, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2735 - val_loss: 0.2637\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2710\n","Epoch 34: val_loss improved from 0.26367 to 0.26142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2710 - val_loss: 0.2614\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2686\n","Epoch 35: val_loss improved from 0.26142 to 0.25920, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2686 - val_loss: 0.2592\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2662\n","Epoch 36: val_loss improved from 0.25920 to 0.25700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2662 - val_loss: 0.2570\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2638\n","Epoch 37: val_loss improved from 0.25700 to 0.25483, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2638 - val_loss: 0.2548\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2614\n","Epoch 38: val_loss improved from 0.25483 to 0.25267, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2614 - val_loss: 0.2527\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2591\n","Epoch 39: val_loss improved from 0.25267 to 0.25054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2591 - val_loss: 0.2505\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2568\n","Epoch 40: val_loss improved from 0.25054 to 0.24843, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2568 - val_loss: 0.2484\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2545\n","Epoch 41: val_loss improved from 0.24843 to 0.24634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2545 - val_loss: 0.2463\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2523\n","Epoch 42: val_loss improved from 0.24634 to 0.24428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2523 - val_loss: 0.2443\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2501\n","Epoch 43: val_loss improved from 0.24428 to 0.24223, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2501 - val_loss: 0.2422\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2478\n","Epoch 44: val_loss improved from 0.24223 to 0.24021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2478 - val_loss: 0.2402\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2456\n","Epoch 45: val_loss improved from 0.24021 to 0.23821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2456 - val_loss: 0.2382\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2435\n","Epoch 46: val_loss improved from 0.23821 to 0.23623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2435 - val_loss: 0.2362\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2413\n","Epoch 47: val_loss improved from 0.23623 to 0.23426, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2413 - val_loss: 0.2343\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2392\n","Epoch 48: val_loss improved from 0.23426 to 0.23232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2392 - val_loss: 0.2323\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2371\n","Epoch 49: val_loss improved from 0.23232 to 0.23041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2371 - val_loss: 0.2304\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2350\n","Epoch 50: val_loss improved from 0.23041 to 0.22850, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2350 - val_loss: 0.2285\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2329\n","Epoch 51: val_loss improved from 0.22850 to 0.22662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2329 - val_loss: 0.2266\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2309\n","Epoch 52: val_loss improved from 0.22662 to 0.22476, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2309 - val_loss: 0.2248\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2289\n","Epoch 53: val_loss improved from 0.22476 to 0.22292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2289 - val_loss: 0.2229\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2269\n","Epoch 54: val_loss improved from 0.22292 to 0.22110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2269 - val_loss: 0.2211\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2249\n","Epoch 55: val_loss improved from 0.22110 to 0.21930, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2249 - val_loss: 0.2193\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2229\n","Epoch 56: val_loss improved from 0.21930 to 0.21751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2229 - val_loss: 0.2175\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2210\n","Epoch 57: val_loss improved from 0.21751 to 0.21575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2210 - val_loss: 0.2157\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2191\n","Epoch 58: val_loss improved from 0.21575 to 0.21400, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2191 - val_loss: 0.2140\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2171\n","Epoch 59: val_loss improved from 0.21400 to 0.21227, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2171 - val_loss: 0.2123\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2153\n","Epoch 60: val_loss improved from 0.21227 to 0.21056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2153 - val_loss: 0.2106\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2134\n","Epoch 61: val_loss improved from 0.21056 to 0.20887, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2134 - val_loss: 0.2089\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2115\n","Epoch 62: val_loss improved from 0.20887 to 0.20719, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2115 - val_loss: 0.2072\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 63: val_loss improved from 0.20719 to 0.20553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2097 - val_loss: 0.2055\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2079\n","Epoch 64: val_loss improved from 0.20553 to 0.20389, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2079 - val_loss: 0.2039\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2061\n","Epoch 65: val_loss improved from 0.20389 to 0.20227, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2061 - val_loss: 0.2023\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2044\n","Epoch 66: val_loss improved from 0.20227 to 0.20066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2044 - val_loss: 0.2007\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2025\n","Epoch 67: val_loss improved from 0.20066 to 0.19907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2025 - val_loss: 0.1991\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2008\n","Epoch 68: val_loss improved from 0.19907 to 0.19750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2008 - val_loss: 0.1975\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1991\n","Epoch 69: val_loss improved from 0.19750 to 0.19594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1991 - val_loss: 0.1959\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1974\n","Epoch 70: val_loss improved from 0.19594 to 0.19440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1974 - val_loss: 0.1944\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1957\n","Epoch 71: val_loss improved from 0.19440 to 0.19288, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1957 - val_loss: 0.1929\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1940\n","Epoch 72: val_loss improved from 0.19288 to 0.19137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1940 - val_loss: 0.1914\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1924\n","Epoch 73: val_loss improved from 0.19137 to 0.18988, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1924 - val_loss: 0.1899\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1907\n","Epoch 74: val_loss improved from 0.18988 to 0.18840, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1907 - val_loss: 0.1884\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 75: val_loss improved from 0.18840 to 0.18694, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1891 - val_loss: 0.1869\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1875\n","Epoch 76: val_loss improved from 0.18694 to 0.18549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1875 - val_loss: 0.1855\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1859\n","Epoch 77: val_loss improved from 0.18549 to 0.18406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1859 - val_loss: 0.1841\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1844\n","Epoch 78: val_loss improved from 0.18406 to 0.18265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1844 - val_loss: 0.1826\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1827\n","Epoch 79: val_loss improved from 0.18265 to 0.18124, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1827 - val_loss: 0.1812\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1813\n","Epoch 80: val_loss improved from 0.18124 to 0.17986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1813 - val_loss: 0.1799\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1797\n","Epoch 81: val_loss improved from 0.17986 to 0.17849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1797 - val_loss: 0.1785\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1782\n","Epoch 82: val_loss improved from 0.17849 to 0.17713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1782 - val_loss: 0.1771\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1768\n","Epoch 83: val_loss improved from 0.17713 to 0.17579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1768 - val_loss: 0.1758\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1753\n","Epoch 84: val_loss improved from 0.17579 to 0.17446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1753 - val_loss: 0.1745\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1738\n","Epoch 85: val_loss improved from 0.17446 to 0.17314, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1738 - val_loss: 0.1731\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1723\n","Epoch 86: val_loss improved from 0.17314 to 0.17184, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1723 - val_loss: 0.1718\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1709\n","Epoch 87: val_loss improved from 0.17184 to 0.17055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1709 - val_loss: 0.1706\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1695\n","Epoch 88: val_loss improved from 0.17055 to 0.16928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1695 - val_loss: 0.1693\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1680\n","Epoch 89: val_loss improved from 0.16928 to 0.16802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1680 - val_loss: 0.1680\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1666\n","Epoch 90: val_loss improved from 0.16802 to 0.16677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1666 - val_loss: 0.1668\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1652\n","Epoch 91: val_loss improved from 0.16677 to 0.16554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1652 - val_loss: 0.1655\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1639\n","Epoch 92: val_loss improved from 0.16554 to 0.16431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1639 - val_loss: 0.1643\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1625\n","Epoch 93: val_loss improved from 0.16431 to 0.16311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.1625 - val_loss: 0.1631\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1612\n","Epoch 94: val_loss improved from 0.16311 to 0.16191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1612 - val_loss: 0.1619\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1599\n","Epoch 95: val_loss improved from 0.16191 to 0.16073, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.1599 - val_loss: 0.1607\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1585\n","Epoch 96: val_loss improved from 0.16073 to 0.15956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.1585 - val_loss: 0.1596\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1573\n","Epoch 97: val_loss improved from 0.15956 to 0.15840, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.1573 - val_loss: 0.1584\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1559\n","Epoch 98: val_loss improved from 0.15840 to 0.15725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1559 - val_loss: 0.1573\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1547\n","Epoch 99: val_loss improved from 0.15725 to 0.15612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.1547 - val_loss: 0.1561\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1534\n","Epoch 100: val_loss improved from 0.15612 to 0.15499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1534 - val_loss: 0.1550\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1522\n","Epoch 101: val_loss improved from 0.15499 to 0.15388, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1522 - val_loss: 0.1539\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1510\n","Epoch 102: val_loss improved from 0.15388 to 0.15279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1510 - val_loss: 0.1528\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1497\n","Epoch 103: val_loss improved from 0.15279 to 0.15170, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1497 - val_loss: 0.1517\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1485\n","Epoch 104: val_loss improved from 0.15170 to 0.15062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1485 - val_loss: 0.1506\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1473\n","Epoch 105: val_loss improved from 0.15062 to 0.14956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1473 - val_loss: 0.1496\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1461\n","Epoch 106: val_loss improved from 0.14956 to 0.14851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1461 - val_loss: 0.1485\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1450\n","Epoch 107: val_loss improved from 0.14851 to 0.14746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1450 - val_loss: 0.1475\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1438\n","Epoch 108: val_loss improved from 0.14746 to 0.14643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1438 - val_loss: 0.1464\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1427\n","Epoch 109: val_loss improved from 0.14643 to 0.14541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1427 - val_loss: 0.1454\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1414\n","Epoch 110: val_loss improved from 0.14541 to 0.14440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.1414 - val_loss: 0.1444\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1403\n","Epoch 111: val_loss improved from 0.14440 to 0.14341, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.1403 - val_loss: 0.1434\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1392\n","Epoch 112: val_loss improved from 0.14341 to 0.14242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1392 - val_loss: 0.1424\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1381\n","Epoch 113: val_loss improved from 0.14242 to 0.14144, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1381 - val_loss: 0.1414\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1369\n","Epoch 114: val_loss improved from 0.14144 to 0.14047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1369 - val_loss: 0.1405\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1359\n","Epoch 115: val_loss improved from 0.14047 to 0.13952, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.1359 - val_loss: 0.1395\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1348\n","Epoch 116: val_loss improved from 0.13952 to 0.13857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1348 - val_loss: 0.1386\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1339\n","Epoch 117: val_loss improved from 0.13857 to 0.13763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.1339 - val_loss: 0.1376\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1328\n","Epoch 118: val_loss improved from 0.13763 to 0.13671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.1328 - val_loss: 0.1367\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1316\n","Epoch 119: val_loss improved from 0.13671 to 0.13579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1316 - val_loss: 0.1358\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1307\n","Epoch 120: val_loss improved from 0.13579 to 0.13488, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.1307 - val_loss: 0.1349\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1296\n","Epoch 121: val_loss improved from 0.13488 to 0.13398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1296 - val_loss: 0.1340\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1286\n","Epoch 122: val_loss improved from 0.13398 to 0.13310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1286 - val_loss: 0.1331\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1276\n","Epoch 123: val_loss improved from 0.13310 to 0.13222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1276 - val_loss: 0.1322\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1266\n","Epoch 124: val_loss improved from 0.13222 to 0.13135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1266 - val_loss: 0.1313\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1257\n","Epoch 125: val_loss improved from 0.13135 to 0.13049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1257 - val_loss: 0.1305\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1247\n","Epoch 126: val_loss improved from 0.13049 to 0.12964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1247 - val_loss: 0.1296\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1237\n","Epoch 127: val_loss improved from 0.12964 to 0.12880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1237 - val_loss: 0.1288\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1228\n","Epoch 128: val_loss improved from 0.12880 to 0.12796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1228 - val_loss: 0.1280\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1218\n","Epoch 129: val_loss improved from 0.12796 to 0.12714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1218 - val_loss: 0.1271\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1208\n","Epoch 130: val_loss improved from 0.12714 to 0.12632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1208 - val_loss: 0.1263\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1199\n","Epoch 131: val_loss improved from 0.12632 to 0.12552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1199 - val_loss: 0.1255\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1190\n","Epoch 132: val_loss improved from 0.12552 to 0.12472, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1190 - val_loss: 0.1247\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1181\n","Epoch 133: val_loss improved from 0.12472 to 0.12393, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1181 - val_loss: 0.1239\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1172\n","Epoch 134: val_loss improved from 0.12393 to 0.12315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1172 - val_loss: 0.1231\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1163\n","Epoch 135: val_loss improved from 0.12315 to 0.12238, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1163 - val_loss: 0.1224\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1155\n","Epoch 136: val_loss improved from 0.12238 to 0.12161, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1155 - val_loss: 0.1216\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1146\n","Epoch 137: val_loss improved from 0.12161 to 0.12086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1146 - val_loss: 0.1209\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1137\n","Epoch 138: val_loss improved from 0.12086 to 0.12011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1137 - val_loss: 0.1201\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1128\n","Epoch 139: val_loss improved from 0.12011 to 0.11937, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1128 - val_loss: 0.1194\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1120\n","Epoch 140: val_loss improved from 0.11937 to 0.11864, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1120 - val_loss: 0.1186\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1112\n","Epoch 141: val_loss improved from 0.11864 to 0.11791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1112 - val_loss: 0.1179\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1104\n","Epoch 142: val_loss improved from 0.11791 to 0.11720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1104 - val_loss: 0.1172\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1096\n","Epoch 143: val_loss improved from 0.11720 to 0.11649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1096 - val_loss: 0.1165\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1087\n","Epoch 144: val_loss improved from 0.11649 to 0.11578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1087 - val_loss: 0.1158\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1079\n","Epoch 145: val_loss improved from 0.11578 to 0.11509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1079 - val_loss: 0.1151\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 146: val_loss improved from 0.11509 to 0.11440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1071 - val_loss: 0.1144\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 147: val_loss improved from 0.11440 to 0.11373, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1063 - val_loss: 0.1137\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1056\n","Epoch 148: val_loss improved from 0.11373 to 0.11305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1056 - val_loss: 0.1131\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1048\n","Epoch 149: val_loss improved from 0.11305 to 0.11239, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1048 - val_loss: 0.1124\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1040\n","Epoch 150: val_loss improved from 0.11239 to 0.11173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1040 - val_loss: 0.1117\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1033\n","Epoch 151: val_loss improved from 0.11173 to 0.11108, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1033 - val_loss: 0.1111\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1025\n","Epoch 152: val_loss improved from 0.11108 to 0.11044, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1025 - val_loss: 0.1104\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1018\n","Epoch 153: val_loss improved from 0.11044 to 0.10980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1018 - val_loss: 0.1098\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1010\n","Epoch 154: val_loss improved from 0.10980 to 0.10917, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1010 - val_loss: 0.1092\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1003\n","Epoch 155: val_loss improved from 0.10917 to 0.10855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1003 - val_loss: 0.1085\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0996\n","Epoch 156: val_loss improved from 0.10855 to 0.10793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0996 - val_loss: 0.1079\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0989\n","Epoch 157: val_loss improved from 0.10793 to 0.10732, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0989 - val_loss: 0.1073\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0981\n","Epoch 158: val_loss improved from 0.10732 to 0.10672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0981 - val_loss: 0.1067\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 159: val_loss improved from 0.10672 to 0.10612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0975 - val_loss: 0.1061\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0967\n","Epoch 160: val_loss improved from 0.10612 to 0.10553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0967 - val_loss: 0.1055\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0961\n","Epoch 161: val_loss improved from 0.10553 to 0.10495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0961 - val_loss: 0.1050\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0954\n","Epoch 162: val_loss improved from 0.10495 to 0.10437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0954 - val_loss: 0.1044\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0948\n","Epoch 163: val_loss improved from 0.10437 to 0.10380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0948 - val_loss: 0.1038\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0942\n","Epoch 164: val_loss improved from 0.10380 to 0.10324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0942 - val_loss: 0.1032\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0934\n","Epoch 165: val_loss improved from 0.10324 to 0.10268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0934 - val_loss: 0.1027\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0928\n","Epoch 166: val_loss improved from 0.10268 to 0.10213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0928 - val_loss: 0.1021\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0922\n","Epoch 167: val_loss improved from 0.10213 to 0.10158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0922 - val_loss: 0.1016\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0915\n","Epoch 168: val_loss improved from 0.10158 to 0.10104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0915 - val_loss: 0.1010\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0909\n","Epoch 169: val_loss improved from 0.10104 to 0.10050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0909 - val_loss: 0.1005\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0902\n","Epoch 170: val_loss improved from 0.10050 to 0.09997, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0902 - val_loss: 0.1000\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0896\n","Epoch 171: val_loss improved from 0.09997 to 0.09945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0896 - val_loss: 0.0994\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0891\n","Epoch 172: val_loss improved from 0.09945 to 0.09893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0891 - val_loss: 0.0989\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0884\n","Epoch 173: val_loss improved from 0.09893 to 0.09842, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0884 - val_loss: 0.0984\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0878\n","Epoch 174: val_loss improved from 0.09842 to 0.09791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0878 - val_loss: 0.0979\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0872\n","Epoch 175: val_loss improved from 0.09791 to 0.09741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0872 - val_loss: 0.0974\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0866\n","Epoch 176: val_loss improved from 0.09741 to 0.09692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0866 - val_loss: 0.0969\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0861\n","Epoch 177: val_loss improved from 0.09692 to 0.09643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0861 - val_loss: 0.0964\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0854\n","Epoch 178: val_loss improved from 0.09643 to 0.09594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0854 - val_loss: 0.0959\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0849\n","Epoch 179: val_loss improved from 0.09594 to 0.09546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0849 - val_loss: 0.0955\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0843\n","Epoch 180: val_loss improved from 0.09546 to 0.09499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0843 - val_loss: 0.0950\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0837\n","Epoch 181: val_loss improved from 0.09499 to 0.09452, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0837 - val_loss: 0.0945\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0832\n","Epoch 182: val_loss improved from 0.09452 to 0.09405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0832 - val_loss: 0.0941\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0826\n","Epoch 183: val_loss improved from 0.09405 to 0.09359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0826 - val_loss: 0.0936\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0821\n","Epoch 184: val_loss improved from 0.09359 to 0.09314, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0821 - val_loss: 0.0931\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0816\n","Epoch 185: val_loss improved from 0.09314 to 0.09269, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0816 - val_loss: 0.0927\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 186: val_loss improved from 0.09269 to 0.09225, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0810 - val_loss: 0.0922\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0805\n","Epoch 187: val_loss improved from 0.09225 to 0.09181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0805 - val_loss: 0.0918\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0800\n","Epoch 188: val_loss improved from 0.09181 to 0.09137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0800 - val_loss: 0.0914\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0794\n","Epoch 189: val_loss improved from 0.09137 to 0.09094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0794 - val_loss: 0.0909\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 190: val_loss improved from 0.09094 to 0.09052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0790 - val_loss: 0.0905\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0784\n","Epoch 191: val_loss improved from 0.09052 to 0.09010, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0784 - val_loss: 0.0901\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0779\n","Epoch 192: val_loss improved from 0.09010 to 0.08968, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0779 - val_loss: 0.0897\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0775\n","Epoch 193: val_loss improved from 0.08968 to 0.08927, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0775 - val_loss: 0.0893\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0769\n","Epoch 194: val_loss improved from 0.08927 to 0.08886, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0769 - val_loss: 0.0889\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0764\n","Epoch 195: val_loss improved from 0.08886 to 0.08846, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0764 - val_loss: 0.0885\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0760\n","Epoch 196: val_loss improved from 0.08846 to 0.08806, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0760 - val_loss: 0.0881\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 197: val_loss improved from 0.08806 to 0.08767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0755 - val_loss: 0.0877\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0750\n","Epoch 198: val_loss improved from 0.08767 to 0.08728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0750 - val_loss: 0.0873\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0746\n","Epoch 199: val_loss improved from 0.08728 to 0.08689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0746 - val_loss: 0.0869\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0740\n","Epoch 200: val_loss improved from 0.08689 to 0.08651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0740 - val_loss: 0.0865\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0736\n","Epoch 201: val_loss improved from 0.08651 to 0.08614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0736 - val_loss: 0.0861\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0732\n","Epoch 202: val_loss improved from 0.08614 to 0.08576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0732 - val_loss: 0.0858\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 203: val_loss improved from 0.08576 to 0.08539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0727 - val_loss: 0.0854\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0723\n","Epoch 204: val_loss improved from 0.08539 to 0.08503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0723 - val_loss: 0.0850\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0718\n","Epoch 205: val_loss improved from 0.08503 to 0.08467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0718 - val_loss: 0.0847\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0714\n","Epoch 206: val_loss improved from 0.08467 to 0.08431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0714 - val_loss: 0.0843\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0710\n","Epoch 207: val_loss improved from 0.08431 to 0.08396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0710 - val_loss: 0.0840\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0705\n","Epoch 208: val_loss improved from 0.08396 to 0.08361, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0705 - val_loss: 0.0836\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0701\n","Epoch 209: val_loss improved from 0.08361 to 0.08327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0701 - val_loss: 0.0833\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0697\n","Epoch 210: val_loss improved from 0.08327 to 0.08293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0697 - val_loss: 0.0829\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0693\n","Epoch 211: val_loss improved from 0.08293 to 0.08259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0693 - val_loss: 0.0826\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0689\n","Epoch 212: val_loss improved from 0.08259 to 0.08226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0689 - val_loss: 0.0823\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0685\n","Epoch 213: val_loss improved from 0.08226 to 0.08193, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0685 - val_loss: 0.0819\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 214: val_loss improved from 0.08193 to 0.08160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0681 - val_loss: 0.0816\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 215: val_loss improved from 0.08160 to 0.08128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0676 - val_loss: 0.0813\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0673\n","Epoch 216: val_loss improved from 0.08128 to 0.08096, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0673 - val_loss: 0.0810\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0669\n","Epoch 217: val_loss improved from 0.08096 to 0.08064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0669 - val_loss: 0.0806\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0665\n","Epoch 218: val_loss improved from 0.08064 to 0.08033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0665 - val_loss: 0.0803\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 219: val_loss improved from 0.08033 to 0.08002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0661 - val_loss: 0.0800\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0658\n","Epoch 220: val_loss improved from 0.08002 to 0.07972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0658 - val_loss: 0.0797\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0654\n","Epoch 221: val_loss improved from 0.07972 to 0.07942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0654 - val_loss: 0.0794\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 222: val_loss improved from 0.07942 to 0.07912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0651 - val_loss: 0.0791\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0646\n","Epoch 223: val_loss improved from 0.07912 to 0.07883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0646 - val_loss: 0.0788\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0642\n","Epoch 224: val_loss improved from 0.07883 to 0.07853, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0642 - val_loss: 0.0785\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0639\n","Epoch 225: val_loss improved from 0.07853 to 0.07825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0639 - val_loss: 0.0782\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0636\n","Epoch 226: val_loss improved from 0.07825 to 0.07796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0636 - val_loss: 0.0780\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0632\n","Epoch 227: val_loss improved from 0.07796 to 0.07768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0632 - val_loss: 0.0777\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 228: val_loss improved from 0.07768 to 0.07740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0629 - val_loss: 0.0774\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0625\n","Epoch 229: val_loss improved from 0.07740 to 0.07712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0625 - val_loss: 0.0771\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 230: val_loss improved from 0.07712 to 0.07685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0622 - val_loss: 0.0769\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0619\n","Epoch 231: val_loss improved from 0.07685 to 0.07658, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0619 - val_loss: 0.0766\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0615\n","Epoch 232: val_loss improved from 0.07658 to 0.07632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0615 - val_loss: 0.0763\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0611\n","Epoch 233: val_loss improved from 0.07632 to 0.07605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0611 - val_loss: 0.0761\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 234: val_loss improved from 0.07605 to 0.07579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0608 - val_loss: 0.0758\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0606\n","Epoch 235: val_loss improved from 0.07579 to 0.07553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0606 - val_loss: 0.0755\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0602\n","Epoch 236: val_loss improved from 0.07553 to 0.07528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0602 - val_loss: 0.0753\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 237: val_loss improved from 0.07528 to 0.07503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0599 - val_loss: 0.0750\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0596\n","Epoch 238: val_loss improved from 0.07503 to 0.07478, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0596 - val_loss: 0.0748\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0592\n","Epoch 239: val_loss improved from 0.07478 to 0.07453, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0592 - val_loss: 0.0745\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0590\n","Epoch 240: val_loss improved from 0.07453 to 0.07429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0590 - val_loss: 0.0743\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 241: val_loss improved from 0.07429 to 0.07405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0586 - val_loss: 0.0740\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0583\n","Epoch 242: val_loss improved from 0.07405 to 0.07381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0583 - val_loss: 0.0738\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0580\n","Epoch 243: val_loss improved from 0.07381 to 0.07358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0580 - val_loss: 0.0736\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 244: val_loss improved from 0.07358 to 0.07334, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0577 - val_loss: 0.0733\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0573\n","Epoch 245: val_loss improved from 0.07334 to 0.07311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0573 - val_loss: 0.0731\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 246: val_loss improved from 0.07311 to 0.07289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0571 - val_loss: 0.0729\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 247: val_loss improved from 0.07289 to 0.07266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0569 - val_loss: 0.0727\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0566\n","Epoch 248: val_loss improved from 0.07266 to 0.07244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0566 - val_loss: 0.0724\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0563\n","Epoch 249: val_loss improved from 0.07244 to 0.07222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0563 - val_loss: 0.0722\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0560\n","Epoch 250: val_loss improved from 0.07222 to 0.07200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 131ms/step - loss: 0.0560 - val_loss: 0.0720\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 251: val_loss improved from 0.07200 to 0.07179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0557 - val_loss: 0.0718\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 252: val_loss improved from 0.07179 to 0.07158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0554 - val_loss: 0.0716\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 253: val_loss improved from 0.07158 to 0.07137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0551 - val_loss: 0.0714\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0549\n","Epoch 254: val_loss improved from 0.07137 to 0.07116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0549 - val_loss: 0.0712\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 255: val_loss improved from 0.07116 to 0.07095, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0547 - val_loss: 0.0710\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0544\n","Epoch 256: val_loss improved from 0.07095 to 0.07075, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0544 - val_loss: 0.0708\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0541\n","Epoch 257: val_loss improved from 0.07075 to 0.07055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0541 - val_loss: 0.0706\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0538\n","Epoch 258: val_loss improved from 0.07055 to 0.07035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0538 - val_loss: 0.0704\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 259: val_loss improved from 0.07035 to 0.07016, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0537 - val_loss: 0.0702\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 260: val_loss improved from 0.07016 to 0.06996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0534 - val_loss: 0.0700\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 261: val_loss improved from 0.06996 to 0.06977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0530 - val_loss: 0.0698\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0529\n","Epoch 262: val_loss improved from 0.06977 to 0.06958, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0529 - val_loss: 0.0696\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0526\n","Epoch 263: val_loss improved from 0.06958 to 0.06940, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0526 - val_loss: 0.0694\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 264: val_loss improved from 0.06940 to 0.06921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 153ms/step - loss: 0.0524 - val_loss: 0.0692\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 265: val_loss improved from 0.06921 to 0.06903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0521 - val_loss: 0.0690\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 266: val_loss improved from 0.06903 to 0.06885, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0519 - val_loss: 0.0688\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0517\n","Epoch 267: val_loss improved from 0.06885 to 0.06867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0517 - val_loss: 0.0687\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 268: val_loss improved from 0.06867 to 0.06849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0514 - val_loss: 0.0685\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 269: val_loss improved from 0.06849 to 0.06832, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0513 - val_loss: 0.0683\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0510\n","Epoch 270: val_loss improved from 0.06832 to 0.06815, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0510 - val_loss: 0.0681\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 271: val_loss improved from 0.06815 to 0.06798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0506 - val_loss: 0.0680\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 272: val_loss improved from 0.06798 to 0.06781, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0506 - val_loss: 0.0678\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0502\n","Epoch 273: val_loss improved from 0.06781 to 0.06764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0502 - val_loss: 0.0676\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 274: val_loss improved from 0.06764 to 0.06748, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0500 - val_loss: 0.0675\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 275: val_loss improved from 0.06748 to 0.06731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0498 - val_loss: 0.0673\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 276: val_loss improved from 0.06731 to 0.06715, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0497 - val_loss: 0.0672\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0493\n","Epoch 277: val_loss improved from 0.06715 to 0.06699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0493 - val_loss: 0.0670\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0491\n","Epoch 278: val_loss improved from 0.06699 to 0.06684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0491 - val_loss: 0.0668\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 279: val_loss improved from 0.06684 to 0.06668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0490 - val_loss: 0.0667\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 280: val_loss improved from 0.06668 to 0.06653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0488 - val_loss: 0.0665\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0486\n","Epoch 281: val_loss improved from 0.06653 to 0.06638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0486 - val_loss: 0.0664\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 282: val_loss improved from 0.06638 to 0.06623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0484 - val_loss: 0.0662\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 283: val_loss improved from 0.06623 to 0.06608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0481 - val_loss: 0.0661\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0480\n","Epoch 284: val_loss improved from 0.06608 to 0.06593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0480 - val_loss: 0.0659\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 285: val_loss improved from 0.06593 to 0.06579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0477 - val_loss: 0.0658\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 286: val_loss improved from 0.06579 to 0.06565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0476 - val_loss: 0.0656\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 287: val_loss improved from 0.06565 to 0.06550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0474 - val_loss: 0.0655\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 288: val_loss improved from 0.06550 to 0.06536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0472 - val_loss: 0.0654\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 289: val_loss improved from 0.06536 to 0.06523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0470 - val_loss: 0.0652\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 290: val_loss improved from 0.06523 to 0.06509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0468 - val_loss: 0.0651\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 291: val_loss improved from 0.06509 to 0.06496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0467 - val_loss: 0.0650\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0465\n","Epoch 292: val_loss improved from 0.06496 to 0.06482, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0465 - val_loss: 0.0648\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 293: val_loss improved from 0.06482 to 0.06469, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0462 - val_loss: 0.0647\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 294: val_loss improved from 0.06469 to 0.06456, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0460 - val_loss: 0.0646\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 295: val_loss improved from 0.06456 to 0.06443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0458 - val_loss: 0.0644\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 296: val_loss improved from 0.06443 to 0.06431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0458 - val_loss: 0.0643\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 297: val_loss improved from 0.06431 to 0.06418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0456 - val_loss: 0.0642\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 298: val_loss improved from 0.06418 to 0.06406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0454 - val_loss: 0.0641\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 299: val_loss improved from 0.06406 to 0.06393, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0451 - val_loss: 0.0639\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 300: val_loss improved from 0.06393 to 0.06381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0451 - val_loss: 0.0638\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 301: val_loss improved from 0.06381 to 0.06369, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0448 - val_loss: 0.0637\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 302: val_loss improved from 0.06369 to 0.06358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0447 - val_loss: 0.0636\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 303: val_loss improved from 0.06358 to 0.06346, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0446 - val_loss: 0.0635\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 304: val_loss improved from 0.06346 to 0.06334, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0444 - val_loss: 0.0633\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 305: val_loss improved from 0.06334 to 0.06323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0442 - val_loss: 0.0632\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 306: val_loss improved from 0.06323 to 0.06312, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0440 - val_loss: 0.0631\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 307: val_loss improved from 0.06312 to 0.06301, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0439 - val_loss: 0.0630\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 308: val_loss improved from 0.06301 to 0.06290, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0436 - val_loss: 0.0629\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 309: val_loss improved from 0.06290 to 0.06279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0436 - val_loss: 0.0628\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 310: val_loss improved from 0.06279 to 0.06268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0435 - val_loss: 0.0627\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 311: val_loss improved from 0.06268 to 0.06257, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0432 - val_loss: 0.0626\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 312: val_loss improved from 0.06257 to 0.06247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0431 - val_loss: 0.0625\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 313: val_loss improved from 0.06247 to 0.06237, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0429 - val_loss: 0.0624\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 314: val_loss improved from 0.06237 to 0.06226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0429 - val_loss: 0.0623\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 315: val_loss improved from 0.06226 to 0.06216, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0427 - val_loss: 0.0622\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 316: val_loss improved from 0.06216 to 0.06206, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0426 - val_loss: 0.0621\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 317: val_loss improved from 0.06206 to 0.06197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0424 - val_loss: 0.0620\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 318: val_loss improved from 0.06197 to 0.06187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0422 - val_loss: 0.0619\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 319: val_loss improved from 0.06187 to 0.06177, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0421 - val_loss: 0.0618\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 320: val_loss improved from 0.06177 to 0.06168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0420 - val_loss: 0.0617\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 321: val_loss improved from 0.06168 to 0.06158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 322: val_loss improved from 0.06158 to 0.06149, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0417 - val_loss: 0.0615\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 323: val_loss improved from 0.06149 to 0.06140, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0416 - val_loss: 0.0614\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 324: val_loss improved from 0.06140 to 0.06131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0414 - val_loss: 0.0613\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 325: val_loss improved from 0.06131 to 0.06122, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0412 - val_loss: 0.0612\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 326: val_loss improved from 0.06122 to 0.06113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0411 - val_loss: 0.0611\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 327: val_loss improved from 0.06113 to 0.06104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0410 - val_loss: 0.0610\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 328: val_loss improved from 0.06104 to 0.06096, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0409 - val_loss: 0.0610\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 329: val_loss improved from 0.06096 to 0.06087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0407 - val_loss: 0.0609\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 330: val_loss improved from 0.06087 to 0.06079, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0408 - val_loss: 0.0608\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 331: val_loss improved from 0.06079 to 0.06071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0406 - val_loss: 0.0607\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 332: val_loss improved from 0.06071 to 0.06063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0404 - val_loss: 0.0606\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 333: val_loss improved from 0.06063 to 0.06055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 334: val_loss improved from 0.06055 to 0.06047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 335: val_loss improved from 0.06047 to 0.06039, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0400 - val_loss: 0.0604\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 336: val_loss improved from 0.06039 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0399 - val_loss: 0.0603\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 337: val_loss improved from 0.06031 to 0.06023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0397 - val_loss: 0.0602\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 338: val_loss improved from 0.06023 to 0.06016, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0397 - val_loss: 0.0602\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 339: val_loss improved from 0.06016 to 0.06008, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0395 - val_loss: 0.0601\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 340: val_loss improved from 0.06008 to 0.06001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0394 - val_loss: 0.0600\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 341: val_loss improved from 0.06001 to 0.05993, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0393 - val_loss: 0.0599\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 342: val_loss improved from 0.05993 to 0.05986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0392 - val_loss: 0.0599\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 343: val_loss improved from 0.05986 to 0.05979, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0391 - val_loss: 0.0598\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 344: val_loss improved from 0.05979 to 0.05972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0391 - val_loss: 0.0597\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 345: val_loss improved from 0.05972 to 0.05965, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0389 - val_loss: 0.0597\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 346: val_loss improved from 0.05965 to 0.05958, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0387 - val_loss: 0.0596\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 347: val_loss improved from 0.05958 to 0.05952, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0387 - val_loss: 0.0595\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 348: val_loss improved from 0.05952 to 0.05945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0386 - val_loss: 0.0594\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 349: val_loss improved from 0.05945 to 0.05938, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0384 - val_loss: 0.0594\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 350: val_loss improved from 0.05938 to 0.05932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0384 - val_loss: 0.0593\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 351: val_loss improved from 0.05932 to 0.05925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0383 - val_loss: 0.0593\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 352: val_loss improved from 0.05925 to 0.05919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0383 - val_loss: 0.0592\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 353: val_loss improved from 0.05919 to 0.05913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0380 - val_loss: 0.0591\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 354: val_loss improved from 0.05913 to 0.05907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0379 - val_loss: 0.0591\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 355: val_loss improved from 0.05907 to 0.05900, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0380 - val_loss: 0.0590\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 356: val_loss improved from 0.05900 to 0.05894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0379 - val_loss: 0.0589\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 357: val_loss improved from 0.05894 to 0.05889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0376 - val_loss: 0.0589\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 358: val_loss improved from 0.05889 to 0.05883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 359: val_loss improved from 0.05883 to 0.05877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 360: val_loss improved from 0.05877 to 0.05871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0374 - val_loss: 0.0587\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 361: val_loss improved from 0.05871 to 0.05866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0373 - val_loss: 0.0587\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 362: val_loss improved from 0.05866 to 0.05860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0372 - val_loss: 0.0586\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 363: val_loss improved from 0.05860 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0372 - val_loss: 0.0585\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 364: val_loss improved from 0.05854 to 0.05849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0371 - val_loss: 0.0585\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 365: val_loss improved from 0.05849 to 0.05844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 366: val_loss improved from 0.05844 to 0.05838, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 367: val_loss improved from 0.05838 to 0.05833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0369 - val_loss: 0.0583\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 368: val_loss improved from 0.05833 to 0.05828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0367 - val_loss: 0.0583\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 369: val_loss improved from 0.05828 to 0.05823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 370: val_loss improved from 0.05823 to 0.05818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 371: val_loss improved from 0.05818 to 0.05813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0365 - val_loss: 0.0581\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 372: val_loss improved from 0.05813 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 373: val_loss improved from 0.05808 to 0.05803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0363 - val_loss: 0.0580\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 374: val_loss improved from 0.05803 to 0.05799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0361 - val_loss: 0.0580\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 375: val_loss improved from 0.05799 to 0.05794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0360 - val_loss: 0.0579\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 376: val_loss improved from 0.05794 to 0.05789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0360 - val_loss: 0.0579\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 377: val_loss improved from 0.05789 to 0.05785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 378: val_loss improved from 0.05785 to 0.05780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 379: val_loss improved from 0.05780 to 0.05776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0358 - val_loss: 0.0578\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 380: val_loss improved from 0.05776 to 0.05771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0358 - val_loss: 0.0577\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 381: val_loss improved from 0.05771 to 0.05767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 382: val_loss improved from 0.05767 to 0.05763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 383: val_loss improved from 0.05763 to 0.05759, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 384: val_loss improved from 0.05759 to 0.05755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0356 - val_loss: 0.0575\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 385: val_loss improved from 0.05755 to 0.05750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 386: val_loss improved from 0.05750 to 0.05746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0352 - val_loss: 0.0575\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 387: val_loss improved from 0.05746 to 0.05742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 388: val_loss improved from 0.05742 to 0.05739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0351 - val_loss: 0.0574\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 389: val_loss improved from 0.05739 to 0.05735, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0351 - val_loss: 0.0573\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 390: val_loss improved from 0.05735 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0351 - val_loss: 0.0573\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 391: val_loss improved from 0.05731 to 0.05727, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 392: val_loss improved from 0.05727 to 0.05723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0349 - val_loss: 0.0572\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 393: val_loss improved from 0.05723 to 0.05720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 154ms/step - loss: 0.0349 - val_loss: 0.0572\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 394: val_loss improved from 0.05720 to 0.05716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0347 - val_loss: 0.0572\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 395: val_loss improved from 0.05716 to 0.05713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 396: val_loss improved from 0.05713 to 0.05709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 397: val_loss improved from 0.05709 to 0.05706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 398: val_loss improved from 0.05706 to 0.05702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 399: val_loss improved from 0.05702 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 400: val_loss improved from 0.05699 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0343 - val_loss: 0.0570\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 401: val_loss improved from 0.05696 to 0.05692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 402: val_loss improved from 0.05692 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0342 - val_loss: 0.0569\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 403: val_loss improved from 0.05689 to 0.05686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 404: val_loss improved from 0.05686 to 0.05683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 405: val_loss improved from 0.05683 to 0.05680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 156ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 406: val_loss improved from 0.05680 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 407: val_loss improved from 0.05677 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 138ms/step - loss: 0.0341 - val_loss: 0.0567\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 408: val_loss improved from 0.05674 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 409: val_loss improved from 0.05671 to 0.05668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 410: val_loss improved from 0.05668 to 0.05665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 411: val_loss improved from 0.05665 to 0.05662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0338 - val_loss: 0.0566\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 412: val_loss improved from 0.05662 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 413: val_loss improved from 0.05659 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0336 - val_loss: 0.0566\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 414: val_loss improved from 0.05657 to 0.05654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0336 - val_loss: 0.0565\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 415: val_loss improved from 0.05654 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 416: val_loss improved from 0.05651 to 0.05649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 417: val_loss improved from 0.05649 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0333 - val_loss: 0.0565\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 418: val_loss improved from 0.05646 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 419: val_loss improved from 0.05644 to 0.05641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 420: val_loss improved from 0.05641 to 0.05639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 421: val_loss improved from 0.05639 to 0.05636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 422: val_loss improved from 0.05636 to 0.05634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0332 - val_loss: 0.0563\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 423: val_loss improved from 0.05634 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 424: val_loss improved from 0.05632 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 425: val_loss improved from 0.05629 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 426: val_loss improved from 0.05627 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0330 - val_loss: 0.0562\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 427: val_loss improved from 0.05625 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 428: val_loss improved from 0.05623 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 429: val_loss improved from 0.05620 to 0.05618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 430: val_loss improved from 0.05618 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0326 - val_loss: 0.0562\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 431: val_loss improved from 0.05616 to 0.05614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 432: val_loss improved from 0.05614 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 433: val_loss improved from 0.05612 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 434: val_loss improved from 0.05610 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 435: val_loss improved from 0.05608 to 0.05606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 436: val_loss improved from 0.05606 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 437: val_loss improved from 0.05604 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 438: val_loss improved from 0.05602 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 439: val_loss improved from 0.05600 to 0.05599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 440: val_loss improved from 0.05599 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 441: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 442: val_loss improved from 0.05595 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0323 - val_loss: 0.0559\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 443: val_loss improved from 0.05593 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 444: val_loss improved from 0.05592 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 445: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 446: val_loss improved from 0.05588 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 447: val_loss improved from 0.05587 to 0.05585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 448: val_loss improved from 0.05585 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0321 - val_loss: 0.0558\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 449: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 450: val_loss improved from 0.05582 to 0.05581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 451: val_loss improved from 0.05581 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 452: val_loss improved from 0.05579 to 0.05578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 453: val_loss improved from 0.05578 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 454: val_loss improved from 0.05576 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 455: val_loss improved from 0.05575 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 456: val_loss improved from 0.05573 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 457: val_loss improved from 0.05572 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 458: val_loss improved from 0.05571 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 459: val_loss improved from 0.05569 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 460: val_loss improved from 0.05568 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 461: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0315 - val_loss: 0.0557\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 462: val_loss improved from 0.05566 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0316 - val_loss: 0.0556\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 463: val_loss improved from 0.05564 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 464: val_loss improved from 0.05563 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 465: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 466: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 467: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 468: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 469: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 470: val_loss improved from 0.05557 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 471: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 472: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0313 - val_loss: 0.0555\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 473: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 474: val_loss improved from 0.05552 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 475: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 476: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 477: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 478: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 479: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 480: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 481: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 482: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 483: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 484: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 485: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 486: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 487: val_loss improved from 0.05541 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 488: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 489: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 490: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 491: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 492: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 493: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 494: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 495: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 496: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 497: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 498: val_loss improved from 0.05534 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 499: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 500: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 501: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 502: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 503: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 504: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 505: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 506: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 507: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 508: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 509: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 510: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 511: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 512: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 513: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 514: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 515: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 516: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 517: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 518: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 519: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 520: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 521: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 522: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 523: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 524: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 525: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 526: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 527: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 528: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 529: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 530: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 531: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 532/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 532: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 533/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 533: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 534/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 534: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 535/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 535: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 536/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 536: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 537/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 537: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 538/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 538: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 539/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 539: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 540/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 540: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 541/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 541: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 542/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 542: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 543/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 543: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 544/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 544: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 545/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 545: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 546/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 546: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 547/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 547: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 548/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 548: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 549/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 549: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 550/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 550: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 551/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 551: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 552/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 552: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 553/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 553: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 554/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 554: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 555/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 555: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 556/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 556: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 557/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 557: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 558/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 558: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 559/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 559: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 560/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 560: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 561/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 561: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 562/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 562: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 563/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 563: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 564/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 564: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 565/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 565: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 566/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 566: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 567/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 567: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 568/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 568: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 569/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 569: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 570/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 570: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 571/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 571: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 572/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 572: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 573/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 573: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 574/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 574: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 575/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 575: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 576/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 576: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 577/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 577: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 578/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 578: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 579/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 579: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 580/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 580: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 581/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 581: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 582/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 582: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 583/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 583: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 584/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 584: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 585/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 585: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 586/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 586: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 587/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 587: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 588/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 588: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 589/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 589: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 590/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 590: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 591/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 591: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 592/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 592: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 593/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 593: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 594/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 594: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 595/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 595: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 596/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 596: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 597/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 597: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 598/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 598: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 599/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 599: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 600/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 600: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 601/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 601: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 602/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 602: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 603/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 603: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 604/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 604: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 605/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 605: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 606/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 606: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 607/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 607: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 608/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 608: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 609/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 609: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 610/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 610: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 611/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 611: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 612/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 612: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 613/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 613: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 614/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 614: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 615/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 615: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 616/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 616: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 617/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 617: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 618/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 618: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 619/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 619: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 620/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 620: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 621/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 621: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 622/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 622: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 623/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 623: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 624/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 624: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 625/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 625: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 626/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 626: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 627/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 627: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 628/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 628: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 629/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 629: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 630/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 630: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 631/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 631: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 632/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 632: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 633/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 633: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 634/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 634: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 635/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 635: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 636/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 636: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 637/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 637: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 638/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 638: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 639/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 639: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 640/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 640: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 641/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 641: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 642/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 642: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 643/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 643: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 644/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 644: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 645/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 645: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 646/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 646: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 647/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 647: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 648/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 648: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 649/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 649: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 650/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 650: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 651/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 651: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 652/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 652: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 653/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 653: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0286 - val_loss: 0.0554\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0733\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78547c5068c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["loss_and_metrics : 0.07334905862808228\n","1/1 [==============================] - 0s 95ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=20, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.8))\n","model.add(Dense(units=20, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(units=2, activation=\"elu\", kernel_initializer=\"normal\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}